{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1401f6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 91.60863735 183.51532841 132.56073006 169.74548723 109.38710684\n",
      "  91.8873967  153.60027817 172.78285757 140.41406043 154.91218081\n",
      " 188.90697787 127.45961423  95.03019196 167.11835818 125.48796284\n",
      "  92.16406301 153.33525391 233.04203623 264.20843664 153.64811496\n",
      " 184.312558   266.99663946 122.46726976 213.03087064  90.93047685\n",
      " 156.1655288  218.20817779 104.09672539 172.09531621 167.38728029\n",
      " 141.52894566 190.71767249  95.77397289 127.68754558 213.58494261\n",
      " 218.34263236 140.69422066 204.42852979 160.80862002  91.77653797\n",
      " 175.13824234 123.49138062 237.94162554 117.12257348 230.56017014\n",
      " 205.83790515 205.01187406 189.62570754  96.09681171 112.31746603\n",
      "  96.96269963 110.8882703  195.56109359 220.36339839 111.86307897\n",
      " 184.51855948 134.34892503  96.51838405 136.41582943 121.53020929\n",
      " 176.40127842 177.13342264 157.1553569   82.69463892 168.17676896\n",
      " 223.40296735 221.76335438 167.76295775  95.66053876 119.80214073\n",
      " 176.74705795  94.01339666 146.57798495 166.2048195  148.56333223\n",
      " 124.79658047 168.79207951 220.51971478 182.42604627 249.13560922\n",
      " 136.57108345  83.27023285  89.95189849 207.86613879 216.3194085\n",
      " 174.55623873 229.542148   194.00381365 110.22647823 102.70892615\n",
      " 186.98367929 178.13071479 120.46971243 161.03044251 204.74999152\n",
      " 131.38765429 115.56537953 165.30225391 134.00399532 103.41134986\n",
      " 154.01998878 119.07245875 123.12033364 106.69124888 103.15006071\n",
      " 108.27820494 157.65320275 163.53320685 205.3172481  187.35471317\n",
      " 152.7323299 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy_zhang/anaconda3/envs/env_a/lib/python3.11/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Load the diabetes dataset.\n",
    "    db = load_diabetes()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "\n",
    "    # Create and train models.\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model to make predictions on the test dataset.\n",
    "    predictions = rf.predict(X_test)\n",
    "    print(predictions)\n",
    "\n",
    "    signature = infer_signature(X_test, predictions)\n",
    "    mlflow.sklearn.log_model(rf, \"model\", signature=signature)\n",
    "\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d7709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104.19536386 147.25514102  91.84426681 115.80501541 157.70044376\n",
      " 158.65071799 102.86626522  97.0914957  132.83355183 110.22541016\n",
      " 163.32277693 253.0153023  146.96195239 180.37670036 173.90352583\n",
      " 119.7895962  146.37269389 188.23106419  88.33669408 168.59123033\n",
      " 106.47170419 182.3451415  255.44942373 118.48276204 131.18710908\n",
      " 208.7356625  151.63915439  82.62822981  99.08058361 156.70690877\n",
      " 145.25441605 117.39754791 171.3146377  230.9451768  101.02379233\n",
      " 151.90895284 104.38198485 100.09255233 179.60171136 104.81741926\n",
      " 170.77205692 192.93174608  92.18088478 263.97781958  95.65757008\n",
      " 225.22134731  90.13179584 103.9148921  165.66134177 118.52265308\n",
      " 220.30838645  84.61558267 179.13133026 112.09000491 156.78057206\n",
      " 165.35991822 138.42297315 161.08177485 178.41138952 213.11511399\n",
      "  95.57566294 180.39743557 121.56561795 197.67272806  83.48467499\n",
      " 247.29370666 221.92175581 176.50853053 111.50459766 108.73558724\n",
      " 133.13785873  98.25706647 154.94286142 193.50328359 176.49765671\n",
      " 178.17246626 142.47211064  92.5270128  174.50276302  79.42208886\n",
      " 165.2352602  168.66912363 102.3518511  165.02072805 189.8160161\n",
      " 146.19247655  99.54082797 159.0704448  165.3015846  231.90940692\n",
      " 136.73326935 101.80133679 154.2719119   94.45945384 179.72586923\n",
      " 193.29041475 108.89364534 107.60240872 146.98548433 160.96589536\n",
      " 173.57257336  97.88144896 110.04425455 109.94849997 152.07978808\n",
      " 248.58008483 177.85427    151.34709287  95.910541   235.57688265\n",
      " 138.17606719]\n",
      "Run ID: cd839e2bba404761802c5f91191ae82d\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Load the diabetes dataset.\n",
    "    db = load_diabetes()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "\n",
    "    # Create and train models.\n",
    "    rf = RandomForestRegressor(n_estimators=100, max_depth=6, max_features=3)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Use the model to make predictions on the test dataset.\n",
    "    predictions = rf.predict(X_test)\n",
    "    print(predictions)\n",
    "\n",
    "    signature = infer_signature(X_test, predictions)\n",
    "    mlflow.sklearn.log_model(rf, \"model\", signature=signature)\n",
    "\n",
    "    print(f\"Run ID: {run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482e1940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d1ccc59fc747539f4e851633e8124f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "MlflowException",
     "evalue": "Could not find an \"MLmodel\" configuration file at \"/tmp/tmp7z5x_38k/\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m db \u001b[38;5;241m=\u001b[39m load_diabetes()\n\u001b[1;32m      7\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(db\u001b[38;5;241m.\u001b[39mdata, db\u001b[38;5;241m.\u001b[39mtarget)\n\u001b[0;32m----> 9\u001b[0m model \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mruns:/cd839e2bba404761802c5f91191ae82d/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(predictions)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/sklearn/__init__.py:611\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_uri, dst_path)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;124;03mLoad a scikit-learn model from a local file or a run.\u001b[39;00m\n\u001b[1;32m    580\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;124;03m    predictions = sk_model.predict(pandas_df)\u001b[39;00m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    610\u001b[0m local_model_path \u001b[38;5;241m=\u001b[39m _download_artifact_from_uri(artifact_uri\u001b[38;5;241m=\u001b[39mmodel_uri, output_path\u001b[38;5;241m=\u001b[39mdst_path)\n\u001b[0;32m--> 611\u001b[0m flavor_conf \u001b[38;5;241m=\u001b[39m _get_flavor_configuration(model_path\u001b[38;5;241m=\u001b[39mlocal_model_path, flavor_name\u001b[38;5;241m=\u001b[39mFLAVOR_NAME)\n\u001b[1;32m    612\u001b[0m _add_code_from_conf_to_system_path(local_model_path, flavor_conf)\n\u001b[1;32m    613\u001b[0m sklearn_model_artifacts_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_model_path, flavor_conf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpickled_model\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/mlflow/utils/model_utils.py:31\u001b[0m, in \u001b[0;36m_get_flavor_configuration\u001b[0;34m(model_path, flavor_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m model_configuration_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, MLMODEL_FILE_NAME)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(model_configuration_path):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not find an \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMLMODEL_FILE_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m configuration file at \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     33\u001b[0m         RESOURCE_DOES_NOT_EXIST,\n\u001b[1;32m     34\u001b[0m     )\n\u001b[1;32m     36\u001b[0m model_conf \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39mload(model_configuration_path)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flavor_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_conf\u001b[38;5;241m.\u001b[39mflavors:\n",
      "\u001b[0;31mMlflowException\u001b[0m: Could not find an \"MLmodel\" configuration file at \"/tmp/tmp7z5x_38k/\""
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "db = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(db.data, db.target)\n",
    "\n",
    "model = mlflow.sklearn.load_model('runs:/cd839e2bba404761802c5f91191ae82d/')\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da8876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestRegressor(max_depth=6, min_samples_leaf=4, min_samples_split=10,\n",
      "                      random_state=888)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(sklearn.ensemble._forest.RandomForestRegressor, None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "f = open('model.pkl','rb')\n",
    "data = pickle.load(f)\n",
    "type(data),print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1a699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 2,\n",
       " 'global_step': 1290,\n",
       " 'pytorch-lightning_version': '2.1.0',\n",
       " 'state_dict': OrderedDict([('layer_1.weight',\n",
       "               tensor([[ 0.0633,  0.0950,  0.0672,  ...,  0.0951,  0.0898,  0.0449],\n",
       "                       [ 0.0761,  0.0939,  0.1276,  ...,  0.1244,  0.1177,  0.0911],\n",
       "                       [-0.0055, -0.0034,  0.0207,  ...,  0.0250,  0.0401,  0.0028],\n",
       "                       ...,\n",
       "                       [ 0.0681,  0.0823,  0.0889,  ...,  0.0914,  0.0354,  0.0783],\n",
       "                       [ 0.2150,  0.1816,  0.1800,  ...,  0.1896,  0.2103,  0.1745],\n",
       "                       [ 0.0587,  0.0460,  0.0539,  ...,  0.0215,  0.0619,  0.0116]],\n",
       "                      device='cuda:0')),\n",
       "              ('layer_1.bias',\n",
       "               tensor([-0.0605, -0.1051,  0.0227, -0.1274, -0.0625, -0.0796, -0.0308, -0.0489,\n",
       "                       -0.0805, -0.0413, -0.1458, -0.0321, -0.1841, -0.0365, -0.0591, -0.0373,\n",
       "                       -0.3431, -0.0860, -0.0698, -0.0624, -0.0958, -0.0166, -0.0644, -0.1173,\n",
       "                       -0.0909, -0.0362, -0.1441, -0.0761, -0.1211, -0.0171, -0.0494, -0.1406,\n",
       "                       -0.0962, -0.1390, -0.1661, -0.0783, -0.2392, -0.1023, -0.0578, -0.0709,\n",
       "                       -0.0785, -0.0602, -0.1172, -0.0945, -0.0412, -0.0803, -0.0709, -0.1027,\n",
       "                       -0.1913, -0.0269, -0.0991, -0.0317, -0.0371, -0.0954, -0.0613, -0.0361,\n",
       "                       -0.0621, -0.0692, -0.0452, -0.0505, -0.0786, -0.0513, -0.0669, -0.0400,\n",
       "                       -0.0034, -0.0565, -0.0725,  0.0318, -0.1518, -0.0140, -0.0307, -0.1128,\n",
       "                       -0.0495, -0.1619, -0.1703, -0.0307, -0.0556, -0.0725, -0.1074, -0.1457,\n",
       "                       -0.0168,  0.0570, -0.0283, -0.0208, -0.0800, -0.1321, -0.0318, -0.1281,\n",
       "                       -0.0338, -0.0825, -0.0242, -0.2051, -0.1050, -0.1715, -0.0560, -0.0293,\n",
       "                       -0.0735, -0.0839, -0.0507, -0.0571, -0.0952, -0.1580, -0.0519, -0.0858,\n",
       "                       -0.0200, -0.1343, -0.1617,  0.0294, -0.0625, -0.1631, -0.0827, -0.0458,\n",
       "                       -0.0930, -0.0793, -0.0166, -0.0631, -0.0694,  0.0115, -0.0724, -0.0461,\n",
       "                       -0.0617, -0.0970, -0.1346, -0.1197, -0.0754, -0.0362, -0.1894, -0.0050],\n",
       "                      device='cuda:0')),\n",
       "              ('layer_2.weight',\n",
       "               tensor([[ 0.0379, -0.1212, -0.1481,  ..., -0.0280, -0.1220, -0.0192],\n",
       "                       [ 0.0288, -0.1169, -0.4617,  ..., -0.0284, -0.1497,  0.0615],\n",
       "                       [ 0.2090, -0.2261, -0.3298,  ..., -0.0284, -0.0655, -0.0787],\n",
       "                       ...,\n",
       "                       [ 0.2706, -0.1760, -0.0316,  ...,  0.0084,  0.0345,  0.1423],\n",
       "                       [ 0.0482, -0.1884, -0.0868,  ..., -0.1224,  0.0065, -0.0914],\n",
       "                       [-0.0168, -0.0197, -0.2511,  ..., -0.1469, -0.0953,  0.1372]],\n",
       "                      device='cuda:0')),\n",
       "              ('layer_2.bias',\n",
       "               tensor([-4.1440e-02,  1.8051e-01, -2.4446e-02, -2.2317e-01, -1.4048e-01,\n",
       "                       -5.4855e-02, -1.5281e-01, -5.1719e-01, -3.0031e-01, -1.1742e-01,\n",
       "                       -3.4629e-01, -1.3003e-01, -6.5994e-02, -1.7803e-01, -1.1637e-01,\n",
       "                       -1.3334e-01, -2.0988e-01, -1.6803e-01, -2.1581e-01, -5.1124e-01,\n",
       "                       -3.7622e-01, -3.2530e-01, -1.5911e-01,  1.0097e-01, -1.0687e-01,\n",
       "                       -3.6056e-02, -1.7592e-01, -2.4695e-01, -1.4563e-01, -3.7867e-01,\n",
       "                       -8.4437e-02, -1.5229e-01, -1.6636e-01, -2.9056e-01, -3.0223e-01,\n",
       "                        5.8220e-02, -1.6763e-01, -5.6071e-02, -3.8221e-01, -1.7763e-01,\n",
       "                       -1.2331e-01, -1.1056e-01, -1.3867e-01,  6.0718e-01, -3.1318e-01,\n",
       "                       -2.2911e-01, -7.3581e-02, -1.6781e-01, -7.3404e-02, -4.4641e-01,\n",
       "                       -1.2287e-01, -1.2261e-01, -2.2640e-01, -1.7224e-01, -1.6651e-01,\n",
       "                       -7.1282e-02, -1.0791e-01, -4.0372e-01, -1.2594e-01, -5.4593e-01,\n",
       "                       -9.9431e-02,  2.7173e-01, -1.1918e-01, -2.0453e-01,  6.0489e-02,\n",
       "                       -4.1611e-01, -4.1685e-02, -8.1062e-02, -4.6056e-01, -1.3084e-01,\n",
       "                       -3.5061e-01, -3.6734e-02, -3.3390e-01, -2.2790e-01, -2.3265e-01,\n",
       "                        3.1477e-02, -2.7270e-01,  4.5863e-02, -4.7083e-02, -4.4009e-01,\n",
       "                       -2.7223e-01,  3.0599e-02, -3.0499e-01, -4.9715e-01, -4.4289e-01,\n",
       "                       -1.8499e-01, -2.5614e-01, -1.8474e-01, -1.5693e-01, -3.3279e-01,\n",
       "                       -2.5747e-01, -4.7570e-01, -3.1095e-01, -3.2410e-01,  4.5286e-01,\n",
       "                       -3.4269e-02, -2.9168e-01, -2.8125e-01, -1.7227e-01, -3.4655e-01,\n",
       "                       -4.6069e-01, -8.2401e-02, -1.6624e-01, -1.2386e-01, -1.1052e-01,\n",
       "                        7.7896e-03, -7.4338e-02, -2.4628e-01, -3.7161e-01, -3.5730e-01,\n",
       "                       -3.7771e-01, -1.9144e-01, -3.0039e-01,  5.2286e-02, -2.6416e-01,\n",
       "                       -1.9250e-01, -4.6879e-01, -2.1279e-01, -2.3370e-01, -7.8711e-02,\n",
       "                       -2.5992e-01, -2.3927e-01, -4.4275e-01,  2.6735e-01, -5.2827e-01,\n",
       "                       -1.8809e-01, -1.1286e-01, -2.6248e-01, -4.5498e-01, -7.9675e-03,\n",
       "                       -3.2980e-01, -4.2770e-01, -3.0113e-01, -2.0062e-01, -2.5992e-01,\n",
       "                       -2.2414e-01, -1.5684e-01, -6.8717e-02, -1.4113e-01, -3.0964e-01,\n",
       "                       -3.6070e-01, -1.4330e-01, -3.8341e-01, -1.1383e-01,  5.9527e-02,\n",
       "                       -2.3136e-01, -2.4979e-02, -3.1133e-01, -4.0841e-01, -8.7050e-02,\n",
       "                       -1.2143e-01, -2.4208e-01, -3.5707e-01, -1.8043e-01, -1.5882e-01,\n",
       "                       -3.3243e-01, -5.6729e-02, -1.6223e-01, -1.5957e-01, -6.6305e-02,\n",
       "                       -1.9000e-01, -2.3058e-01, -1.2935e-01, -1.2884e-01, -9.8403e-02,\n",
       "                       -2.9052e-01, -3.5034e-01, -3.1683e-01,  8.5085e-02, -3.5044e-01,\n",
       "                       -1.3287e-01, -5.0922e-01, -2.4932e-01,  1.7628e-01, -3.4822e-01,\n",
       "                       -2.7239e-01, -2.0101e-01, -5.1606e-01, -1.6075e-01, -4.2853e-01,\n",
       "                       -5.7096e-03, -1.3583e-01,  1.4192e-01, -4.0340e-02, -2.3938e-01,\n",
       "                       -2.1145e-01, -1.4884e-01, -1.7685e-01,  5.3427e-05, -7.2457e-02,\n",
       "                       -2.3527e-01, -3.8383e-01, -2.4914e-01, -1.5519e-01, -1.5653e-01,\n",
       "                        1.5667e-01, -1.0779e-02, -2.7824e-01, -5.7596e-02,  1.9030e-01,\n",
       "                       -1.0542e-01, -2.9466e-01, -8.0848e-02, -2.4865e-01, -2.9129e-01,\n",
       "                       -2.4174e-01, -1.9183e-01, -6.0781e-02, -2.0849e-01, -3.4054e-01,\n",
       "                       -1.8534e-01, -1.5077e-01, -1.2676e-01, -2.1977e-01, -1.2472e-01,\n",
       "                       -4.9511e-02, -3.4432e-01, -2.5398e-01, -1.8837e-01, -1.1002e-01,\n",
       "                        3.1497e-02, -1.3551e-01, -2.7917e-01, -8.1195e-02, -2.9013e-01,\n",
       "                       -2.3836e-01, -2.3652e-01, -1.1161e-01, -6.2695e-01,  6.8226e-02,\n",
       "                       -9.6134e-03, -1.9225e-01, -3.3004e-01, -3.5385e-01, -2.5318e-01,\n",
       "                       -3.3640e-01, -8.5721e-02, -3.3514e-01, -4.1914e-01, -3.6519e-01,\n",
       "                       -8.4748e-02, -4.1921e-01, -1.9435e-01, -2.1490e-01, -1.5758e-01,\n",
       "                       -1.3108e-01, -4.7116e-01, -9.7443e-02, -2.9750e-01,  2.0858e-01,\n",
       "                       -1.0667e-01, -3.6839e-02, -1.4387e-01,  4.2239e-01, -1.2805e-01,\n",
       "                       -6.4690e-02], device='cuda:0')),\n",
       "              ('layer_3.weight',\n",
       "               tensor([[ 0.0095,  0.1411,  0.0169,  ..., -0.2285,  0.0270, -0.0645],\n",
       "                       [-0.0638, -0.0732, -0.0869,  ..., -0.0633, -0.0704, -0.0265],\n",
       "                       [-0.0307, -0.2239,  0.1199,  ..., -0.1323, -0.0540, -0.1038],\n",
       "                       ...,\n",
       "                       [-0.1631, -0.2209,  0.0925,  ..., -0.1287, -0.0336, -0.0166],\n",
       "                       [ 0.0273,  0.1389,  0.0604,  ...,  0.0938,  0.0187, -0.0828],\n",
       "                       [-0.0800,  0.0642, -0.1109,  ...,  0.1525, -0.0481,  0.0502]],\n",
       "                      device='cuda:0')),\n",
       "              ('layer_3.bias',\n",
       "               tensor([ 0.1193, -0.2864,  0.0729,  0.0338, -0.0305, -0.1375, -0.0497, -0.3375,\n",
       "                        0.5478, -0.1602], device='cuda:0'))]),\n",
       " 'loops': {'fit_loop': {'state_dict': {},\n",
       "   'epoch_loop.state_dict': {'_batches_that_stepped': 1290},\n",
       "   'epoch_loop.batch_progress': {'total': {'ready': 1290,\n",
       "     'completed': 1290,\n",
       "     'started': 1290,\n",
       "     'processed': 1290},\n",
       "    'current': {'ready': 430,\n",
       "     'completed': 430,\n",
       "     'started': 430,\n",
       "     'processed': 430},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_loop.scheduler_progress': {'total': {'ready': 2, 'completed': 2},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.automatic_optimization.state_dict': {},\n",
       "   'epoch_loop.automatic_optimization.optim_progress': {'optimizer': {'step': {'total': {'ready': 1290,\n",
       "       'completed': 1290},\n",
       "      'current': {'ready': 430, 'completed': 430}},\n",
       "     'zero_grad': {'total': {'ready': 1290,\n",
       "       'completed': 1290,\n",
       "       'started': 1290},\n",
       "      'current': {'ready': 430, 'completed': 430, 'started': 430}}}},\n",
       "   'epoch_loop.manual_optimization.state_dict': {},\n",
       "   'epoch_loop.manual_optimization.optim_step_progress': {'total': {'ready': 0,\n",
       "     'completed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0}},\n",
       "   'epoch_loop.val_loop.state_dict': {},\n",
       "   'epoch_loop.val_loop.batch_progress': {'total': {'ready': 40,\n",
       "     'completed': 40,\n",
       "     'started': 40,\n",
       "     'processed': 40},\n",
       "    'current': {'ready': 40, 'completed': 40, 'started': 40, 'processed': 40},\n",
       "    'is_last_batch': True},\n",
       "   'epoch_progress': {'total': {'ready': 3,\n",
       "     'completed': 2,\n",
       "     'started': 3,\n",
       "     'processed': 3},\n",
       "    'current': {'ready': 3, 'completed': 2, 'started': 3, 'processed': 3}}},\n",
       "  'validate_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'test_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0},\n",
       "    'is_last_batch': False}},\n",
       "  'predict_loop': {'state_dict': {},\n",
       "   'batch_progress': {'total': {'ready': 0,\n",
       "     'completed': 0,\n",
       "     'started': 0,\n",
       "     'processed': 0},\n",
       "    'current': {'ready': 0, 'completed': 0, 'started': 0, 'processed': 0}}}},\n",
       " 'callbacks': {\"EarlyStopping{'monitor': 'val_loss', 'mode': 'min'}\": {'wait_count': 0,\n",
       "   'stopped_epoch': 0,\n",
       "   'best_score': tensor(0.1647, device='cuda:0'),\n",
       "   'patience': 3},\n",
       "  \"ModelCheckpoint{'monitor': 'val_loss', 'mode': 'min', 'every_n_train_steps': 0, 'every_n_epochs': 1, 'train_time_interval': None}\": {'monitor': 'val_loss',\n",
       "   'best_model_score': tensor(0.1647, device='cuda:0'),\n",
       "   'best_model_path': '/home/jy_zhang/code/epoch=2-step=1290.ckpt',\n",
       "   'current_score': tensor(0.1647, device='cuda:0'),\n",
       "   'dirpath': '/home/jy_zhang/code',\n",
       "   'best_k_models': {'/home/jy_zhang/code/epoch=2-step=1290.ckpt': tensor(0.1647, device='cuda:0')},\n",
       "   'kth_best_model_path': '/home/jy_zhang/code/epoch=2-step=1290.ckpt',\n",
       "   'kth_value': tensor(0.1647, device='cuda:0'),\n",
       "   'last_model_path': ''}},\n",
       " 'optimizer_states': [{'state': {0: {'step': tensor(1290.),\n",
       "     'exp_avg': tensor([[ 1.6761e-04,  1.6761e-04,  1.6761e-04,  ...,  1.6761e-04,\n",
       "               1.6761e-04,  1.6761e-04],\n",
       "             [ 8.8411e-05,  8.8411e-05,  8.8411e-05,  ...,  8.8411e-05,\n",
       "               8.8411e-05,  8.8411e-05],\n",
       "             [-4.0608e-06, -4.0608e-06, -4.0608e-06,  ..., -4.0608e-06,\n",
       "              -4.0608e-06, -4.0608e-06],\n",
       "             ...,\n",
       "             [ 4.0610e-18,  4.0610e-18,  4.0610e-18,  ...,  4.0610e-18,\n",
       "               4.0610e-18,  4.0610e-18],\n",
       "             [ 3.7349e-09,  3.7349e-09,  3.7349e-09,  ...,  3.7349e-09,\n",
       "               3.7349e-09,  3.7349e-09],\n",
       "             [-5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
       "              -5.6052e-45, -5.6052e-45]], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([[6.8148e-07, 6.8148e-07, 6.8148e-07,  ..., 6.8148e-07, 6.8148e-07,\n",
       "              6.8148e-07],\n",
       "             [2.8916e-07, 2.8916e-07, 2.8916e-07,  ..., 2.8916e-07, 2.8916e-07,\n",
       "              2.8916e-07],\n",
       "             [4.3850e-07, 4.3850e-07, 4.3850e-07,  ..., 4.3850e-07, 4.3850e-07,\n",
       "              4.3850e-07],\n",
       "             ...,\n",
       "             [2.5405e-10, 2.5405e-10, 2.5405e-10,  ..., 2.5405e-10, 2.5405e-10,\n",
       "              2.5405e-10],\n",
       "             [1.7045e-08, 1.7045e-08, 1.7045e-08,  ..., 1.7045e-08, 1.7045e-08,\n",
       "              1.7045e-08],\n",
       "             [1.7751e-08, 1.7751e-08, 1.7751e-08,  ..., 1.7751e-08, 1.7751e-08,\n",
       "              1.7751e-08]], device='cuda:0')},\n",
       "    1: {'step': tensor(1290.),\n",
       "     'exp_avg': tensor([-3.9510e-04, -2.0841e-04,  9.5724e-06,  5.6052e-45, -1.5205e-04,\n",
       "              5.9628e-04,  5.6052e-45, -1.2645e-15,  8.3601e-05,  5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45, -2.4335e-05,  1.4482e-09, -1.7656e-11,\n",
       "             -4.6535e-16,  2.2000e-42,  1.6821e-04, -1.0918e-04,  1.7177e-04,\n",
       "              2.4220e-04,  2.2074e-04,  5.6052e-45,  5.6052e-45,  2.4445e-14,\n",
       "             -2.4568e-04,  1.9993e-04,  5.6052e-45,  4.6460e-05,  3.6153e-04,\n",
       "              5.6052e-45, -1.6819e-09,  1.7879e-05, -6.2358e-05,  5.5959e-04,\n",
       "              5.6052e-45,  1.4289e-09,  5.6052e-45,  5.6052e-45, -6.2682e-05,\n",
       "             -3.9187e-04,  5.6052e-45, -7.2445e-04,  5.6052e-45,  1.4295e-15,\n",
       "             -1.1114e-12, -1.7564e-04, -2.3712e-04,  9.4389e-06,  5.6052e-45,\n",
       "              3.7297e-04, -2.6499e-04, -6.7444e-04,  5.6052e-45,  1.2624e-04,\n",
       "              5.6052e-45,  5.9418e-04,  6.8019e-05,  5.6052e-45,  6.7774e-19,\n",
       "              4.9509e-05,  2.7464e-04,  5.6052e-45,  5.6052e-45,  1.2181e-04,\n",
       "             -8.2154e-05, -6.1598e-05, -2.3569e-04,  5.6052e-45,  2.7170e-04,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  2.8718e-16,\n",
       "              5.6052e-45,  5.6052e-45,  5.6052e-45, -3.9413e-04,  1.0795e-34,\n",
       "             -3.4312e-04, -2.0166e-04,  5.6052e-45,  1.6617e-04,  8.5900e-43,\n",
       "              6.7899e-04,  5.6052e-45, -5.1681e-05, -8.2990e-05, -1.5761e-04,\n",
       "              4.1741e-04,  1.8962e-08,  2.2074e-04, -7.3278e-05,  5.6052e-45,\n",
       "              1.9727e-05,  5.6052e-45,  1.0053e-05,  5.6052e-45,  5.6052e-45,\n",
       "              5.6052e-45, -8.4374e-13,  5.6052e-45,  1.2114e-31, -2.0265e-25,\n",
       "             -1.2978e-05,  5.6052e-45,  3.6691e-04,  5.6052e-45,  6.3445e-07,\n",
       "              2.8744e-33,  5.4511e-43, -9.8724e-09,  8.2525e-25,  5.6052e-45,\n",
       "              5.6052e-45, -2.6363e-04, -1.7435e-04,  5.6052e-45,  5.6052e-45,\n",
       "              1.6060e-17,  5.6052e-45,  6.5770e-06,  5.6052e-45, -1.5793e-04,\n",
       "             -9.5731e-18, -8.8042e-09,  5.6052e-45], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([3.7869e-06, 1.6068e-06, 2.4367e-06, 4.8166e-09, 1.2238e-06, 4.7464e-06,\n",
       "             2.3514e-08, 1.3934e-09, 6.0041e-07, 2.2986e-10, 1.9790e-09, 2.6869e-10,\n",
       "             7.2622e-07, 1.5001e-10, 7.9059e-08, 1.6268e-09, 2.5933e-09, 9.6160e-07,\n",
       "             1.2005e-06, 4.4771e-07, 3.5679e-06, 4.0490e-06, 3.2747e-09, 1.3223e-10,\n",
       "             1.1830e-07, 4.3552e-06, 7.6076e-07, 3.1501e-10, 1.9489e-06, 3.1126e-06,\n",
       "             7.0494e-08, 4.5265e-07, 2.9505e-06, 3.3276e-07, 2.1448e-06, 4.0576e-08,\n",
       "             1.2438e-07, 3.6207e-09, 9.1445e-09, 1.9971e-07, 5.7132e-06, 1.3950e-10,\n",
       "             3.8495e-06, 1.7969e-10, 9.1862e-08, 4.3475e-08, 2.7117e-06, 5.1634e-06,\n",
       "             5.2652e-08, 3.0571e-10, 2.4152e-06, 2.3503e-06, 5.4113e-06, 6.6236e-11,\n",
       "             3.5045e-06, 1.7455e-10, 3.0160e-06, 1.4800e-06, 1.9337e-10, 5.6273e-10,\n",
       "             1.6625e-06, 4.1071e-06, 6.9586e-09, 7.2835e-09, 1.6267e-06, 1.5562e-06,\n",
       "             2.6422e-06, 8.2917e-07, 1.7960e-09, 3.7105e-06, 1.4699e-10, 2.1784e-09,\n",
       "             4.8935e-11, 7.0627e-08, 1.9530e-07, 3.7720e-09, 7.4141e-08, 1.6171e-08,\n",
       "             9.7089e-07, 1.1830e-07, 1.9191e-06, 8.8510e-06, 9.5660e-08, 5.2561e-06,\n",
       "             4.4330e-08, 2.2736e-06, 1.1216e-07, 1.3654e-06, 1.4351e-06, 1.7611e-06,\n",
       "             1.8428e-06, 2.2594e-08, 1.3581e-06, 8.1457e-07, 9.7125e-08, 4.0892e-06,\n",
       "             2.3325e-10, 5.5031e-07, 3.6390e-08, 3.3801e-09, 1.7296e-09, 2.1589e-09,\n",
       "             4.8349e-11, 2.6796e-10, 4.2774e-08, 1.5574e-06, 5.5171e-10, 4.8285e-06,\n",
       "             2.3133e-09, 6.1717e-07, 3.5178e-08, 4.5873e-08, 4.1584e-07, 6.2790e-08,\n",
       "             1.3745e-07, 4.2763e-08, 1.3156e-06, 4.7452e-06, 3.2023e-07, 2.3178e-08,\n",
       "             1.1262e-09, 2.8247e-09, 4.7976e-07, 8.1501e-08, 1.1267e-06, 1.4118e-09,\n",
       "             9.4715e-08, 9.8640e-08], device='cuda:0')},\n",
       "    2: {'step': tensor(1290.),\n",
       "     'exp_avg': tensor([[-6.0196e-06, -3.4526e-13, -2.2366e-09,  ..., -5.6052e-45,\n",
       "               5.6052e-45,  5.6052e-45],\n",
       "             [-1.4958e-04, -2.0993e-23, -9.9892e-06,  ...,  5.6052e-45,\n",
       "              -5.6052e-45, -5.6052e-45],\n",
       "             [ 1.6554e-04, -6.3895e-13, -6.4918e-07,  ...,  5.6052e-45,\n",
       "              -1.7730e-18,  5.6052e-45],\n",
       "             ...,\n",
       "             [-7.8254e-04,  2.2165e-03, -5.7847e-04,  ..., -5.6052e-45,\n",
       "               5.6971e-19, -5.6052e-45],\n",
       "             [-1.0604e-10, -3.4361e-18, -1.6185e-27,  ...,  5.6052e-45,\n",
       "              -5.6052e-45,  5.6052e-45],\n",
       "             [ 5.7884e-04,  8.1438e-04, -2.2965e-08,  ..., -5.6052e-45,\n",
       "               4.0422e-38, -5.6052e-45]], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([[1.0715e-05, 1.3030e-06, 6.0150e-07,  ..., 7.0535e-11, 9.6631e-08,\n",
       "              1.0662e-11],\n",
       "             [7.3917e-06, 2.8561e-07, 1.5838e-07,  ..., 9.9545e-12, 3.1662e-08,\n",
       "              5.5441e-12],\n",
       "             [7.5218e-05, 3.1855e-06, 5.3941e-06,  ..., 4.7382e-11, 5.1365e-08,\n",
       "              1.0240e-06],\n",
       "             ...,\n",
       "             [1.7566e-04, 4.5576e-05, 4.8835e-05,  ..., 1.8433e-12, 1.5908e-07,\n",
       "              2.3215e-11],\n",
       "             [2.0716e-06, 2.9039e-08, 1.0134e-07,  ..., 8.3577e-12, 2.0435e-08,\n",
       "              3.4412e-08],\n",
       "             [1.0861e-05, 6.1756e-06, 2.0107e-06,  ..., 2.4161e-10, 4.5843e-08,\n",
       "              2.0631e-08]], device='cuda:0')},\n",
       "    3: {'step': tensor(1290.),\n",
       "     'exp_avg': tensor([-2.5679e-06, -1.9266e-04, -1.1465e-05,  3.8117e-06, -3.7510e-05,\n",
       "              3.4999e-07,  2.3196e-06,  1.7566e-06,  2.8527e-04,  3.9909e-11,\n",
       "              9.9093e-07, -2.0766e-05,  3.2709e-12,  1.0501e-04, -3.6245e-05,\n",
       "             -1.7793e-05, -4.7873e-08,  1.6610e-05,  3.7314e-36,  1.9498e-04,\n",
       "             -5.4618e-05,  1.1684e-05, -3.0660e-06, -1.7777e-04,  6.8252e-05,\n",
       "              3.1843e-05,  7.0065e-45,  3.2999e-04, -9.2271e-13,  1.4978e-06,\n",
       "             -1.7578e-05,  3.2513e-08,  6.3529e-05,  2.3108e-05,  7.2818e-05,\n",
       "             -5.0954e-04,  5.2690e-05,  1.8443e-09,  3.4050e-04, -7.2084e-07,\n",
       "             -7.0239e-08, -5.6782e-07, -4.8261e-10, -3.3104e-04,  6.4741e-05,\n",
       "              5.1001e-33, -3.6156e-04,  2.2981e-10, -2.1875e-04,  3.4580e-04,\n",
       "              4.2594e-05, -3.7240e-09,  1.0485e-05,  3.2963e-05, -3.5710e-05,\n",
       "             -8.9061e-08,  3.6087e-06,  5.2646e-12, -7.4027e-05, -4.0163e-05,\n",
       "             -6.4940e-05, -2.2923e-04, -2.1552e-07,  1.5180e-24, -3.8262e-04,\n",
       "              4.1668e-15, -4.5617e-05,  3.3294e-13, -5.4248e-05, -2.0440e-09,\n",
       "             -8.2717e-06,  4.3521e-18,  1.2388e-05,  8.7868e-06,  1.1539e-05,\n",
       "             -4.9699e-04,  1.2549e-05, -9.3415e-05,  3.6303e-07,  1.4413e-05,\n",
       "             -8.4047e-06,  2.0689e-06, -5.6827e-10,  7.1491e-09, -2.4627e-04,\n",
       "             -2.3209e-05,  2.9249e-05,  1.5391e-06,  4.5346e-37, -1.8015e-05,\n",
       "             -2.1459e-05,  1.0510e-05,  6.1280e-05,  5.3263e-05, -1.1720e-03,\n",
       "              5.7733e-43, -1.6461e-05,  6.5582e-05,  3.1061e-06,  2.0916e-06,\n",
       "              4.5069e-05, -6.0997e-05,  1.4973e-05,  1.6126e-17, -7.7212e-11,\n",
       "              1.1090e-06, -2.8576e-06,  3.6589e-07,  1.9335e-05, -1.6355e-09,\n",
       "             -4.2083e-06,  3.2165e-05,  3.0500e-05, -3.1085e-07, -4.2911e-11,\n",
       "              5.6052e-45,  2.7615e-04,  5.5824e-06, -1.3224e-06, -3.7112e-08,\n",
       "             -7.5826e-07, -1.7196e-06,  2.5086e-06,  1.9910e-05,  6.1746e-06,\n",
       "             -2.4467e-06,  2.4883e-06,  1.0740e-05,  6.6209e-06,  3.2374e-06,\n",
       "             -9.7250e-07, -7.4621e-05,  3.5825e-04,  1.0199e-04,  1.0171e-04,\n",
       "             -2.3527e-04,  2.5486e-07, -2.7013e-05,  1.2793e-05,  2.0562e-05,\n",
       "             -1.1669e-06, -8.2383e-07,  3.5841e-05, -3.1359e-23, -8.0880e-05,\n",
       "              7.3590e-35, -2.1878e-07, -7.9015e-07,  1.2237e-06,  6.9142e-05,\n",
       "              9.0278e-07,  3.1215e-07,  6.7056e-40,  1.0128e-04,  5.8740e-06,\n",
       "              2.1541e-10,  2.0840e-14, -2.8622e-04,  1.7511e-11,  1.7173e-09,\n",
       "              1.7514e-05, -2.8935e-06, -1.6655e-05, -1.3015e-04, -4.0245e-08,\n",
       "              2.5015e-05, -1.5209e-05,  3.7564e-09, -2.6544e-04,  5.4947e-08,\n",
       "             -1.2566e-06,  4.4374e-04, -8.9518e-05, -6.8365e-05,  4.0799e-05,\n",
       "             -1.3781e-06,  6.9403e-06,  9.7221e-14,  3.2187e-06, -3.7407e-34,\n",
       "             -1.8770e-04, -4.4083e-41, -8.2038e-05,  1.7219e-07,  1.0497e-05,\n",
       "              4.6428e-06, -2.1624e-11, -3.4417e-05, -3.8250e-04, -1.2374e-04,\n",
       "              5.6052e-45,  1.6983e-04,  2.3454e-06, -9.0868e-05, -2.2878e-04,\n",
       "              7.1976e-05, -2.4834e-05,  2.6674e-04,  1.4824e-05, -9.3954e-05,\n",
       "             -1.0200e-06, -7.4233e-12, -1.7035e-09,  1.1923e-04,  6.3817e-05,\n",
       "              1.0537e-04,  8.2889e-07,  4.5878e-08, -1.7909e-04,  3.5019e-06,\n",
       "              2.4578e-06,  2.9394e-13,  1.1264e-38,  9.1438e-07, -1.4556e-04,\n",
       "             -7.4962e-17, -5.2822e-06, -7.4497e-18,  4.6417e-10, -8.7412e-08,\n",
       "              7.8282e-17,  4.8721e-18, -2.5213e-11,  2.2956e-15, -3.9910e-05,\n",
       "              4.8861e-06, -1.1723e-13, -4.0530e-38, -1.0104e-06, -3.1831e-04,\n",
       "              9.2181e-08,  4.1901e-04,  6.8560e-05, -4.7207e-05,  6.9607e-07,\n",
       "             -1.1070e-04, -1.4897e-11, -1.2518e-10,  4.6952e-09,  3.6715e-32,\n",
       "             -4.0807e-07, -8.7285e-05, -9.0055e-08,  7.6701e-08,  4.4546e-17,\n",
       "             -1.9300e-04, -1.5440e-07, -4.5282e-04,  1.4122e-05, -3.0531e-05,\n",
       "              9.3426e-05, -5.1962e-05,  3.0381e-08, -4.6667e-04, -2.6992e-11,\n",
       "              1.5735e-04], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([2.8066e-07, 3.6220e-07, 1.1280e-06, 2.8822e-07, 5.3464e-07, 2.4062e-07,\n",
       "             5.7193e-07, 9.3090e-08, 1.2558e-06, 1.7802e-07, 3.0346e-07, 2.0588e-06,\n",
       "             1.5692e-07, 4.2970e-07, 4.6072e-07, 2.5235e-07, 7.7932e-08, 1.7249e-07,\n",
       "             2.0469e-08, 2.1938e-06, 5.2812e-07, 2.1727e-07, 1.2760e-07, 9.6364e-07,\n",
       "             3.8619e-07, 4.5994e-07, 5.9372e-08, 3.9217e-07, 1.6967e-07, 1.5164e-06,\n",
       "             5.3036e-07, 2.5898e-07, 3.8720e-07, 1.3185e-06, 3.3171e-07, 3.8768e-06,\n",
       "             4.3872e-07, 2.8481e-07, 1.0887e-06, 1.5735e-07, 8.4891e-08, 1.8884e-07,\n",
       "             1.6296e-07, 4.0737e-06, 7.2739e-07, 2.8520e-09, 9.3019e-07, 8.9006e-08,\n",
       "             6.8802e-07, 1.5079e-06, 1.1660e-06, 1.3718e-07, 3.9291e-07, 6.4535e-07,\n",
       "             2.8650e-07, 1.3044e-07, 3.8170e-07, 9.8439e-08, 4.0023e-07, 3.1214e-07,\n",
       "             6.0808e-07, 1.1483e-06, 7.1249e-08, 2.9197e-09, 9.1507e-07, 5.5795e-08,\n",
       "             2.2075e-06, 1.4826e-07, 3.9224e-07, 2.4652e-07, 3.4643e-07, 4.5370e-09,\n",
       "             5.0831e-07, 9.8617e-07, 6.9283e-07, 2.6881e-06, 1.6948e-07, 6.0970e-07,\n",
       "             2.3605e-07, 1.5781e-06, 1.8684e-07, 4.7781e-07, 1.0430e-07, 6.4565e-08,\n",
       "             1.1761e-06, 7.6558e-07, 3.3479e-07, 2.4031e-07, 2.9695e-08, 5.1426e-07,\n",
       "             1.2073e-06, 4.8297e-07, 3.9165e-08, 2.0358e-06, 3.4728e-06, 9.9903e-08,\n",
       "             4.7368e-07, 6.9274e-07, 5.6354e-07, 4.3691e-07, 1.2534e-06, 1.0867e-06,\n",
       "             2.5658e-07, 3.6009e-08, 2.2296e-07, 5.2692e-07, 4.5704e-07, 5.0430e-07,\n",
       "             3.5148e-07, 8.1382e-08, 5.4562e-07, 1.0430e-07, 2.6745e-06, 4.4812e-07,\n",
       "             2.6042e-08, 1.4891e-08, 1.6894e-06, 1.3311e-07, 5.4809e-07, 1.8716e-07,\n",
       "             4.7387e-07, 4.1272e-07, 2.2950e-07, 2.0687e-06, 2.1879e-07, 4.5922e-07,\n",
       "             2.8723e-07, 5.7718e-07, 4.0443e-08, 1.7664e-07, 2.9455e-07, 6.5295e-07,\n",
       "             1.0010e-06, 1.2869e-06, 5.5667e-07, 1.1527e-06, 2.4714e-07, 4.2284e-07,\n",
       "             9.7157e-08, 3.8541e-07, 2.7627e-07, 4.3117e-07, 1.2342e-07, 1.9433e-07,\n",
       "             6.4088e-07, 5.5459e-09, 2.8180e-07, 1.6493e-06, 2.3889e-07, 1.1550e-06,\n",
       "             1.3365e-07, 1.2554e-07, 2.9777e-08, 1.8828e-06, 1.1261e-08, 2.8607e-08,\n",
       "             1.2438e-07, 1.1182e-06, 1.0578e-08, 1.9753e-07, 4.7000e-07, 3.0357e-07,\n",
       "             2.9739e-07, 6.2095e-07, 3.3994e-07, 5.0529e-07, 6.0962e-07, 2.1078e-08,\n",
       "             1.3418e-06, 1.3456e-07, 3.5788e-07, 8.1560e-07, 1.7216e-07, 1.9139e-06,\n",
       "             1.9322e-07, 3.7228e-07, 3.8216e-07, 8.7594e-08, 3.2543e-07, 5.1562e-08,\n",
       "             9.5921e-07, 9.3592e-08, 6.5871e-07, 5.5352e-07, 3.0710e-07, 3.3947e-07,\n",
       "             1.5040e-07, 3.6125e-07, 1.4235e-06, 2.1145e-06, 1.4890e-09, 5.5908e-08,\n",
       "             3.1244e-07, 7.7628e-07, 8.2355e-07, 8.3358e-07, 5.0639e-07, 2.5006e-06,\n",
       "             4.1968e-07, 5.9884e-07, 1.6445e-07, 1.9387e-07, 2.1847e-07, 1.6981e-06,\n",
       "             1.9361e-06, 6.7338e-07, 5.4867e-07, 2.5664e-07, 9.8863e-07, 1.1559e-07,\n",
       "             8.5042e-07, 5.3852e-08, 1.5343e-09, 1.9647e-07, 7.9963e-07, 6.3430e-08,\n",
       "             3.6980e-07, 5.8042e-08, 2.5166e-07, 3.6376e-07, 1.8407e-07, 1.3392e-07,\n",
       "             1.5426e-07, 1.3686e-07, 1.5877e-06, 1.5149e-07, 4.9229e-08, 4.1135e-08,\n",
       "             3.4548e-07, 1.2141e-06, 1.6277e-07, 1.1487e-06, 8.3694e-07, 9.4051e-07,\n",
       "             2.3762e-07, 2.9608e-07, 1.6438e-07, 1.1026e-07, 1.8113e-07, 4.9103e-08,\n",
       "             2.7137e-08, 6.9573e-07, 2.8385e-07, 2.4249e-07, 1.7927e-07, 6.0729e-07,\n",
       "             7.0897e-08, 3.2175e-06, 3.9175e-07, 1.1714e-06, 3.0305e-07, 9.2827e-07,\n",
       "             2.3767e-07, 4.1259e-06, 6.8739e-08, 4.7816e-07], device='cuda:0')},\n",
       "    4: {'step': tensor(1290.),\n",
       "     'exp_avg': tensor([[ 3.2781e-08,  1.1633e-06,  2.7407e-05,  ...,  4.8693e-04,\n",
       "               1.7434e-14,  1.7366e-04],\n",
       "             [ 2.1680e-06,  9.7178e-05,  3.6801e-05,  ...,  2.0126e-03,\n",
       "               1.7225e-14,  3.0777e-04],\n",
       "             [ 2.4365e-06,  6.9052e-05, -2.5042e-04,  ...,  7.1069e-04,\n",
       "               6.4311e-14,  4.3731e-04],\n",
       "             ...,\n",
       "             [ 6.4113e-07,  3.6986e-06,  6.9064e-05,  ...,  1.0265e-03,\n",
       "               6.2548e-12, -8.5693e-04],\n",
       "             [-9.0775e-06,  4.2852e-05,  2.1280e-04,  ..., -6.9875e-03,\n",
       "               5.6596e-16, -1.5016e-03],\n",
       "             [ 1.8972e-07,  9.5513e-05,  3.4215e-05,  ...,  4.1325e-03,\n",
       "               1.1729e-10,  5.3247e-04]], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([[4.0730e-06, 2.3716e-06, 6.5108e-05,  ..., 2.6825e-05, 8.5421e-07,\n",
       "              9.0940e-05],\n",
       "             [2.7964e-06, 4.1435e-06, 2.1305e-05,  ..., 5.6294e-05, 5.1357e-07,\n",
       "              4.5054e-05],\n",
       "             [9.9904e-06, 8.2540e-06, 1.2760e-04,  ..., 6.9946e-05, 1.5193e-06,\n",
       "              3.9065e-05],\n",
       "             ...,\n",
       "             [9.9327e-07, 6.1618e-07, 4.0822e-05,  ..., 2.0390e-04, 6.1991e-07,\n",
       "              7.8569e-05],\n",
       "             [5.0045e-05, 2.8690e-05, 3.1034e-04,  ..., 1.3438e-03, 8.8036e-06,\n",
       "              7.7731e-05],\n",
       "             [1.4635e-05, 1.0502e-05, 7.3658e-05,  ..., 3.0850e-03, 5.0149e-07,\n",
       "              3.5348e-04]], device='cuda:0')},\n",
       "    5: {'step': tensor(1290.),\n",
       "     'exp_avg': tensor([-0.0010,  0.0034,  0.0017,  0.0008,  0.0023, -0.0021, -0.0024,  0.0006,\n",
       "             -0.0030, -0.0004], device='cuda:0'),\n",
       "     'exp_avg_sq': tensor([6.0606e-05, 5.1674e-05, 7.5424e-05, 9.9189e-05, 7.8793e-05, 8.0648e-05,\n",
       "             5.0309e-05, 6.5323e-05, 1.1923e-04, 1.1661e-04], device='cuda:0')}},\n",
       "   'param_groups': [{'lr': 0.01,\n",
       "     'betas': (0.9, 0.999),\n",
       "     'eps': 1e-08,\n",
       "     'weight_decay': 0,\n",
       "     'amsgrad': False,\n",
       "     'maximize': False,\n",
       "     'foreach': None,\n",
       "     'capturable': False,\n",
       "     'differentiable': False,\n",
       "     'fused': None,\n",
       "     'params': [0, 1, 2, 3, 4, 5]}]}],\n",
       " 'lr_schedulers': [{'factor': 0.2,\n",
       "   'min_lrs': [1e-06],\n",
       "   'patience': 2,\n",
       "   'verbose': True,\n",
       "   'cooldown': 0,\n",
       "   'cooldown_counter': 0,\n",
       "   'mode': 'min',\n",
       "   'threshold': 0.0001,\n",
       "   'threshold_mode': 'rel',\n",
       "   'best': 0.19499154388904572,\n",
       "   'num_bad_epochs': 0,\n",
       "   'mode_worse': inf,\n",
       "   'eps': 1e-08,\n",
       "   'last_epoch': 2,\n",
       "   '_last_lr': [0.01]}]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.load(\"epoch=2-step=1290.ckpt\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jy_zhang/anaconda3/envs/pytorch/lib/python3.11/site-packages/onnx/__init__.py:108: RuntimeWarning: Unexpected end-group tag: Not all data was converted\n",
      "  decoded = cast(Optional[int], proto.ParseFromString(s))\n"
     ]
    },
    {
     "ename": "DecodeError",
     "evalue": "Protobuf decoding consumed too few bytes: 5 out of 2680131",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDecodeError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/jy_zhang/code/test1.ipynb 单元格 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jy_zhang/code/test1.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39monnx\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jy_zhang/code/test1.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m onnx\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mmodel2.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/onnx/__init__.py:134\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(f, format, load_external_data)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[39mLoads a serialized ModelProto into memory\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mload_external_data is true if the external data under the same directory of the model and load the external data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39m    Loaded in-memory ModelProto\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    133\u001b[0m s \u001b[39m=\u001b[39m _load_bytes(f)\n\u001b[0;32m--> 134\u001b[0m model \u001b[39m=\u001b[39m load_model_from_string(s, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m load_external_data:\n\u001b[1;32m    137\u001b[0m     model_filepath \u001b[39m=\u001b[39m _get_file_path(f)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/onnx/__init__.py:171\u001b[0m, in \u001b[0;36mload_model_from_string\u001b[0;34m(s, format)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model_from_string\u001b[39m(s: \u001b[39mbytes\u001b[39m, \u001b[39mformat\u001b[39m: Optional[Any] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ModelProto:\n\u001b[1;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    Loads a binary string (bytes) that contains serialized ModelProto\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m        Loaded in-memory ModelProto\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     \u001b[39mreturn\u001b[39;00m _deserialize(s, ModelProto())\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/onnx/__init__.py:110\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(s, proto)\u001b[0m\n\u001b[1;32m    108\u001b[0m decoded \u001b[39m=\u001b[39m cast(Optional[\u001b[39mint\u001b[39m], proto\u001b[39m.\u001b[39mParseFromString(s))\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m decoded \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m decoded \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 110\u001b[0m     \u001b[39mraise\u001b[39;00m google\u001b[39m.\u001b[39mprotobuf\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mDecodeError(\n\u001b[1;32m    111\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProtobuf decoding consumed too few bytes: \u001b[39m\u001b[39m{\u001b[39;00mdecoded\u001b[39m}\u001b[39;00m\u001b[39m out of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(s)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m proto\n",
      "\u001b[0;31mDecodeError\u001b[0m: Protobuf decoding consumed too few bytes: 5 out of 2680131"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "data = onnx.load(\"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('11.8', 8700)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.version.cuda, torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 2, 'NVIDIA GeForce RTX 4080')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available(), torch.cuda.device_count(), torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 15:57:42.746185: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-23 15:57:42.765074: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-23 15:57:42.765093: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-23 15:57:42.765106: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-23 15:57:42.769017: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-23 15:57:43.164151: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_41700/2360224332.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 15:57:43.535937: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2023-11-23 15:57:43.535951: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: dd-System-Product-Name\n",
      "2023-11-23 15:57:43.535953: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: dd-System-Product-Name\n",
      "2023-11-23 15:57:43.535988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 535.129.3\n",
      "2023-11-23 15:57:43.535994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 535.129.3\n",
      "2023-11-23 15:57:43.535996: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 535.129.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(False, [])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available(),tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/jy_zhang/code/test1.ipynb 单元格 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jy_zhang/code/test1.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model_1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39m\"\u001b[39m\u001b[39mmodel2.pth\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/saving/saving_api.py:262\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    255\u001b[0m         filepath,\n\u001b[1;32m    256\u001b[0m         custom_objects\u001b[39m=\u001b[39mcustom_objects,\n\u001b[1;32m    257\u001b[0m         \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m,\n\u001b[1;32m    258\u001b[0m         safe_mode\u001b[39m=\u001b[39msafe_mode,\n\u001b[1;32m    259\u001b[0m     )\n\u001b[1;32m    261\u001b[0m \u001b[39m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m \u001b[39mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[39m.\u001b[39mload_model(\n\u001b[1;32m    263\u001b[0m     filepath, custom_objects\u001b[39m=\u001b[39mcustom_objects, \u001b[39mcompile\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcompile\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    264\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/h5py/_hl/files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    558\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    560\u001b[0m                      alignment_threshold\u001b[39m=\u001b[39malignment_threshold,\n\u001b[1;32m    561\u001b[0m                      alignment_interval\u001b[39m=\u001b[39malignment_interval,\n\u001b[1;32m    562\u001b[0m                      meta_block_size\u001b[39m=\u001b[39mmeta_block_size,\n\u001b[1;32m    563\u001b[0m                      \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    564\u001b[0m     fcpl \u001b[39m=\u001b[39m make_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[1;32m    565\u001b[0m                      fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold,\n\u001b[1;32m    566\u001b[0m                      fs_page_size\u001b[39m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 567\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size, fapl, fcpl, swmr\u001b[39m=\u001b[39mswmr)\n\u001b[1;32m    569\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    570\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[0;32m~/anaconda3/envs/tf/lib/python3.11/site-packages/h5py/_hl/files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[1;32m    230\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[0;32m--> 231\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, flags, fapl\u001b[39m=\u001b[39mfapl)\n\u001b[1;32m    232\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    233\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "model_1 = tf.keras.models.load_model(\"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "with open(\"model2.pth\", \"rb\") as f:\n",
    "    data = f.read()\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-24 15:46:18.760753: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-24 15:46:18.885845: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-24 15:46:18.885866: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-24 15:46:18.886581: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-24 15:46:18.943393: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-24 15:46:19.476411: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is using: PyTorch\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "def detect_framework(model_path):\n",
    "    # 尝试使用 PyTorch 加载模型\n",
    "    try:\n",
    "        torch.load(model_path)\n",
    "        return \"PyTorch\"\n",
    "    except (torch.nn.modules.module.ModuleAttributeError, RuntimeError, pickle.UnpicklingError):\n",
    "        pass\n",
    "\n",
    "    # 尝试使用 TensorFlow 加载模型\n",
    "    try:\n",
    "        tf.keras.models.load_model(model_path)\n",
    "        return \"TensorFlow\"\n",
    "    except (ValueError, AttributeError, OSError, EOFError):\n",
    "        pass\n",
    "\n",
    "    # 尝试使用 joblib 或者 pickle 加载模型\n",
    "    try:\n",
    "        with open(model_path, 'rb') as file:\n",
    "            joblib.load(file)\n",
    "        return \"Scikit-learn\"\n",
    "    except (ValueError, AttributeError, OSError, EOFError, pickle.UnpicklingError):\n",
    "        pass\n",
    "\n",
    "    return \"Unknown\"\n",
    "\n",
    "# 使用 detect_framework 函数判断模型类型\n",
    "model_path = 'model2.pth'  # 替换成你的模型文件路径\n",
    "framework = detect_framework(model_path)\n",
    "print(f\"The model is using: {framework}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设有一个模型并保存它\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(10, 20)\n",
    "        self.layer2 = nn.Linear(20, 30)\n",
    "        self.layer3 = nn.Linear(30, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "\n",
    "# 创建模型实例并保存\n",
    "model = MyModel()\n",
    "#torch.save(model.state_dict(), 'my_model.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model, '../my_model.pth')\n",
    "isinstance(model,torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = torch.load('../my_model.pt')\n",
    "isinstance(model_test, dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function save in module torch.serialization:\n",
      "\n",
      "save(obj: object, f: Union[str, os.PathLike, BinaryIO, IO[bytes]], pickle_module: Any = <module 'pickle' from '/home/jy_zhang/anaconda3/envs/env_a/lib/python3.11/pickle.py'>, pickle_protocol: int = 2, _use_new_zipfile_serialization: bool = True) -> None\n",
      "    save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\n",
      "    \n",
      "    Saves an object to a disk file.\n",
      "    \n",
      "    See also: :ref:`saving-loading-tensors`\n",
      "    \n",
      "    Args:\n",
      "        obj: saved object\n",
      "        f: a file-like object (has to implement write and flush) or a string or\n",
      "           os.PathLike object containing a file name\n",
      "        pickle_module: module used for pickling metadata and objects\n",
      "        pickle_protocol: can be specified to override the default protocol\n",
      "    \n",
      "    .. note::\n",
      "        A common PyTorch convention is to save tensors using .pt file extension.\n",
      "    \n",
      "    .. note::\n",
      "        PyTorch preserves storage sharing across serialization. See\n",
      "        :ref:`preserve-storage-sharing` for more details.\n",
      "    \n",
      "    .. note::\n",
      "        The 1.6 release of PyTorch switched ``torch.save`` to use a new\n",
      "        zipfile-based file format. ``torch.load`` still retains the ability to\n",
      "        load files in the old format. If for any reason you want ``torch.save``\n",
      "        to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.\n",
      "    \n",
      "    Example:\n",
      "        >>> # xdoctest: +SKIP(\"makes cwd dirty\")\n",
      "        >>> # Save to file\n",
      "        >>> x = torch.tensor([0, 1, 2, 3, 4])\n",
      "        >>> torch.save(x, 'tensor.pt')\n",
      "        >>> # Save to io.BytesIO buffer\n",
      "        >>> buffer = io.BytesIO()\n",
      "        >>> torch.save(x, buffer)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(torch.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpmb81wzc2\n",
      "test1\n",
      "test2\n"
     ]
    }
   ],
   "source": [
    "from mlflow.utils.file_utils import TempDir\n",
    "import os\n",
    "local_path = '/home/jy_zhang/code'\n",
    "\n",
    "# dir_name = os.path.basename(os.path.normpath(local_path))\n",
    "# dir_name, os.path.normpath(local_path)\n",
    "with TempDir() as tmp:\n",
    "    print(os.path.abspath(tmp.path()))\n",
    "    print(\"test1\")\n",
    "    print('test2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pycode'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_path = '../pycode'\n",
    "dir_name = os.path.basename(os.path.normpath(local_path))\n",
    "dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jy_zhang/anaconda3/envs/env_a/lib/python3.11/abc.py'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import abc\n",
    "abc.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlflow-artifacts:/295829569244001181/14c2eaa75dd141b482ae9721a435e3f8'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "a = \"mlflow-artifacts:/295829569244001181/14c2eaa75dd141b482ae9721a435e3f8/artifacts\"\n",
    "\n",
    "a = os.path.dirname(a)\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdir = {'a':1, 'b':2}\n",
    "testdir.get('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isdir('../pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jy_zhang/code/pycode/test1.ipynb'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.abspath('./test1.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载已保存的模型\n",
    "loaded_model_state = torch.load('my_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.3043, -0.1405,  0.1828, -0.1613, -0.1103, -0.1103, -0.0647,  0.2471,\n",
       "                       -0.0250, -0.0297],\n",
       "                      [ 0.1087, -0.2892,  0.0645,  0.1057,  0.1449,  0.2218, -0.1186,  0.2042,\n",
       "                       -0.2256,  0.1482],\n",
       "                      [-0.1018,  0.0414,  0.1842, -0.2359,  0.2814, -0.2790, -0.2442,  0.1207,\n",
       "                        0.0629,  0.1218],\n",
       "                      [-0.1345,  0.0481,  0.0255, -0.0806, -0.1713, -0.2509, -0.0230, -0.0381,\n",
       "                       -0.2611, -0.2364],\n",
       "                      [-0.0640,  0.1308, -0.2923, -0.1276,  0.2030, -0.0780, -0.0863, -0.0105,\n",
       "                       -0.2465, -0.3130],\n",
       "                      [ 0.2388, -0.2534,  0.2532, -0.3075, -0.0874,  0.1945, -0.0466, -0.0656,\n",
       "                       -0.0473, -0.2508],\n",
       "                      [-0.2468,  0.1568,  0.1116, -0.0136,  0.1595, -0.1011,  0.1885,  0.1895,\n",
       "                        0.3063, -0.0671],\n",
       "                      [-0.1310, -0.0613,  0.2123,  0.2174,  0.0429,  0.2266,  0.0005, -0.1755,\n",
       "                       -0.0132,  0.2872],\n",
       "                      [ 0.1597, -0.2958, -0.1711,  0.2935,  0.2341, -0.0812,  0.2751,  0.2193,\n",
       "                        0.2476, -0.1913],\n",
       "                      [ 0.0623, -0.1759, -0.2225, -0.2161, -0.0792,  0.0907,  0.2708, -0.2632,\n",
       "                        0.0271, -0.2851],\n",
       "                      [-0.2871, -0.1971, -0.0785, -0.2908,  0.1201, -0.1728, -0.1260,  0.2470,\n",
       "                        0.0697, -0.2519],\n",
       "                      [-0.2424, -0.1421,  0.0051,  0.1878, -0.2581, -0.1477, -0.0416, -0.0287,\n",
       "                        0.0948,  0.3076],\n",
       "                      [-0.0736, -0.0987, -0.1207,  0.2934, -0.2641, -0.0367,  0.1581,  0.2564,\n",
       "                        0.2768,  0.1311],\n",
       "                      [ 0.1874, -0.0815,  0.2233, -0.2568,  0.1304,  0.2970, -0.1437,  0.2480,\n",
       "                       -0.2154, -0.0932],\n",
       "                      [ 0.2730, -0.1168,  0.2123, -0.3087,  0.0797,  0.2368,  0.1304,  0.0417,\n",
       "                        0.1831, -0.1695],\n",
       "                      [-0.0800,  0.0288, -0.1927,  0.2499, -0.0104, -0.0199,  0.3108, -0.1588,\n",
       "                       -0.0020, -0.1335],\n",
       "                      [ 0.1200,  0.1681, -0.2190,  0.0144,  0.1884, -0.0726,  0.1208,  0.0307,\n",
       "                        0.2287, -0.0079],\n",
       "                      [-0.0418,  0.2642, -0.0756, -0.1534, -0.1416,  0.2862,  0.0018,  0.2052,\n",
       "                        0.1246, -0.1760],\n",
       "                      [ 0.0795,  0.1534,  0.1753,  0.2667, -0.2928, -0.2747, -0.0737, -0.1750,\n",
       "                       -0.2223, -0.0699],\n",
       "                      [-0.0593, -0.2365, -0.2682, -0.2438, -0.2597, -0.1585, -0.0648, -0.1763,\n",
       "                        0.2391,  0.0148]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.2464,  0.1146, -0.0766, -0.1257, -0.0492, -0.2676, -0.1328,  0.2497,\n",
       "                      -0.2535, -0.1606, -0.0786,  0.0381,  0.0216, -0.2997, -0.0164,  0.1954,\n",
       "                      -0.0863,  0.0064, -0.0329,  0.0881])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 1.2627e-01,  2.2189e-01,  1.1112e-01, -1.5570e-01, -4.8542e-02,\n",
       "                        8.4769e-02, -2.1164e-01,  2.1957e-01, -2.1178e-01, -1.8703e-01,\n",
       "                        4.3175e-02,  1.7754e-01,  1.1732e-01,  3.5934e-02,  1.5101e-02,\n",
       "                       -2.1207e-01, -4.4739e-02, -7.6392e-02, -4.3298e-02,  1.9029e-01],\n",
       "                      [ 6.8214e-02,  2.1261e-01,  3.6947e-02,  9.2854e-02, -9.5580e-02,\n",
       "                        1.8530e-01,  9.3286e-02,  1.2177e-01,  1.9131e-01, -1.8038e-01,\n",
       "                        2.9181e-02, -5.2813e-02,  1.5744e-01, -1.2679e-02,  1.2097e-01,\n",
       "                        1.0470e-01,  3.2376e-02, -8.4969e-02,  8.2885e-02, -1.0146e-01],\n",
       "                      [ 7.4613e-02,  1.1598e-01,  8.2787e-02, -2.1923e-01,  1.6968e-01,\n",
       "                       -1.0874e-01, -1.3013e-01,  1.7536e-01, -1.9208e-01, -1.3591e-01,\n",
       "                        5.2652e-02, -4.5504e-02,  2.2298e-01,  1.9997e-01,  4.3312e-02,\n",
       "                        5.8314e-02,  1.4259e-01, -5.6239e-02, -9.2740e-02, -2.3311e-02],\n",
       "                      [-4.3734e-02,  4.0986e-02, -2.0868e-01,  2.0155e-01, -3.2980e-02,\n",
       "                       -5.3518e-02,  2.0563e-01, -5.0777e-02,  3.7773e-02,  1.0279e-01,\n",
       "                       -9.2190e-02, -9.3686e-02, -1.2838e-01,  1.7589e-01, -2.4636e-02,\n",
       "                       -1.7875e-01,  1.2945e-01, -1.0098e-02, -2.0484e-01,  1.9401e-01],\n",
       "                      [ 8.9313e-03, -1.0424e-01,  1.9656e-01,  1.3335e-01, -1.8362e-01,\n",
       "                       -1.7010e-01,  5.3391e-02,  8.1706e-02,  5.6856e-02, -1.9184e-01,\n",
       "                       -6.0178e-02,  8.5864e-02, -1.1078e-01,  1.5559e-01,  9.6113e-02,\n",
       "                       -6.1419e-02,  2.1295e-02, -6.9854e-02,  4.5333e-02,  1.6211e-01],\n",
       "                      [ 1.5048e-01, -1.6515e-01, -2.1882e-01, -1.2502e-01, -3.4249e-02,\n",
       "                        4.0294e-02, -1.8844e-01,  2.0945e-01, -2.0796e-01, -7.0833e-03,\n",
       "                       -1.9144e-01, -7.5077e-02,  2.0713e-01,  2.1149e-01, -1.0310e-01,\n",
       "                       -7.5997e-03,  1.9326e-01, -6.0069e-02,  1.5278e-01, -1.5707e-01],\n",
       "                      [ 3.8676e-02,  6.4843e-02,  1.7767e-02, -1.8773e-01, -1.3430e-01,\n",
       "                       -2.1236e-01, -2.1178e-02, -2.7322e-02,  1.4960e-03,  1.2941e-01,\n",
       "                       -1.3942e-01, -6.1069e-02,  1.6364e-02,  4.4624e-02, -1.0013e-01,\n",
       "                        2.0560e-01,  6.3615e-02, -1.0902e-01, -4.1851e-02,  1.6281e-01],\n",
       "                      [ 2.7842e-02, -2.1612e-01,  1.0246e-01,  5.1756e-02,  1.2372e-01,\n",
       "                        1.9037e-01, -1.5693e-01,  2.0969e-01,  1.2704e-01, -1.2746e-01,\n",
       "                       -1.2986e-01,  2.2257e-01,  8.1657e-02, -1.8497e-01,  9.7611e-02,\n",
       "                       -3.1400e-03,  5.8986e-02,  1.1808e-01,  2.1401e-01, -3.4539e-02],\n",
       "                      [-1.4436e-01,  6.6000e-02, -1.8757e-01,  6.5860e-02,  8.0064e-02,\n",
       "                        8.1243e-02,  1.9538e-01, -2.1805e-01, -5.9540e-02,  7.5290e-02,\n",
       "                       -1.3154e-01, -4.2055e-02,  2.9019e-02, -1.1675e-01, -1.0761e-01,\n",
       "                       -1.4887e-02, -1.7025e-02,  8.6265e-02, -1.2164e-02, -5.3294e-02],\n",
       "                      [-2.0632e-01, -1.3057e-01, -2.1677e-01, -1.1842e-01,  1.1781e-02,\n",
       "                       -3.0722e-03,  1.5641e-01,  6.2656e-02, -1.7530e-01, -1.5906e-01,\n",
       "                        2.1968e-01, -4.0365e-02, -1.6817e-01,  6.4646e-02, -1.8539e-02,\n",
       "                       -4.3749e-02,  6.1747e-02, -2.6904e-03, -5.4039e-02,  2.1309e-01],\n",
       "                      [ 8.9345e-02,  1.2757e-01,  1.2703e-01, -1.5688e-01, -1.6211e-01,\n",
       "                       -1.7506e-03, -1.9788e-01, -1.5197e-01, -9.6741e-03,  6.5480e-02,\n",
       "                       -1.6944e-01,  8.3370e-02,  1.6858e-01,  2.0947e-01, -3.3187e-02,\n",
       "                       -1.7084e-02, -1.4878e-01,  5.6851e-02,  9.9655e-02, -1.4209e-02],\n",
       "                      [-8.5748e-03, -8.4113e-02, -2.5664e-02, -1.0187e-02, -1.7911e-01,\n",
       "                        1.2735e-01, -9.7017e-02,  1.1162e-01,  9.8900e-02, -1.7405e-01,\n",
       "                       -1.6987e-01,  7.1528e-02, -7.5878e-02, -2.1291e-01, -7.6915e-02,\n",
       "                       -1.2683e-01,  8.4714e-02, -9.4461e-03, -1.0815e-01,  5.1438e-02],\n",
       "                      [ 8.9400e-02, -2.2062e-02, -1.1327e-01,  1.3362e-01, -4.1498e-02,\n",
       "                       -6.6469e-03, -4.9049e-02,  1.4740e-01,  1.5805e-02, -2.8627e-02,\n",
       "                       -1.0992e-01,  1.6348e-01,  1.3842e-01,  1.1140e-01, -8.3107e-02,\n",
       "                        5.7800e-02,  2.0924e-01, -6.7385e-02, -6.8806e-02,  2.0515e-01],\n",
       "                      [-2.0512e-01, -2.9330e-02, -1.1969e-01,  1.5765e-01, -5.8953e-02,\n",
       "                        1.3059e-01,  1.9347e-01,  6.6114e-02,  2.0823e-01, -6.7303e-02,\n",
       "                       -1.5523e-02,  6.1942e-02,  8.0640e-02, -1.4831e-01, -1.7031e-02,\n",
       "                       -1.3413e-02,  4.9968e-02,  4.0673e-02, -1.5521e-01,  1.7534e-01],\n",
       "                      [-1.4266e-01,  1.9708e-01,  5.7336e-02,  1.3580e-01, -1.9093e-01,\n",
       "                        4.5385e-02,  1.2343e-01, -1.1069e-01, -2.0367e-01, -1.7843e-01,\n",
       "                       -1.8635e-01,  9.4840e-02,  8.6661e-02, -3.8228e-02, -1.5516e-01,\n",
       "                       -5.7098e-02, -2.5244e-02, -9.9784e-02, -1.8173e-01,  1.8835e-02],\n",
       "                      [ 1.5627e-01, -1.3789e-01,  6.5454e-02,  1.5655e-01,  1.9287e-02,\n",
       "                       -1.9369e-01, -1.6396e-01, -5.4534e-02, -1.9533e-02, -1.7593e-01,\n",
       "                        1.6194e-01, -1.6811e-01,  1.9198e-01,  2.2024e-01, -9.6717e-02,\n",
       "                        1.4273e-01,  2.1315e-01,  2.9920e-02, -1.9860e-01,  2.0542e-02],\n",
       "                      [-3.2931e-03, -1.0572e-01, -1.3690e-01, -2.7332e-02, -2.0774e-03,\n",
       "                       -7.8788e-02,  1.9073e-01, -1.1732e-01,  1.1944e-01, -1.0359e-01,\n",
       "                       -3.6339e-02, -7.4552e-02,  5.4640e-02,  2.2218e-01, -9.1836e-02,\n",
       "                       -3.5816e-03,  9.5056e-02,  8.9077e-03, -6.5322e-02, -1.2120e-04],\n",
       "                      [-1.6877e-01,  5.2964e-02,  1.8614e-01, -1.2856e-01,  1.7209e-01,\n",
       "                        1.4421e-01,  1.6598e-01, -1.4221e-01,  1.9606e-01,  7.4432e-02,\n",
       "                        1.4943e-01, -1.2270e-01,  1.6490e-01, -8.2606e-02,  1.1438e-01,\n",
       "                        1.9126e-01, -3.9041e-02, -1.8318e-01,  7.6002e-02,  1.4276e-01],\n",
       "                      [ 8.6861e-02,  2.2355e-01, -1.9367e-01,  1.4286e-03, -2.1611e-01,\n",
       "                       -2.1445e-01, -3.3513e-02, -9.2587e-03, -2.1000e-01,  5.4115e-02,\n",
       "                        1.5250e-01,  1.8282e-01,  2.1722e-01, -1.7488e-01,  1.7458e-01,\n",
       "                       -4.1364e-02, -5.3315e-02,  4.5019e-02,  2.1466e-01,  9.4334e-03],\n",
       "                      [ 1.0050e-01, -6.9329e-02, -1.1442e-02,  1.9282e-01, -2.0574e-01,\n",
       "                       -8.6316e-03, -2.1197e-01,  1.5350e-01,  6.8640e-02, -2.2463e-02,\n",
       "                       -1.2720e-01, -4.7129e-02,  5.0073e-02, -1.8309e-01,  1.7512e-01,\n",
       "                        5.0897e-02, -1.1390e-01,  1.2511e-01,  2.6232e-03,  8.2565e-02],\n",
       "                      [-6.1875e-02,  1.4388e-01, -5.8590e-02, -9.9928e-03,  1.9122e-01,\n",
       "                       -1.0542e-01,  1.4550e-01,  1.9338e-01,  1.4411e-01,  1.2009e-01,\n",
       "                       -2.1203e-01, -1.6971e-01, -8.8512e-02,  1.5368e-01,  1.6920e-01,\n",
       "                        1.2762e-02,  8.6274e-02, -1.6364e-01, -8.9030e-02,  3.3485e-02],\n",
       "                      [ 1.2844e-01, -1.5532e-01, -1.9886e-01, -3.5452e-02,  1.4786e-02,\n",
       "                        5.9721e-02, -1.7453e-01,  1.9982e-01,  1.8928e-01, -1.2088e-01,\n",
       "                        1.7136e-01, -2.7363e-02,  9.3507e-02, -5.0892e-02, -1.1715e-01,\n",
       "                        2.2179e-01, -1.6264e-01, -9.4717e-02,  2.5327e-02, -1.2192e-01],\n",
       "                      [-1.6258e-02, -1.4881e-01, -1.5315e-01, -3.1142e-02, -1.6877e-01,\n",
       "                       -1.8694e-01,  2.1042e-02, -1.9639e-03,  9.2836e-02,  5.7801e-02,\n",
       "                       -1.3253e-02,  4.7403e-02,  1.7105e-01, -1.8528e-01,  1.9437e-01,\n",
       "                       -1.8547e-01,  4.6922e-03, -1.0312e-01, -1.8628e-01,  1.1654e-01],\n",
       "                      [-8.0524e-02, -3.0574e-02, -1.8943e-01, -6.6947e-02, -1.0113e-01,\n",
       "                        1.3231e-01, -1.2220e-01,  1.4655e-01,  2.0959e-01,  1.0099e-02,\n",
       "                       -1.6586e-01,  1.4843e-01, -7.4506e-03,  1.2790e-01,  9.2980e-02,\n",
       "                        2.1521e-01,  1.7845e-01, -1.0301e-02,  4.1050e-02, -2.0542e-01],\n",
       "                      [-1.2609e-01, -1.6210e-01, -4.7657e-02, -1.8580e-01, -1.4446e-01,\n",
       "                        2.6588e-02,  8.4436e-02,  1.6409e-01, -7.3092e-02, -1.7015e-01,\n",
       "                       -3.0041e-02,  4.3747e-02, -1.0909e-01, -1.6607e-01,  7.1276e-02,\n",
       "                        5.4841e-02,  1.6402e-01,  1.3860e-02, -1.6104e-02, -1.2348e-01],\n",
       "                      [ 1.3954e-01, -1.3785e-01, -2.8819e-02, -1.9256e-02, -3.3588e-02,\n",
       "                        1.8573e-01,  4.0425e-03, -1.0994e-01,  3.4550e-02, -2.1838e-01,\n",
       "                       -2.0672e-01,  1.0894e-01,  3.8112e-02,  2.0199e-01, -1.9403e-02,\n",
       "                        1.3590e-01,  6.0857e-02,  4.9084e-02, -1.8994e-01,  1.3509e-01],\n",
       "                      [ 1.2221e-01,  5.4719e-02, -3.3005e-02, -1.4725e-01,  1.4723e-01,\n",
       "                       -1.4027e-01,  4.8757e-02,  1.2916e-02, -2.0748e-01,  2.3584e-03,\n",
       "                       -2.0183e-02,  1.7798e-01,  1.0589e-01, -1.8436e-01, -2.5054e-02,\n",
       "                        1.4085e-01, -3.6055e-03,  3.4574e-02, -8.4805e-02, -9.8966e-02],\n",
       "                      [-1.3662e-01, -3.5971e-02, -2.0877e-01,  1.7761e-01,  2.1783e-01,\n",
       "                        4.1259e-02, -1.4086e-01, -4.3967e-02, -9.3567e-03, -9.6743e-02,\n",
       "                        2.2198e-01,  2.0161e-01, -2.4063e-02, -8.6718e-02,  1.8441e-01,\n",
       "                       -5.0608e-03,  1.3436e-01,  1.8260e-02, -1.1290e-01,  1.6764e-01],\n",
       "                      [ 1.1283e-01,  1.0680e-01,  1.8544e-01,  1.1274e-01, -2.0463e-01,\n",
       "                       -1.7722e-01,  2.1261e-02,  8.4822e-02, -1.6041e-01, -1.2699e-01,\n",
       "                       -1.0010e-01,  1.9514e-01, -5.3211e-02,  1.1087e-01,  9.6334e-03,\n",
       "                       -4.3798e-03,  8.2692e-02, -1.2453e-01, -1.4052e-01,  1.5914e-01],\n",
       "                      [-5.9451e-02, -1.1152e-01,  9.3960e-02, -2.0015e-01, -1.7577e-01,\n",
       "                        6.3778e-02,  2.1946e-01,  2.1303e-01,  8.7645e-02,  5.3810e-03,\n",
       "                        1.4179e-01, -1.9594e-01, -1.8803e-01, -2.0220e-01,  1.8064e-01,\n",
       "                       -2.0317e-01,  2.0835e-01,  8.1254e-02,  8.6184e-02,  8.6981e-02]])),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.0814,  0.1611,  0.0684,  0.1677, -0.1810,  0.1262, -0.1104,  0.0483,\n",
       "                       0.2180,  0.1222, -0.2127, -0.2060, -0.0681,  0.1970,  0.1971, -0.0915,\n",
       "                      -0.1077,  0.1684, -0.1819,  0.1366, -0.1403,  0.1031,  0.0176, -0.1499,\n",
       "                      -0.1406,  0.0745,  0.0565, -0.0896, -0.1649,  0.1593])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[-0.0803, -0.0326,  0.1210,  0.0931,  0.0256, -0.0215,  0.0642, -0.0281,\n",
       "                        0.0763,  0.1369,  0.1700,  0.1730,  0.0203, -0.0196, -0.0435,  0.1499,\n",
       "                       -0.0095, -0.1054, -0.0927, -0.0695, -0.1610,  0.0372,  0.1197, -0.0682,\n",
       "                        0.1483,  0.1252, -0.1800, -0.0140, -0.0082, -0.1713],\n",
       "                      [ 0.0752, -0.0144, -0.1419, -0.0428, -0.0717,  0.0934,  0.1577, -0.0396,\n",
       "                        0.0584,  0.0269, -0.0251,  0.0967, -0.0315, -0.0241, -0.0024,  0.0778,\n",
       "                        0.0748,  0.0886,  0.0593, -0.0366,  0.1627, -0.0570,  0.1301,  0.0864,\n",
       "                        0.1103,  0.1049, -0.1112,  0.0169, -0.0809,  0.0437],\n",
       "                      [ 0.1002, -0.1298,  0.1591,  0.0442,  0.0601,  0.1147, -0.0231, -0.1351,\n",
       "                        0.0758, -0.1513,  0.0763,  0.0506,  0.1490, -0.1065, -0.1467,  0.0916,\n",
       "                       -0.1616,  0.0183, -0.1094,  0.1290, -0.1164, -0.0115, -0.1245,  0.1167,\n",
       "                        0.1207, -0.0881,  0.0649, -0.1067, -0.1649, -0.0855],\n",
       "                      [ 0.0932, -0.0039, -0.1100,  0.0774, -0.1537, -0.0166,  0.1015, -0.1564,\n",
       "                       -0.0634, -0.1546, -0.1290, -0.0436,  0.1111,  0.0917,  0.0467, -0.0046,\n",
       "                       -0.1700,  0.1305, -0.0953,  0.1017, -0.1377, -0.0014,  0.0243, -0.0037,\n",
       "                        0.0681,  0.0718,  0.0544, -0.0775,  0.0003,  0.0692],\n",
       "                      [ 0.0997,  0.0683,  0.0400, -0.0777, -0.0217,  0.0367,  0.0583,  0.1017,\n",
       "                       -0.1663,  0.0673, -0.0026, -0.0234, -0.0733,  0.1216, -0.0715, -0.0858,\n",
       "                       -0.0699,  0.0898, -0.0516,  0.0952,  0.0581, -0.0564, -0.0578, -0.0396,\n",
       "                        0.1672, -0.1802, -0.0701, -0.1598,  0.1294,  0.0991]])),\n",
       "             ('layer3.bias',\n",
       "              tensor([-0.0879, -0.1647, -0.0303,  0.0997, -0.0768]))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model = MyModel()\n",
    "temp_model_state = temp_model.state_dict()\n",
    "temp_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 更新临时模型状态字典中第一层的权重\n",
    "temp_model_state['layer1.weight'] = loaded_model_state['layer1.weight']\n",
    "temp_model_state['layer1.bias'] = loaded_model_state['layer1.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载更新后的状态字典到临时模型中\n",
    "temp_model.load_state_dict(temp_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1327,  0.2410,  0.0160,  0.0796, -0.0917, -0.2165,  0.1379, -0.2588,\n",
      "         -0.0418, -0.2426],\n",
      "        [ 0.0121, -0.2791, -0.1170, -0.2728, -0.2103, -0.2047,  0.1350,  0.0389,\n",
      "         -0.0615, -0.0656],\n",
      "        [ 0.2304, -0.1965, -0.2160, -0.1875, -0.2475, -0.2976, -0.2708,  0.0793,\n",
      "         -0.0839, -0.2597],\n",
      "        [-0.0657, -0.1457, -0.0153,  0.2022, -0.0098, -0.2434, -0.1064,  0.1861,\n",
      "         -0.3156, -0.0242],\n",
      "        [-0.0724,  0.0799,  0.0811, -0.2133, -0.3034,  0.0598, -0.2264, -0.2900,\n",
      "         -0.3108,  0.2913],\n",
      "        [ 0.1186, -0.0654, -0.0765, -0.1273, -0.0830, -0.0324, -0.1567,  0.3038,\n",
      "         -0.0756, -0.1254],\n",
      "        [-0.1813, -0.1068, -0.0429, -0.0960, -0.0951, -0.2517,  0.0334,  0.3103,\n",
      "          0.2028,  0.0773],\n",
      "        [-0.0661, -0.2851, -0.2932, -0.1070,  0.1101, -0.1198,  0.0904,  0.1695,\n",
      "         -0.3161, -0.0128],\n",
      "        [ 0.1358,  0.3056,  0.2715,  0.1985,  0.2016, -0.0567,  0.0483, -0.1020,\n",
      "          0.2979,  0.1270],\n",
      "        [-0.0805,  0.2874,  0.0396,  0.2939,  0.2433, -0.2308,  0.0592, -0.2183,\n",
      "         -0.2936, -0.0658],\n",
      "        [-0.0778, -0.2354,  0.2604, -0.0301,  0.2545, -0.2677, -0.0695, -0.1103,\n",
      "          0.2879, -0.1266],\n",
      "        [-0.0386, -0.0406, -0.0935, -0.2101, -0.1062, -0.0360, -0.1308, -0.0335,\n",
      "         -0.0343, -0.0628],\n",
      "        [-0.0653,  0.0042,  0.0041, -0.0333,  0.1764,  0.1032,  0.1860,  0.2621,\n",
      "          0.0490,  0.0005],\n",
      "        [-0.2946,  0.1929, -0.2875,  0.0143, -0.0402, -0.2322, -0.2394,  0.1298,\n",
      "          0.1937, -0.2351],\n",
      "        [ 0.0706, -0.1743, -0.1184,  0.2218, -0.3158, -0.0178, -0.0881,  0.2427,\n",
      "         -0.2698, -0.2309],\n",
      "        [ 0.2895,  0.2775,  0.0188, -0.0431, -0.3124,  0.0343, -0.2762, -0.2953,\n",
      "         -0.0198, -0.0141],\n",
      "        [-0.1661, -0.0303,  0.2854,  0.2220,  0.1657, -0.2075, -0.1902, -0.2638,\n",
      "         -0.1990,  0.2974],\n",
      "        [-0.2180, -0.2218, -0.1965,  0.2886, -0.1237,  0.0934,  0.0760, -0.1464,\n",
      "          0.0232,  0.0937],\n",
      "        [-0.0217, -0.0136,  0.1252,  0.2043,  0.0153, -0.2903,  0.0276, -0.1390,\n",
      "          0.2518,  0.0681],\n",
      "        [-0.2492, -0.0595,  0.1570,  0.2726, -0.0255, -0.2247,  0.1850, -0.1377,\n",
      "          0.1828,  0.2056]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 现在你可以访问临时模型中的第一层权重\n",
    "layer1_weights = temp_model.layer1.weight\n",
    "print(layer1_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True],\n",
       "         [True, True, True, True, True, True, True, True, True, True]]),\n",
       " tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_state['layer1.weight'] == temp_model.state_dict()['layer1.weight'],loaded_model_state['layer1.bias'] == temp_model.state_dict()['layer1.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False],\n",
       "        [False, False, False, False, False, False, False, False, False, False,\n",
       "         False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_state['layer2.weight'] == temp_model.state_dict()['layer2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(temp_model.layer1.weight.data.numpy())  == temp_model.layer1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Parameter in module torch.nn.parameter object:\n",
      "\n",
      "class Parameter(torch.Tensor)\n",
      " |  Parameter(data=None, requires_grad=True)\n",
      " |  \n",
      " |  A kind of Tensor that is to be considered a module parameter.\n",
      " |  \n",
      " |  Parameters are :class:`~torch.Tensor` subclasses, that have a\n",
      " |  very special property when used with :class:`Module` s - when they're\n",
      " |  assigned as Module attributes they are automatically added to the list of\n",
      " |  its parameters, and will appear e.g. in :meth:`~Module.parameters` iterator.\n",
      " |  Assigning a Tensor doesn't have such effect. This is because one might\n",
      " |  want to cache some temporary state, like last hidden state of the RNN, in\n",
      " |  the model. If there was no such class as :class:`Parameter`, these\n",
      " |  temporaries would get registered too.\n",
      " |  \n",
      " |  Args:\n",
      " |      data (Tensor): parameter tensor.\n",
      " |      requires_grad (bool, optional): if the parameter requires gradient. See\n",
      " |          :ref:`locally-disable-grad-doc` for more details. Default: `True`\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Parameter\n",
      " |      torch.Tensor\n",
      " |      torch._C._TensorBase\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __deepcopy__(self, memo)\n",
      " |      # Note: the 3 methods below only apply to standard Tensor. Parameters of custom tensor types\n",
      " |      # are still considered that custom tensor type and these methods will not be called for them.\n",
      " |  \n",
      " |  __reduce_ex__(self, proto)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(cls, data=None, requires_grad=True)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __torch_function__ = _disabled_torch_function_impl(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.Tensor:\n",
      " |  \n",
      " |  __abs__ = abs(...)\n",
      " |  \n",
      " |  __array__(self, dtype=None)\n",
      " |  \n",
      " |  __array_wrap__(self, array)\n",
      " |      # Wrap Numpy array again in a suitable tensor when done, to support e.g.\n",
      " |      # `numpy.sin(tensor) -> tensor` or `numpy.greater(tensor, 0) -> ByteTensor`\n",
      " |  \n",
      " |  __contains__(self, element)\n",
      " |      Check if `element` is present in tensor\n",
      " |      \n",
      " |      Args:\n",
      " |          element (Tensor or scalar): element to be checked\n",
      " |              for presence in current tensor\"\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __dlpack__(self, stream=None)\n",
      " |      Creates a DLpack `capsule https://data-apis.org/array-api/latest/design_topics/data_interchange.html#data-interchange`_\n",
      " |      of the current tensor to be exported to other libraries.\n",
      " |      \n",
      " |      This function will be called from the `from_dlpack` method\n",
      " |      of the library that will consume the capsule. `from_dlpack` passes the current\n",
      " |      stream to this method as part of the specification.\n",
      " |      \n",
      " |      Args:\n",
      " |          stream (integer or None): An optional Python integer representing a\n",
      " |          pointer to a CUDA stream. The current stream is synchronized with\n",
      " |          this stream before the capsule is created, and since the capsule\n",
      " |          shares its storage with the tensor this make it safe to access from\n",
      " |          both streams.  If None or -1 is passed then no synchronization is performed.\n",
      " |  \n",
      " |  __dlpack_device__(self) -> Tuple[enum.IntEnum, int]\n",
      " |  \n",
      " |  __floordiv__(self, other)\n",
      " |  \n",
      " |  __format__(self, format_spec)\n",
      " |      Default object formatter.\n",
      " |  \n",
      " |  __hash__(self)\n",
      " |      Return hash(self).\n",
      " |  \n",
      " |  __ipow__ = pow_(...)\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |  \n",
      " |  __itruediv__ = __idiv__(...)\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return len(self).\n",
      " |  \n",
      " |  __neg__ = neg(...)\n",
      " |  \n",
      " |  __pos__ = positive(...)\n",
      " |  \n",
      " |  __pow__ = pow(...)\n",
      " |  \n",
      " |  __rdiv__(self, other)\n",
      " |  \n",
      " |  __reversed__(self)\n",
      " |      Reverses the tensor along dimension 0.\n",
      " |  \n",
      " |  __rfloordiv__(self, other)\n",
      " |  \n",
      " |  __rlshift__(self, other)\n",
      " |  \n",
      " |  __rmatmul__(self, other)\n",
      " |  \n",
      " |  __rmod__(self, other)\n",
      " |  \n",
      " |  __rpow__(self, other)\n",
      " |  \n",
      " |  __rrshift__(self, other)\n",
      " |  \n",
      " |  __rsub__(self, other)\n",
      " |  \n",
      " |  __rtruediv__ = __rdiv__(self, other)\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  align_to(self, *names)\n",
      " |      Permutes the dimensions of the :attr:`self` tensor to match the order\n",
      " |      specified in :attr:`names`, adding size-one dims for any new names.\n",
      " |      \n",
      " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
      " |      The resulting tensor is a view on the original tensor.\n",
      " |      \n",
      " |      All dimension names of :attr:`self` must be present in :attr:`names`.\n",
      " |      :attr:`names` may contain additional names that are not in ``self.names``;\n",
      " |      the output tensor has a size-one dimension for each of those new names.\n",
      " |      \n",
      " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
      " |      The Ellipsis is expanded to be equal to all dimension names of :attr:`self`\n",
      " |      that are not mentioned in :attr:`names`, in the order that they appear\n",
      " |      in :attr:`self`.\n",
      " |      \n",
      " |      Python 2 does not support Ellipsis but one may use a string literal\n",
      " |      instead (``'...'``).\n",
      " |      \n",
      " |      Args:\n",
      " |          names (iterable of str): The desired dimension ordering of the\n",
      " |              output tensor. May contain up to one Ellipsis that is expanded\n",
      " |              to all unmentioned dim names of :attr:`self`.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)\n",
      " |          >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')\n",
      " |      \n",
      " |          # Move the F and E dims to the front while keeping the rest in order\n",
      " |          >>> named_tensor.align_to('F', 'E', ...)\n",
      " |      \n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |  \n",
      " |  backward(self, gradient=None, retain_graph=None, create_graph=False, inputs=None)\n",
      " |      Computes the gradient of current tensor w.r.t. graph leaves.\n",
      " |      \n",
      " |      The graph is differentiated using the chain rule. If the tensor is\n",
      " |      non-scalar (i.e. its data has more than one element) and requires\n",
      " |      gradient, the function additionally requires specifying ``gradient``.\n",
      " |      It should be a tensor of matching type and location, that contains\n",
      " |      the gradient of the differentiated function w.r.t. ``self``.\n",
      " |      \n",
      " |      This function accumulates gradients in the leaves - you might need to zero\n",
      " |      ``.grad`` attributes or set them to ``None`` before calling it.\n",
      " |      See :ref:`Default gradient layouts<default-grad-layouts>`\n",
      " |      for details on the memory layout of accumulated gradients.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If you run any forward ops, create ``gradient``, and/or call ``backward``\n",
      " |          in a user-specified CUDA stream context, see\n",
      " |          :ref:`Stream semantics of backward passes<bwd-cuda-stream-semantics>`.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          When ``inputs`` are provided and a given input is not a leaf,\n",
      " |          the current implementation will call its grad_fn (though it is not strictly needed to get this gradients).\n",
      " |          It is an implementation detail on which the user should not rely.\n",
      " |          See https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780 for more details.\n",
      " |      \n",
      " |      Args:\n",
      " |          gradient (Tensor or None): Gradient w.r.t. the\n",
      " |              tensor. If it is a tensor, it will be automatically converted\n",
      " |              to a Tensor that does not require grad unless ``create_graph`` is True.\n",
      " |              None values can be specified for scalar Tensors or ones that\n",
      " |              don't require grad. If a None value would be acceptable then\n",
      " |              this argument is optional.\n",
      " |          retain_graph (bool, optional): If ``False``, the graph used to compute\n",
      " |              the grads will be freed. Note that in nearly all cases setting\n",
      " |              this option to True is not needed and often can be worked around\n",
      " |              in a much more efficient way. Defaults to the value of\n",
      " |              ``create_graph``.\n",
      " |          create_graph (bool, optional): If ``True``, graph of the derivative will\n",
      " |              be constructed, allowing to compute higher order derivative\n",
      " |              products. Defaults to ``False``.\n",
      " |          inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be\n",
      " |              accumulated into ``.grad``. All other Tensors will be ignored. If not\n",
      " |              provided, the gradient is accumulated into all the leaf Tensors that were\n",
      " |              used to compute the attr::tensors.\n",
      " |  \n",
      " |  detach(...)\n",
      " |      Returns a new Tensor, detached from the current graph.\n",
      " |      \n",
      " |      The result will never require gradient.\n",
      " |      \n",
      " |      This method also affects forward mode AD gradients and the result will never\n",
      " |      have forward mode AD gradients.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |        Returned Tensor shares the same storage with the original one.\n",
      " |        In-place modifications on either of them will be seen, and may trigger\n",
      " |        errors in correctness checks.\n",
      " |        IMPORTANT NOTE: Previously, in-place size / stride / storage changes\n",
      " |        (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor\n",
      " |        also update the original tensor. Now, these in-place changes will not update the\n",
      " |        original tensor anymore, and will instead trigger an error.\n",
      " |        For sparse tensors:\n",
      " |        In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the\n",
      " |        returned tensor will not update the original tensor anymore, and will instead\n",
      " |        trigger an error.\n",
      " |  \n",
      " |  detach_(...)\n",
      " |      Detaches the Tensor from the graph that created it, making it a leaf.\n",
      " |      Views cannot be detached in-place.\n",
      " |      \n",
      " |      This method also affects forward mode AD gradients and the result will never\n",
      " |      have forward mode AD gradients.\n",
      " |  \n",
      " |  eig(self, eigenvectors=False)\n",
      " |  \n",
      " |  is_shared(self)\n",
      " |      Checks if tensor is in shared memory.\n",
      " |      \n",
      " |      This is always ``True`` for CUDA tensors.\n",
      " |  \n",
      " |  istft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, normalized: bool = False, onesided: Optional[bool] = None, length: Optional[int] = None, return_complex: bool = False)\n",
      " |      See :func:`torch.istft`\n",
      " |  \n",
      " |  lstsq(self, other)\n",
      " |  \n",
      " |  lu(self, pivot=True, get_infos=False)\n",
      " |      See :func:`torch.lu`\n",
      " |  \n",
      " |  norm(self, p: Union[float, str, NoneType] = 'fro', dim=None, keepdim=False, dtype=None)\n",
      " |      See :func:`torch.norm`\n",
      " |  \n",
      " |  refine_names(self, *names)\n",
      " |      Refines the dimension names of :attr:`self` according to :attr:`names`.\n",
      " |      \n",
      " |      Refining is a special case of renaming that \"lifts\" unnamed dimensions.\n",
      " |      A ``None`` dim can be refined to have any name; a named dim can only be\n",
      " |      refined to have the same name.\n",
      " |      \n",
      " |      Because named tensors can coexist with unnamed tensors, refining names\n",
      " |      gives a nice way to write named-tensor-aware code that works with both\n",
      " |      named and unnamed tensors.\n",
      " |      \n",
      " |      :attr:`names` may contain up to one Ellipsis (``...``).\n",
      " |      The Ellipsis is expanded greedily; it is expanded in-place to fill\n",
      " |      :attr:`names` to the same length as ``self.dim()`` using names from the\n",
      " |      corresponding indices of ``self.names``.\n",
      " |      \n",
      " |      Python 2 does not support Ellipsis but one may use a string literal\n",
      " |      instead (``'...'``).\n",
      " |      \n",
      " |      Args:\n",
      " |          names (iterable of str): The desired names of the output tensor. May\n",
      " |              contain up to one Ellipsis.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> imgs = torch.randn(32, 3, 128, 128)\n",
      " |          >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')\n",
      " |          >>> named_imgs.names\n",
      " |          ('N', 'C', 'H', 'W')\n",
      " |      \n",
      " |          >>> tensor = torch.randn(2, 3, 5, 7, 11)\n",
      " |          >>> tensor = tensor.refine_names('A', ..., 'B', 'C')\n",
      " |          >>> tensor.names\n",
      " |          ('A', None, None, 'B', 'C')\n",
      " |      \n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |  \n",
      " |  register_hook(self, hook)\n",
      " |      Registers a backward hook.\n",
      " |      \n",
      " |      The hook will be called every time a gradient with respect to the\n",
      " |      Tensor is computed. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(grad) -> Tensor or None\n",
      " |      \n",
      " |      \n",
      " |      The hook should not modify its argument, but it can optionally return\n",
      " |      a new gradient which will be used in place of :attr:`grad`.\n",
      " |      \n",
      " |      This function returns a handle with a method ``handle.remove()``\n",
      " |      that removes the hook from the module.\n",
      " |      \n",
      " |      .. note::\n",
      " |          See :ref:`backward-hooks-execution` for more information on how when this hook\n",
      " |          is executed, and how its execution is ordered relative to other hooks.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> v = torch.tensor([0., 0., 0.], requires_grad=True)\n",
      " |          >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient\n",
      " |          >>> v.backward(torch.tensor([1., 2., 3.]))\n",
      " |          >>> v.grad\n",
      " |      \n",
      " |           2\n",
      " |           4\n",
      " |           6\n",
      " |          [torch.FloatTensor of size (3,)]\n",
      " |      \n",
      " |          >>> h.remove()  # removes the hook\n",
      " |  \n",
      " |  reinforce(self, reward)\n",
      " |  \n",
      " |  rename(self, *names, **rename_map)\n",
      " |      Renames dimension names of :attr:`self`.\n",
      " |      \n",
      " |      There are two main usages:\n",
      " |      \n",
      " |      ``self.rename(**rename_map)`` returns a view on tensor that has dims\n",
      " |      renamed as specified in the mapping :attr:`rename_map`.\n",
      " |      \n",
      " |      ``self.rename(*names)`` returns a view on tensor, renaming all\n",
      " |      dimensions positionally using :attr:`names`.\n",
      " |      Use ``self.rename(None)`` to drop names on a tensor.\n",
      " |      \n",
      " |      One cannot specify both positional args :attr:`names` and keyword args\n",
      " |      :attr:`rename_map`.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))\n",
      " |          >>> renamed_imgs = imgs.rename(N='batch', C='channels')\n",
      " |          >>> renamed_imgs.names\n",
      " |          ('batch', 'channels', 'H', 'W')\n",
      " |      \n",
      " |          >>> renamed_imgs = imgs.rename(None)\n",
      " |          >>> renamed_imgs.names\n",
      " |          (None, None, None, None)\n",
      " |      \n",
      " |          >>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')\n",
      " |          >>> renamed_imgs.names\n",
      " |          ('batch', 'channel', 'height', 'width')\n",
      " |      \n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |  \n",
      " |  rename_(self, *names, **rename_map)\n",
      " |      In-place version of :meth:`~Tensor.rename`.\n",
      " |  \n",
      " |  resize(self, *sizes)\n",
      " |  \n",
      " |  resize_as(self, tensor)\n",
      " |  \n",
      " |  share_memory_(self)\n",
      " |      Moves the underlying storage to shared memory.\n",
      " |      \n",
      " |      This is a no-op if the underlying storage is already in shared memory\n",
      " |      and for CUDA tensors. Tensors in shared memory cannot be resized.\n",
      " |  \n",
      " |  solve(self, other)\n",
      " |  \n",
      " |  split(self, split_size, dim=0)\n",
      " |      See :func:`torch.split`\n",
      " |  \n",
      " |  stft(self, n_fft: int, hop_length: Optional[int] = None, win_length: Optional[int] = None, window: 'Optional[Tensor]' = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: Optional[bool] = None, return_complex: Optional[bool] = None)\n",
      " |      See :func:`torch.stft`\n",
      " |      \n",
      " |      .. warning::\n",
      " |        This function changed signature at version 0.4.1. Calling with\n",
      " |        the previous signature may cause error or return incorrect result.\n",
      " |  \n",
      " |  storage(self)\n",
      " |      storage() -> torch.TypedStorage\n",
      " |      \n",
      " |      Returns the underlying :class:`TypedStorage`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          :class:`TypedStorage` is deprecated. It will be removed in the future, and\n",
      " |          :class:`UntypedStorage` will be the only storage class. To access the\n",
      " |          :class:`UntypedStorage` directly, use :attr:`Tensor.untyped_storage()`.\n",
      " |  \n",
      " |  storage_type(self)\n",
      " |      storage_type() -> type\n",
      " |      \n",
      " |      Returns the type of the underlying storage.\n",
      " |  \n",
      " |  symeig(self, eigenvectors=False)\n",
      " |  \n",
      " |  to_sparse_coo(self)\n",
      " |      Convert a tensor to :ref:`coordinate format <sparse-coo-docs>`.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |           >>> dense = torch.randn(5, 5)\n",
      " |           >>> sparse = dense.to_sparse_coo()\n",
      " |           >>> sparse._nnz()\n",
      " |           25\n",
      " |  \n",
      " |  unflatten(self, dim, sizes)\n",
      " |      unflatten(dim, sizes) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.unflatten`.\n",
      " |  \n",
      " |  unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None)\n",
      " |      Returns the unique elements of the input tensor.\n",
      " |      \n",
      " |      See :func:`torch.unique`\n",
      " |  \n",
      " |  unique_consecutive(self, return_inverse=False, return_counts=False, dim=None)\n",
      " |      Eliminates all but the first element from every consecutive group of equivalent elements.\n",
      " |      \n",
      " |      See :func:`torch.unique_consecutive`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from torch.Tensor:\n",
      " |  \n",
      " |  __torch_dispatch__ = _disabled_torch_dispatch_impl(...)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from torch.Tensor:\n",
      " |  \n",
      " |  __cuda_array_interface__\n",
      " |      Array view description for cuda tensors.\n",
      " |      \n",
      " |      See:\n",
      " |      https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.Tensor:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.Tensor:\n",
      " |  \n",
      " |  __array_priority__ = 1000\n",
      " |  \n",
      " |  __slotnames__ = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch._C._TensorBase:\n",
      " |  \n",
      " |  __add__(...)\n",
      " |  \n",
      " |  __and__(...)\n",
      " |  \n",
      " |  __bool__(...)\n",
      " |  \n",
      " |  __complex__(...)\n",
      " |  \n",
      " |  __delitem__(self, key, /)\n",
      " |      Delete self[key].\n",
      " |  \n",
      " |  __div__(...)\n",
      " |  \n",
      " |  __eq__(...)\n",
      " |      Return self==value.\n",
      " |  \n",
      " |  __float__(...)\n",
      " |  \n",
      " |  __ge__(...)\n",
      " |      Return self>=value.\n",
      " |  \n",
      " |  __getitem__(self, key, /)\n",
      " |      Return self[key].\n",
      " |  \n",
      " |  __gt__(...)\n",
      " |      Return self>value.\n",
      " |  \n",
      " |  __iadd__(...)\n",
      " |  \n",
      " |  __iand__(...)\n",
      " |  \n",
      " |  __idiv__(...)\n",
      " |  \n",
      " |  __ifloordiv__(...)\n",
      " |  \n",
      " |  __ilshift__(...)\n",
      " |  \n",
      " |  __imod__(...)\n",
      " |  \n",
      " |  __imul__(...)\n",
      " |  \n",
      " |  __index__(...)\n",
      " |  \n",
      " |  __int__(...)\n",
      " |  \n",
      " |  __invert__(...)\n",
      " |  \n",
      " |  __ior__(...)\n",
      " |  \n",
      " |  __irshift__(...)\n",
      " |  \n",
      " |  __isub__(...)\n",
      " |  \n",
      " |  __ixor__(...)\n",
      " |  \n",
      " |  __le__(...)\n",
      " |      Return self<=value.\n",
      " |  \n",
      " |  __long__(...)\n",
      " |  \n",
      " |  __lshift__(...)\n",
      " |  \n",
      " |  __lt__(...)\n",
      " |      Return self<value.\n",
      " |  \n",
      " |  __matmul__(...)\n",
      " |  \n",
      " |  __mod__(...)\n",
      " |  \n",
      " |  __mul__(...)\n",
      " |  \n",
      " |  __ne__(...)\n",
      " |      Return self!=value.\n",
      " |  \n",
      " |  __nonzero__(...)\n",
      " |  \n",
      " |  __or__(...)\n",
      " |      Return self|value.\n",
      " |  \n",
      " |  __radd__(...)\n",
      " |  \n",
      " |  __rand__(...)\n",
      " |  \n",
      " |  __rmul__(...)\n",
      " |  \n",
      " |  __ror__(...)\n",
      " |      Return value|self.\n",
      " |  \n",
      " |  __rshift__(...)\n",
      " |  \n",
      " |  __rxor__(...)\n",
      " |  \n",
      " |  __setitem__(self, key, value, /)\n",
      " |      Set self[key] to value.\n",
      " |  \n",
      " |  __sub__(...)\n",
      " |  \n",
      " |  __truediv__(...)\n",
      " |  \n",
      " |  __xor__(...)\n",
      " |  \n",
      " |  abs(...)\n",
      " |      abs() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.abs`\n",
      " |  \n",
      " |  abs_(...)\n",
      " |      abs_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.abs`\n",
      " |  \n",
      " |  absolute(...)\n",
      " |      absolute() -> Tensor\n",
      " |      \n",
      " |      Alias for :func:`abs`\n",
      " |  \n",
      " |  absolute_(...)\n",
      " |      absolute_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.absolute`\n",
      " |      Alias for :func:`abs_`\n",
      " |  \n",
      " |  acos(...)\n",
      " |      acos() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.acos`\n",
      " |  \n",
      " |  acos_(...)\n",
      " |      acos_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.acos`\n",
      " |  \n",
      " |  acosh(...)\n",
      " |      acosh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.acosh`\n",
      " |  \n",
      " |  acosh_(...)\n",
      " |      acosh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.acosh`\n",
      " |  \n",
      " |  add(...)\n",
      " |      add(other, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      Add a scalar or tensor to :attr:`self` tensor. If both :attr:`alpha`\n",
      " |      and :attr:`other` are specified, each element of :attr:`other` is scaled by\n",
      " |      :attr:`alpha` before being used.\n",
      " |      \n",
      " |      When :attr:`other` is a tensor, the shape of :attr:`other` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor\n",
      " |      \n",
      " |      See :func:`torch.add`\n",
      " |  \n",
      " |  add_(...)\n",
      " |      add_(other, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.add`\n",
      " |  \n",
      " |  addbmm(...)\n",
      " |      addbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addbmm`\n",
      " |  \n",
      " |  addbmm_(...)\n",
      " |      addbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addbmm`\n",
      " |  \n",
      " |  addcdiv(...)\n",
      " |      addcdiv(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addcdiv`\n",
      " |  \n",
      " |  addcdiv_(...)\n",
      " |      addcdiv_(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addcdiv`\n",
      " |  \n",
      " |  addcmul(...)\n",
      " |      addcmul(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addcmul`\n",
      " |  \n",
      " |  addcmul_(...)\n",
      " |      addcmul_(tensor1, tensor2, *, value=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addcmul`\n",
      " |  \n",
      " |  addmm(...)\n",
      " |      addmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addmm`\n",
      " |  \n",
      " |  addmm_(...)\n",
      " |      addmm_(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addmm`\n",
      " |  \n",
      " |  addmv(...)\n",
      " |      addmv(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addmv`\n",
      " |  \n",
      " |  addmv_(...)\n",
      " |      addmv_(mat, vec, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addmv`\n",
      " |  \n",
      " |  addr(...)\n",
      " |      addr(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.addr`\n",
      " |  \n",
      " |  addr_(...)\n",
      " |      addr_(vec1, vec2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.addr`\n",
      " |  \n",
      " |  adjoint(...)\n",
      " |      adjoint() -> Tensor\n",
      " |      \n",
      " |      Alias for :func:`adjoint`\n",
      " |  \n",
      " |  align_as(...)\n",
      " |      align_as(other) -> Tensor\n",
      " |      \n",
      " |      Permutes the dimensions of the :attr:`self` tensor to match the dimension order\n",
      " |      in the :attr:`other` tensor, adding size-one dims for any new names.\n",
      " |      \n",
      " |      This operation is useful for explicit broadcasting by names (see examples).\n",
      " |      \n",
      " |      All of the dims of :attr:`self` must be named in order to use this method.\n",
      " |      The resulting tensor is a view on the original tensor.\n",
      " |      \n",
      " |      All dimension names of :attr:`self` must be present in ``other.names``.\n",
      " |      :attr:`other` may contain named dimensions that are not in ``self.names``;\n",
      " |      the output tensor has a size-one dimension for each of those new names.\n",
      " |      \n",
      " |      To align a tensor to a specific order, use :meth:`~Tensor.align_to`.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          # Example 1: Applying a mask\n",
      " |          >>> mask = torch.randint(2, [127, 128], dtype=torch.bool).refine_names('W', 'H')\n",
      " |          >>> imgs = torch.randn(32, 128, 127, 3, names=('N', 'H', 'W', 'C'))\n",
      " |          >>> imgs.masked_fill_(mask.align_as(imgs), 0)\n",
      " |      \n",
      " |      \n",
      " |          # Example 2: Applying a per-channel-scale\n",
      " |          >>> def scale_channels(input, scale):\n",
      " |          >>>    scale = scale.refine_names('C')\n",
      " |          >>>    return input * scale.align_as(input)\n",
      " |      \n",
      " |          >>> num_channels = 3\n",
      " |          >>> scale = torch.randn(num_channels, names=('C',))\n",
      " |          >>> imgs = torch.rand(32, 128, 128, num_channels, names=('N', 'H', 'W', 'C'))\n",
      " |          >>> more_imgs = torch.rand(32, num_channels, 128, 128, names=('N', 'C', 'H', 'W'))\n",
      " |          >>> videos = torch.randn(3, num_channels, 128, 128, 128, names=('N', 'C', 'H', 'W', 'D'))\n",
      " |      \n",
      " |          # scale_channels is agnostic to the dimension order of the input\n",
      " |          >>> scale_channels(imgs, scale)\n",
      " |          >>> scale_channels(more_imgs, scale)\n",
      " |          >>> scale_channels(videos, scale)\n",
      " |      \n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |  \n",
      " |  all(...)\n",
      " |      all(dim=None, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.all`\n",
      " |  \n",
      " |  allclose(...)\n",
      " |      allclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.allclose`\n",
      " |  \n",
      " |  amax(...)\n",
      " |      amax(dim=None, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.amax`\n",
      " |  \n",
      " |  amin(...)\n",
      " |      amin(dim=None, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.amin`\n",
      " |  \n",
      " |  aminmax(...)\n",
      " |      aminmax(*, dim=None, keepdim=False) -> (Tensor min, Tensor max)\n",
      " |      \n",
      " |      See :func:`torch.aminmax`\n",
      " |  \n",
      " |  angle(...)\n",
      " |      angle() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.angle`\n",
      " |  \n",
      " |  any(...)\n",
      " |      any(dim=None, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.any`\n",
      " |  \n",
      " |  apply_(...)\n",
      " |      apply_(callable) -> Tensor\n",
      " |      \n",
      " |      Applies the function :attr:`callable` to each element in the tensor, replacing\n",
      " |      each element with the value returned by :attr:`callable`.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This function only works with CPU tensors and should not be used in code\n",
      " |          sections that require high performance.\n",
      " |  \n",
      " |  arccos(...)\n",
      " |      arccos() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.arccos`\n",
      " |  \n",
      " |  arccos_(...)\n",
      " |      arccos_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.arccos`\n",
      " |  \n",
      " |  arccosh(...)\n",
      " |      acosh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.arccosh`\n",
      " |  \n",
      " |  arccosh_(...)\n",
      " |      acosh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.arccosh`\n",
      " |  \n",
      " |  arcsin(...)\n",
      " |      arcsin() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.arcsin`\n",
      " |  \n",
      " |  arcsin_(...)\n",
      " |      arcsin_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.arcsin`\n",
      " |  \n",
      " |  arcsinh(...)\n",
      " |      arcsinh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.arcsinh`\n",
      " |  \n",
      " |  arcsinh_(...)\n",
      " |      arcsinh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.arcsinh`\n",
      " |  \n",
      " |  arctan(...)\n",
      " |      arctan() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.arctan`\n",
      " |  \n",
      " |  arctan2(...)\n",
      " |      arctan2(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.arctan2`\n",
      " |  \n",
      " |  arctan2_(...)\n",
      " |      atan2_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.arctan2`\n",
      " |  \n",
      " |  arctan_(...)\n",
      " |      arctan_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.arctan`\n",
      " |  \n",
      " |  arctanh(...)\n",
      " |      arctanh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.arctanh`\n",
      " |  \n",
      " |  arctanh_(...)\n",
      " |      arctanh_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.arctanh`\n",
      " |  \n",
      " |  argmax(...)\n",
      " |      argmax(dim=None, keepdim=False) -> LongTensor\n",
      " |      \n",
      " |      See :func:`torch.argmax`\n",
      " |  \n",
      " |  argmin(...)\n",
      " |      argmin(dim=None, keepdim=False) -> LongTensor\n",
      " |      \n",
      " |      See :func:`torch.argmin`\n",
      " |  \n",
      " |  argsort(...)\n",
      " |      argsort(dim=-1, descending=False) -> LongTensor\n",
      " |      \n",
      " |      See :func:`torch.argsort`\n",
      " |  \n",
      " |  argwhere(...)\n",
      " |      argwhere() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.argwhere`\n",
      " |  \n",
      " |  as_strided(...)\n",
      " |      as_strided(size, stride, storage_offset=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.as_strided`\n",
      " |  \n",
      " |  as_strided_(...)\n",
      " |  \n",
      " |  as_strided_scatter(...)\n",
      " |      as_strided_scatter(src, size, stride, storage_offset=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.as_strided_scatter`\n",
      " |  \n",
      " |  as_subclass(...)\n",
      " |      as_subclass(cls) -> Tensor\n",
      " |      \n",
      " |      Makes a ``cls`` instance with the same data pointer as ``self``. Changes\n",
      " |      in the output mirror changes in ``self``, and the output stays attached\n",
      " |      to the autograd graph. ``cls`` must be a subclass of ``Tensor``.\n",
      " |  \n",
      " |  asin(...)\n",
      " |      asin() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.asin`\n",
      " |  \n",
      " |  asin_(...)\n",
      " |      asin_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.asin`\n",
      " |  \n",
      " |  asinh(...)\n",
      " |      asinh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.asinh`\n",
      " |  \n",
      " |  asinh_(...)\n",
      " |      asinh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.asinh`\n",
      " |  \n",
      " |  atan(...)\n",
      " |      atan() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.atan`\n",
      " |  \n",
      " |  atan2(...)\n",
      " |      atan2(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.atan2`\n",
      " |  \n",
      " |  atan2_(...)\n",
      " |      atan2_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.atan2`\n",
      " |  \n",
      " |  atan_(...)\n",
      " |      atan_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.atan`\n",
      " |  \n",
      " |  atanh(...)\n",
      " |      atanh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.atanh`\n",
      " |  \n",
      " |  atanh_(...)\n",
      " |      atanh_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.atanh`\n",
      " |  \n",
      " |  baddbmm(...)\n",
      " |      baddbmm(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.baddbmm`\n",
      " |  \n",
      " |  baddbmm_(...)\n",
      " |      baddbmm_(batch1, batch2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.baddbmm`\n",
      " |  \n",
      " |  bernoulli(...)\n",
      " |      bernoulli(*, generator=None) -> Tensor\n",
      " |      \n",
      " |      Returns a result tensor where each :math:`\\texttt{result[i]}` is independently\n",
      " |      sampled from :math:`\\text{Bernoulli}(\\texttt{self[i]})`. :attr:`self` must have\n",
      " |      floating point ``dtype``, and the result will have the same ``dtype``.\n",
      " |      \n",
      " |      See :func:`torch.bernoulli`\n",
      " |  \n",
      " |  bernoulli_(...)\n",
      " |      bernoulli_(p=0.5, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills each location of :attr:`self` with an independent sample from\n",
      " |      :math:`\\text{Bernoulli}(\\texttt{p})`. :attr:`self` can have integral\n",
      " |      ``dtype``.\n",
      " |      \n",
      " |      :attr:`p` should either be a scalar or tensor containing probabilities to be\n",
      " |      used for drawing the binary random number.\n",
      " |      \n",
      " |      If it is a tensor, the :math:`\\text{i}^{th}` element of :attr:`self` tensor\n",
      " |      will be set to a value sampled from\n",
      " |      :math:`\\text{Bernoulli}(\\texttt{p\\_tensor[i]})`. In this case `p` must have\n",
      " |      floating point ``dtype``.\n",
      " |      \n",
      " |      See also :meth:`~Tensor.bernoulli` and :func:`torch.bernoulli`\n",
      " |  \n",
      " |  bfloat16(...)\n",
      " |      bfloat16(memory_format=torch.preserve_format) -> Tensor\n",
      " |      ``self.bfloat16()`` is equivalent to ``self.to(torch.bfloat16)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  bincount(...)\n",
      " |      bincount(weights=None, minlength=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bincount`\n",
      " |  \n",
      " |  bitwise_and(...)\n",
      " |      bitwise_and() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bitwise_and`\n",
      " |  \n",
      " |  bitwise_and_(...)\n",
      " |      bitwise_and_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bitwise_and`\n",
      " |  \n",
      " |  bitwise_left_shift(...)\n",
      " |      bitwise_left_shift(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bitwise_left_shift`\n",
      " |  \n",
      " |  bitwise_left_shift_(...)\n",
      " |      bitwise_left_shift_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bitwise_left_shift`\n",
      " |  \n",
      " |  bitwise_not(...)\n",
      " |      bitwise_not() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bitwise_not`\n",
      " |  \n",
      " |  bitwise_not_(...)\n",
      " |      bitwise_not_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bitwise_not`\n",
      " |  \n",
      " |  bitwise_or(...)\n",
      " |      bitwise_or() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bitwise_or`\n",
      " |  \n",
      " |  bitwise_or_(...)\n",
      " |      bitwise_or_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bitwise_or`\n",
      " |  \n",
      " |  bitwise_right_shift(...)\n",
      " |      bitwise_right_shift(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bitwise_right_shift`\n",
      " |  \n",
      " |  bitwise_right_shift_(...)\n",
      " |      bitwise_right_shift_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bitwise_right_shift`\n",
      " |  \n",
      " |  bitwise_xor(...)\n",
      " |      bitwise_xor() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bitwise_xor`\n",
      " |  \n",
      " |  bitwise_xor_(...)\n",
      " |      bitwise_xor_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.bitwise_xor`\n",
      " |  \n",
      " |  bmm(...)\n",
      " |      bmm(batch2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.bmm`\n",
      " |  \n",
      " |  bool(...)\n",
      " |      bool(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.bool()`` is equivalent to ``self.to(torch.bool)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  broadcast_to(...)\n",
      " |      broadcast_to(shape) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.broadcast_to`.\n",
      " |  \n",
      " |  byte(...)\n",
      " |      byte(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.byte()`` is equivalent to ``self.to(torch.uint8)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  cauchy_(...)\n",
      " |      cauchy_(median=0, sigma=1, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills the tensor with numbers drawn from the Cauchy distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(x) = \\dfrac{1}{\\pi} \\dfrac{\\sigma}{(x - \\text{median})^2 + \\sigma^2}\n",
      " |  \n",
      " |  ccol_indices(...)\n",
      " |  \n",
      " |  cdouble(...)\n",
      " |      cdouble(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.cdouble()`` is equivalent to ``self.to(torch.complex128)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  ceil(...)\n",
      " |      ceil() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ceil`\n",
      " |  \n",
      " |  ceil_(...)\n",
      " |      ceil_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ceil`\n",
      " |  \n",
      " |  cfloat(...)\n",
      " |      cfloat(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.cfloat()`` is equivalent to ``self.to(torch.complex64)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  chalf(...)\n",
      " |      chalf(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.chalf()`` is equivalent to ``self.to(torch.complex32)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |           memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  char(...)\n",
      " |      char(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.char()`` is equivalent to ``self.to(torch.int8)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  cholesky(...)\n",
      " |      cholesky(upper=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cholesky`\n",
      " |  \n",
      " |  cholesky_inverse(...)\n",
      " |      cholesky_inverse(upper=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cholesky_inverse`\n",
      " |  \n",
      " |  cholesky_solve(...)\n",
      " |      cholesky_solve(input2, upper=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cholesky_solve`\n",
      " |  \n",
      " |  chunk(...)\n",
      " |      chunk(chunks, dim=0) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.chunk`\n",
      " |  \n",
      " |  clamp(...)\n",
      " |      clamp(min=None, max=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.clamp`\n",
      " |  \n",
      " |  clamp_(...)\n",
      " |      clamp_(min=None, max=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.clamp`\n",
      " |  \n",
      " |  clamp_max(...)\n",
      " |  \n",
      " |  clamp_max_(...)\n",
      " |  \n",
      " |  clamp_min(...)\n",
      " |  \n",
      " |  clamp_min_(...)\n",
      " |  \n",
      " |  clip(...)\n",
      " |      clip(min=None, max=None) -> Tensor\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.clamp`.\n",
      " |  \n",
      " |  clip_(...)\n",
      " |      clip_(min=None, max=None) -> Tensor\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.clamp_`.\n",
      " |  \n",
      " |  clone(...)\n",
      " |      clone(*, memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.clone`\n",
      " |  \n",
      " |  coalesce(...)\n",
      " |      coalesce() -> Tensor\n",
      " |      \n",
      " |      Returns a coalesced copy of :attr:`self` if :attr:`self` is an\n",
      " |      :ref:`uncoalesced tensor <sparse-uncoalesced-coo-docs>`.\n",
      " |      \n",
      " |      Returns :attr:`self` if :attr:`self` is a coalesced tensor.\n",
      " |      \n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |  \n",
      " |  col_indices(...)\n",
      " |      col_indices() -> IntTensor\n",
      " |      \n",
      " |      Returns the tensor containing the column indices of the :attr:`self`\n",
      " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
      " |      The ``col_indices`` tensor is strictly of shape (:attr:`self`.nnz())\n",
      " |      and of type ``int32`` or ``int64``.  When using MKL routines such as sparse\n",
      " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
      " |      to avoid downcasting and potentially losing information.\n",
      " |      \n",
      " |      Example::\n",
      " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
      " |          >>> csr.col_indices()\n",
      " |          tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n",
      " |  \n",
      " |  conj(...)\n",
      " |      conj() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.conj`\n",
      " |  \n",
      " |  conj_physical(...)\n",
      " |      conj_physical() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.conj_physical`\n",
      " |  \n",
      " |  conj_physical_(...)\n",
      " |      conj_physical_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.conj_physical`\n",
      " |  \n",
      " |  contiguous(...)\n",
      " |      contiguous(memory_format=torch.contiguous_format) -> Tensor\n",
      " |      \n",
      " |      Returns a contiguous in memory tensor containing the same data as :attr:`self` tensor. If\n",
      " |      :attr:`self` tensor is already in the specified memory format, this function returns the\n",
      " |      :attr:`self` tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.contiguous_format``.\n",
      " |  \n",
      " |  copy_(...)\n",
      " |      copy_(src, non_blocking=False) -> Tensor\n",
      " |      \n",
      " |      Copies the elements from :attr:`src` into :attr:`self` tensor and returns\n",
      " |      :attr:`self`.\n",
      " |      \n",
      " |      The :attr:`src` tensor must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the :attr:`self` tensor. It may be of a different data type or reside on a\n",
      " |      different device.\n",
      " |      \n",
      " |      Args:\n",
      " |          src (Tensor): the source tensor to copy from\n",
      " |          non_blocking (bool): if ``True`` and this copy is between CPU and GPU,\n",
      " |              the copy may occur asynchronously with respect to the host. For other\n",
      " |              cases, this argument has no effect.\n",
      " |  \n",
      " |  copysign(...)\n",
      " |      copysign(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.copysign`\n",
      " |  \n",
      " |  copysign_(...)\n",
      " |      copysign_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.copysign`\n",
      " |  \n",
      " |  corrcoef(...)\n",
      " |      corrcoef() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.corrcoef`\n",
      " |  \n",
      " |  cos(...)\n",
      " |      cos() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cos`\n",
      " |  \n",
      " |  cos_(...)\n",
      " |      cos_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.cos`\n",
      " |  \n",
      " |  cosh(...)\n",
      " |      cosh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cosh`\n",
      " |  \n",
      " |  cosh_(...)\n",
      " |      cosh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.cosh`\n",
      " |  \n",
      " |  count_nonzero(...)\n",
      " |      count_nonzero(dim=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.count_nonzero`\n",
      " |  \n",
      " |  cov(...)\n",
      " |      cov(*, correction=1, fweights=None, aweights=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cov`\n",
      " |  \n",
      " |  cpu(...)\n",
      " |      cpu(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      Returns a copy of this object in CPU memory.\n",
      " |      \n",
      " |      If this object is already in CPU memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  cross(...)\n",
      " |      cross(other, dim=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cross`\n",
      " |  \n",
      " |  crow_indices(...)\n",
      " |      crow_indices() -> IntTensor\n",
      " |      \n",
      " |      Returns the tensor containing the compressed row indices of the :attr:`self`\n",
      " |      tensor when :attr:`self` is a sparse CSR tensor of layout ``sparse_csr``.\n",
      " |      The ``crow_indices`` tensor is strictly of shape (:attr:`self`.size(0) + 1)\n",
      " |      and of type ``int32`` or ``int64``. When using MKL routines such as sparse\n",
      " |      matrix multiplication, it is necessary to use ``int32`` indexing in order\n",
      " |      to avoid downcasting and potentially losing information.\n",
      " |      \n",
      " |      Example::\n",
      " |          >>> csr = torch.eye(5,5).to_sparse_csr()\n",
      " |          >>> csr.crow_indices()\n",
      " |          tensor([0, 1, 2, 3, 4, 5], dtype=torch.int32)\n",
      " |  \n",
      " |  cuda(...)\n",
      " |      cuda(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      Returns a copy of this object in CUDA memory.\n",
      " |      \n",
      " |      If this object is already in CUDA memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination GPU device.\n",
      " |              Defaults to the current CUDA device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  cummax(...)\n",
      " |      cummax(dim) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.cummax`\n",
      " |  \n",
      " |  cummin(...)\n",
      " |      cummin(dim) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.cummin`\n",
      " |  \n",
      " |  cumprod(...)\n",
      " |      cumprod(dim, dtype=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cumprod`\n",
      " |  \n",
      " |  cumprod_(...)\n",
      " |      cumprod_(dim, dtype=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.cumprod`\n",
      " |  \n",
      " |  cumsum(...)\n",
      " |      cumsum(dim, dtype=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.cumsum`\n",
      " |  \n",
      " |  cumsum_(...)\n",
      " |      cumsum_(dim, dtype=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.cumsum`\n",
      " |  \n",
      " |  data_ptr(...)\n",
      " |      data_ptr() -> int\n",
      " |      \n",
      " |      Returns the address of the first element of :attr:`self` tensor.\n",
      " |  \n",
      " |  deg2rad(...)\n",
      " |      deg2rad() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.deg2rad`\n",
      " |  \n",
      " |  deg2rad_(...)\n",
      " |      deg2rad_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.deg2rad`\n",
      " |  \n",
      " |  dense_dim(...)\n",
      " |      dense_dim() -> int\n",
      " |      \n",
      " |      Return the number of dense dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
      " |      \n",
      " |      .. note::\n",
      " |        Returns ``len(self.shape)`` if :attr:`self` is not a sparse tensor.\n",
      " |      \n",
      " |      See also :meth:`Tensor.sparse_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
      " |  \n",
      " |  dequantize(...)\n",
      " |      dequantize() -> Tensor\n",
      " |      \n",
      " |      Given a quantized Tensor, dequantize it and return the dequantized float Tensor.\n",
      " |  \n",
      " |  det(...)\n",
      " |      det() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.det`\n",
      " |  \n",
      " |  diag(...)\n",
      " |      diag(diagonal=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diag`\n",
      " |  \n",
      " |  diag_embed(...)\n",
      " |      diag_embed(offset=0, dim1=-2, dim2=-1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diag_embed`\n",
      " |  \n",
      " |  diagflat(...)\n",
      " |      diagflat(offset=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diagflat`\n",
      " |  \n",
      " |  diagonal(...)\n",
      " |      diagonal(offset=0, dim1=0, dim2=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diagonal`\n",
      " |  \n",
      " |  diagonal_scatter(...)\n",
      " |      diagonal_scatter(src, offset=0, dim1=0, dim2=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diagonal_scatter`\n",
      " |  \n",
      " |  diff(...)\n",
      " |      diff(n=1, dim=-1, prepend=None, append=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.diff`\n",
      " |  \n",
      " |  digamma(...)\n",
      " |      digamma() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.digamma`\n",
      " |  \n",
      " |  digamma_(...)\n",
      " |      digamma_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.digamma`\n",
      " |  \n",
      " |  dim(...)\n",
      " |      dim() -> int\n",
      " |      \n",
      " |      Returns the number of dimensions of :attr:`self` tensor.\n",
      " |  \n",
      " |  dist(...)\n",
      " |      dist(other, p=2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.dist`\n",
      " |  \n",
      " |  div(...)\n",
      " |      div(value, *, rounding_mode=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.div`\n",
      " |  \n",
      " |  div_(...)\n",
      " |      div_(value, *, rounding_mode=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.div`\n",
      " |  \n",
      " |  divide(...)\n",
      " |      divide(value, *, rounding_mode=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.divide`\n",
      " |  \n",
      " |  divide_(...)\n",
      " |      divide_(value, *, rounding_mode=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.divide`\n",
      " |  \n",
      " |  dot(...)\n",
      " |      dot(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.dot`\n",
      " |  \n",
      " |  double(...)\n",
      " |      double(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.double()`` is equivalent to ``self.to(torch.float64)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  dsplit(...)\n",
      " |      dsplit(split_size_or_sections) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.dsplit`\n",
      " |  \n",
      " |  element_size(...)\n",
      " |      element_size() -> int\n",
      " |      \n",
      " |      Returns the size in bytes of an individual element.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> torch.tensor([]).element_size()\n",
      " |          4\n",
      " |          >>> torch.tensor([], dtype=torch.uint8).element_size()\n",
      " |          1\n",
      " |  \n",
      " |  eq(...)\n",
      " |      eq(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.eq`\n",
      " |  \n",
      " |  eq_(...)\n",
      " |      eq_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.eq`\n",
      " |  \n",
      " |  equal(...)\n",
      " |      equal(other) -> bool\n",
      " |      \n",
      " |      See :func:`torch.equal`\n",
      " |  \n",
      " |  erf(...)\n",
      " |      erf() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.erf`\n",
      " |  \n",
      " |  erf_(...)\n",
      " |      erf_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.erf`\n",
      " |  \n",
      " |  erfc(...)\n",
      " |      erfc() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.erfc`\n",
      " |  \n",
      " |  erfc_(...)\n",
      " |      erfc_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.erfc`\n",
      " |  \n",
      " |  erfinv(...)\n",
      " |      erfinv() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.erfinv`\n",
      " |  \n",
      " |  erfinv_(...)\n",
      " |      erfinv_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.erfinv`\n",
      " |  \n",
      " |  exp(...)\n",
      " |      exp() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.exp`\n",
      " |  \n",
      " |  exp2(...)\n",
      " |      exp2() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.exp2`\n",
      " |  \n",
      " |  exp2_(...)\n",
      " |      exp2_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.exp2`\n",
      " |  \n",
      " |  exp_(...)\n",
      " |      exp_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.exp`\n",
      " |  \n",
      " |  expand(...)\n",
      " |      expand(*sizes) -> Tensor\n",
      " |      \n",
      " |      Returns a new view of the :attr:`self` tensor with singleton dimensions expanded\n",
      " |      to a larger size.\n",
      " |      \n",
      " |      Passing -1 as the size for a dimension means not changing the size of\n",
      " |      that dimension.\n",
      " |      \n",
      " |      Tensor can be also expanded to a larger number of dimensions, and the\n",
      " |      new ones will be appended at the front. For the new dimensions, the\n",
      " |      size cannot be set to -1.\n",
      " |      \n",
      " |      Expanding a tensor does not allocate new memory, but only creates a\n",
      " |      new view on the existing tensor where a dimension of size one is\n",
      " |      expanded to a larger size by setting the ``stride`` to 0. Any dimension\n",
      " |      of size 1 can be expanded to an arbitrary value without allocating new\n",
      " |      memory.\n",
      " |      \n",
      " |      Args:\n",
      " |          *sizes (torch.Size or int...): the desired expanded size\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          More than one element of an expanded tensor may refer to a single\n",
      " |          memory location. As a result, in-place operations (especially ones that\n",
      " |          are vectorized) may result in incorrect behavior. If you need to write\n",
      " |          to the tensors, please clone them first.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1], [2], [3]])\n",
      " |          >>> x.size()\n",
      " |          torch.Size([3, 1])\n",
      " |          >>> x.expand(3, 4)\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |          >>> x.expand(-1, 4)   # -1 means not changing the size of that dimension\n",
      " |          tensor([[ 1,  1,  1,  1],\n",
      " |                  [ 2,  2,  2,  2],\n",
      " |                  [ 3,  3,  3,  3]])\n",
      " |  \n",
      " |  expand_as(...)\n",
      " |      expand_as(other) -> Tensor\n",
      " |      \n",
      " |      Expand this tensor to the same size as :attr:`other`.\n",
      " |      ``self.expand_as(other)`` is equivalent to ``self.expand(other.size())``.\n",
      " |      \n",
      " |      Please see :meth:`~Tensor.expand` for more information about ``expand``.\n",
      " |      \n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
      " |              as :attr:`other`.\n",
      " |  \n",
      " |  expm1(...)\n",
      " |      expm1() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.expm1`\n",
      " |  \n",
      " |  expm1_(...)\n",
      " |      expm1_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.expm1`\n",
      " |  \n",
      " |  exponential_(...)\n",
      " |      exponential_(lambd=1, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with elements drawn from the exponential distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(x) = \\lambda e^{-\\lambda x}\n",
      " |  \n",
      " |  fill_(...)\n",
      " |      fill_(value) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with the specified value.\n",
      " |  \n",
      " |  fill_diagonal_(...)\n",
      " |      fill_diagonal_(fill_value, wrap=False) -> Tensor\n",
      " |      \n",
      " |      Fill the main diagonal of a tensor that has at least 2-dimensions.\n",
      " |      When dims>2, all dimensions of input must be of equal length.\n",
      " |      This function modifies the input tensor in-place, and returns the input tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          fill_value (Scalar): the fill value\n",
      " |          wrap (bool): the diagonal 'wrapped' after N columns for tall matrices.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> a = torch.zeros(3, 3)\n",
      " |          >>> a.fill_diagonal_(5)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.]])\n",
      " |          >>> b = torch.zeros(7, 3)\n",
      " |          >>> b.fill_diagonal_(5)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [0., 0., 0.]])\n",
      " |          >>> c = torch.zeros(7, 3)\n",
      " |          >>> c.fill_diagonal_(5, wrap=True)\n",
      " |          tensor([[5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.],\n",
      " |                  [0., 0., 0.],\n",
      " |                  [5., 0., 0.],\n",
      " |                  [0., 5., 0.],\n",
      " |                  [0., 0., 5.]])\n",
      " |  \n",
      " |  fix(...)\n",
      " |      fix() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fix`.\n",
      " |  \n",
      " |  fix_(...)\n",
      " |      fix_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.fix`\n",
      " |  \n",
      " |  flatten(...)\n",
      " |      flatten(start_dim=0, end_dim=-1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.flatten`\n",
      " |  \n",
      " |  flip(...)\n",
      " |      flip(dims) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.flip`\n",
      " |  \n",
      " |  fliplr(...)\n",
      " |      fliplr() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fliplr`\n",
      " |  \n",
      " |  flipud(...)\n",
      " |      flipud() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.flipud`\n",
      " |  \n",
      " |  float(...)\n",
      " |      float(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.float()`` is equivalent to ``self.to(torch.float32)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  float_power(...)\n",
      " |      float_power(exponent) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.float_power`\n",
      " |  \n",
      " |  float_power_(...)\n",
      " |      float_power_(exponent) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.float_power`\n",
      " |  \n",
      " |  floor(...)\n",
      " |      floor() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.floor`\n",
      " |  \n",
      " |  floor_(...)\n",
      " |      floor_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.floor`\n",
      " |  \n",
      " |  floor_divide(...)\n",
      " |      floor_divide(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.floor_divide`\n",
      " |  \n",
      " |  floor_divide_(...)\n",
      " |      floor_divide_(value) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.floor_divide`\n",
      " |  \n",
      " |  fmax(...)\n",
      " |      fmax(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fmax`\n",
      " |  \n",
      " |  fmin(...)\n",
      " |      fmin(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fmin`\n",
      " |  \n",
      " |  fmod(...)\n",
      " |      fmod(divisor) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.fmod`\n",
      " |  \n",
      " |  fmod_(...)\n",
      " |      fmod_(divisor) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.fmod`\n",
      " |  \n",
      " |  frac(...)\n",
      " |      frac() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.frac`\n",
      " |  \n",
      " |  frac_(...)\n",
      " |      frac_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.frac`\n",
      " |  \n",
      " |  frexp(...)\n",
      " |      frexp(input) -> (Tensor mantissa, Tensor exponent)\n",
      " |      \n",
      " |      See :func:`torch.frexp`\n",
      " |  \n",
      " |  gather(...)\n",
      " |      gather(dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gather`\n",
      " |  \n",
      " |  gcd(...)\n",
      " |      gcd(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gcd`\n",
      " |  \n",
      " |  gcd_(...)\n",
      " |      gcd_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.gcd`\n",
      " |  \n",
      " |  ge(...)\n",
      " |      ge(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ge`.\n",
      " |  \n",
      " |  ge_(...)\n",
      " |      ge_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ge`.\n",
      " |  \n",
      " |  geometric_(...)\n",
      " |      geometric_(p, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with elements drawn from the geometric distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(X=k) = (1 - p)^{k - 1} p\n",
      " |  \n",
      " |  geqrf(...)\n",
      " |      geqrf() -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.geqrf`\n",
      " |  \n",
      " |  ger(...)\n",
      " |      ger(vec2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ger`\n",
      " |  \n",
      " |  get_device(...)\n",
      " |      get_device() -> Device ordinal (Integer)\n",
      " |      \n",
      " |      For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.\n",
      " |      For CPU tensors, this function returns `-1`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.randn(3, 4, 5, device='cuda:0')\n",
      " |          >>> x.get_device()\n",
      " |          0\n",
      " |          >>> x.cpu().get_device()\n",
      " |          -1\n",
      " |  \n",
      " |  greater(...)\n",
      " |      greater(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.greater`.\n",
      " |  \n",
      " |  greater_(...)\n",
      " |      greater_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.greater`.\n",
      " |  \n",
      " |  greater_equal(...)\n",
      " |      greater_equal(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.greater_equal`.\n",
      " |  \n",
      " |  greater_equal_(...)\n",
      " |      greater_equal_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.greater_equal`.\n",
      " |  \n",
      " |  gt(...)\n",
      " |      gt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.gt`.\n",
      " |  \n",
      " |  gt_(...)\n",
      " |      gt_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.gt`.\n",
      " |  \n",
      " |  half(...)\n",
      " |      half(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.half()`` is equivalent to ``self.to(torch.float16)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  hardshrink(...)\n",
      " |      hardshrink(lambd=0.5) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.nn.functional.hardshrink`\n",
      " |  \n",
      " |  has_names(...)\n",
      " |      Is ``True`` if any of this tensor's dimensions are named. Otherwise, is ``False``.\n",
      " |  \n",
      " |  heaviside(...)\n",
      " |      heaviside(values) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.heaviside`\n",
      " |  \n",
      " |  heaviside_(...)\n",
      " |      heaviside_(values) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.heaviside`\n",
      " |  \n",
      " |  histc(...)\n",
      " |      histc(bins=100, min=0, max=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.histc`\n",
      " |  \n",
      " |  histogram(...)\n",
      " |      histogram(input, bins, *, range=None, weight=None, density=False) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.histogram`\n",
      " |  \n",
      " |  hsplit(...)\n",
      " |      hsplit(split_size_or_sections) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.hsplit`\n",
      " |  \n",
      " |  hypot(...)\n",
      " |      hypot(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.hypot`\n",
      " |  \n",
      " |  hypot_(...)\n",
      " |      hypot_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.hypot`\n",
      " |  \n",
      " |  i0(...)\n",
      " |      i0() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.i0`\n",
      " |  \n",
      " |  i0_(...)\n",
      " |      i0_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.i0`\n",
      " |  \n",
      " |  igamma(...)\n",
      " |      igamma(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.igamma`\n",
      " |  \n",
      " |  igamma_(...)\n",
      " |      igamma_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.igamma`\n",
      " |  \n",
      " |  igammac(...)\n",
      " |      igammac(other) -> Tensor\n",
      " |      See :func:`torch.igammac`\n",
      " |  \n",
      " |  igammac_(...)\n",
      " |      igammac_(other) -> Tensor\n",
      " |      In-place version of :meth:`~Tensor.igammac`\n",
      " |  \n",
      " |  index_add(...)\n",
      " |      index_add(dim, index, source, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_add_`.\n",
      " |  \n",
      " |  index_add_(...)\n",
      " |      index_add_(dim, index, source, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      Accumulate the elements of :attr:`alpha` times ``source`` into the :attr:`self`\n",
      " |      tensor by adding to the indices in the order given in :attr:`index`. For example,\n",
      " |      if ``dim == 0``, ``index[i] == j``, and ``alpha=-1``, then the ``i``\\ th row of\n",
      " |      ``source`` is subtracted from the ``j``\\ th row of :attr:`self`.\n",
      " |      \n",
      " |      The :attr:`dim`\\ th dimension of ``source`` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |      \n",
      " |      For a 3-D tensor the output is given as::\n",
      " |      \n",
      " |          self[index[i], :, :] += alpha * src[i, :, :]  # if dim == 0\n",
      " |          self[:, index[i], :] += alpha * src[:, i, :]  # if dim == 1\n",
      " |          self[:, :, index[i]] += alpha * src[:, :, i]  # if dim == 2\n",
      " |      \n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (Tensor): indices of ``source`` to select from,\n",
      " |                  should have dtype either `torch.int64` or `torch.int32`\n",
      " |          source (Tensor): the tensor containing values to add\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          alpha (Number): the scalar multiplier for ``source``\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.ones(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_add_(0, index, t)\n",
      " |          tensor([[  2.,   3.,   4.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  8.,   9.,  10.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  5.,   6.,   7.]])\n",
      " |          >>> x.index_add_(0, index, t, alpha=-1)\n",
      " |          tensor([[  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.],\n",
      " |                  [  1.,   1.,   1.]])\n",
      " |  \n",
      " |  index_copy(...)\n",
      " |      index_copy(dim, index, tensor2) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_copy_`.\n",
      " |  \n",
      " |  index_copy_(...)\n",
      " |      index_copy_(dim, index, tensor) -> Tensor\n",
      " |      \n",
      " |      Copies the elements of :attr:`tensor` into the :attr:`self` tensor by selecting\n",
      " |      the indices in the order given in :attr:`index`. For example, if ``dim == 0``\n",
      " |      and ``index[i] == j``, then the ``i``\\ th row of :attr:`tensor` is copied to the\n",
      " |      ``j``\\ th row of :attr:`self`.\n",
      " |      \n",
      " |      The :attr:`dim`\\ th dimension of :attr:`tensor` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |      \n",
      " |      .. note::\n",
      " |          If :attr:`index` contains duplicate entries, multiple elements from\n",
      " |          :attr:`tensor` will be copied to the same index of :attr:`self`. The result\n",
      " |          is nondeterministic since it depends on which copy occurs last.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`tensor` to select from\n",
      " |          tensor (Tensor): the tensor containing values to copy\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.zeros(5, 3)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2])\n",
      " |          >>> x.index_copy_(0, index, t)\n",
      " |          tensor([[ 1.,  2.,  3.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 7.,  8.,  9.],\n",
      " |                  [ 0.,  0.,  0.],\n",
      " |                  [ 4.,  5.,  6.]])\n",
      " |  \n",
      " |  index_fill(...)\n",
      " |      index_fill(dim, index, value) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.index_fill_`.\n",
      " |  \n",
      " |  index_fill_(...)\n",
      " |      index_fill_(dim, index, value) -> Tensor\n",
      " |      \n",
      " |      Fills the elements of the :attr:`self` tensor with value :attr:`value` by\n",
      " |      selecting the indices in the order given in :attr:`index`.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (LongTensor): indices of :attr:`self` tensor to fill in\n",
      " |          value (float): the value to fill with\n",
      " |      \n",
      " |      Example::\n",
      " |          >>> x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 2])\n",
      " |          >>> x.index_fill_(1, index, -1)\n",
      " |          tensor([[-1.,  2., -1.],\n",
      " |                  [-1.,  5., -1.],\n",
      " |                  [-1.,  8., -1.]])\n",
      " |  \n",
      " |  index_put(...)\n",
      " |      index_put(indices, values, accumulate=False) -> Tensor\n",
      " |      \n",
      " |      Out-place version of :meth:`~Tensor.index_put_`.\n",
      " |  \n",
      " |  index_put_(...)\n",
      " |      index_put_(indices, values, accumulate=False) -> Tensor\n",
      " |      \n",
      " |      Puts values from the tensor :attr:`values` into the tensor :attr:`self` using\n",
      " |      the indices specified in :attr:`indices` (which is a tuple of Tensors). The\n",
      " |      expression ``tensor.index_put_(indices, values)`` is equivalent to\n",
      " |      ``tensor[indices] = values``. Returns :attr:`self`.\n",
      " |      \n",
      " |      If :attr:`accumulate` is ``True``, the elements in :attr:`values` are added to\n",
      " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if indices\n",
      " |      contain duplicate elements.\n",
      " |      \n",
      " |      Args:\n",
      " |          indices (tuple of LongTensor): tensors used to index into `self`.\n",
      " |          values (Tensor): tensor of same dtype as `self`.\n",
      " |          accumulate (bool): whether to accumulate into self\n",
      " |  \n",
      " |  index_reduce(...)\n",
      " |  \n",
      " |  index_reduce_(...)\n",
      " |      index_reduce_(dim, index, source, reduce, *, include_self=True) -> Tensor\n",
      " |      \n",
      " |      Accumulate the elements of ``source`` into the :attr:`self`\n",
      " |      tensor by accumulating to the indices in the order given in :attr:`index`\n",
      " |      using the reduction given by the ``reduce`` argument. For example, if ``dim == 0``,\n",
      " |      ``index[i] == j``, ``reduce == prod`` and ``include_self == True`` then the ``i``\\ th\n",
      " |      row of ``source`` is multiplied by the ``j``\\ th row of :attr:`self`. If\n",
      " |      :obj:`include_self=\"True\"`, the values in the :attr:`self` tensor are included\n",
      " |      in the reduction, otherwise, rows in the :attr:`self` tensor that are accumulated\n",
      " |      to are treated as if they were filled with the reduction identites.\n",
      " |      \n",
      " |      The :attr:`dim`\\ th dimension of ``source`` must have the same size as the\n",
      " |      length of :attr:`index` (which must be a vector), and all other dimensions must\n",
      " |      match :attr:`self`, or an error will be raised.\n",
      " |      \n",
      " |      For a 3-D tensor with :obj:`reduce=\"prod\"` and :obj:`include_self=True` the\n",
      " |      output is given as::\n",
      " |      \n",
      " |          self[index[i], :, :] *= src[i, :, :]  # if dim == 0\n",
      " |          self[:, index[i], :] *= src[:, i, :]  # if dim == 1\n",
      " |          self[:, :, index[i]] *= src[:, :, i]  # if dim == 2\n",
      " |      \n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          This function only supports floating point tensors.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          This function is in beta and may change in the near future.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): dimension along which to index\n",
      " |          index (Tensor): indices of ``source`` to select from,\n",
      " |              should have dtype either `torch.int64` or `torch.int32`\n",
      " |          source (FloatTensor): the tensor containing values to accumulate\n",
      " |          reduce (str): the reduction operation to apply\n",
      " |              (:obj:`\"prod\"`, :obj:`\"mean\"`, :obj:`\"amax\"`, :obj:`\"amin\"`)\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          include_self (bool): whether the elements from the ``self`` tensor are\n",
      " |              included in the reduction\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.empty(5, 3).fill_(2)\n",
      " |          >>> t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]], dtype=torch.float)\n",
      " |          >>> index = torch.tensor([0, 4, 2, 0])\n",
      " |          >>> x.index_reduce_(0, index, t, 'prod')\n",
      " |          tensor([[20., 44., 72.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [14., 16., 18.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 8., 10., 12.]])\n",
      " |          >>> x = torch.empty(5, 3).fill_(2)\n",
      " |          >>> x.index_reduce_(0, index, t, 'prod', include_self=False)\n",
      " |          tensor([[10., 22., 36.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 7.,  8.,  9.],\n",
      " |                  [ 2.,  2.,  2.],\n",
      " |                  [ 4.,  5.,  6.]])\n",
      " |  \n",
      " |  index_select(...)\n",
      " |      index_select(dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.index_select`\n",
      " |  \n",
      " |  indices(...)\n",
      " |      indices() -> Tensor\n",
      " |      \n",
      " |      Return the indices tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |      \n",
      " |      See also :meth:`Tensor.values`.\n",
      " |      \n",
      " |      .. note::\n",
      " |        This method can only be called on a coalesced sparse tensor. See\n",
      " |        :meth:`Tensor.coalesce` for details.\n",
      " |  \n",
      " |  inner(...)\n",
      " |      inner(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.inner`.\n",
      " |  \n",
      " |  int(...)\n",
      " |      int(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.int()`` is equivalent to ``self.to(torch.int32)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  int_repr(...)\n",
      " |      int_repr() -> Tensor\n",
      " |      \n",
      " |      Given a quantized Tensor,\n",
      " |      ``self.int_repr()`` returns a CPU Tensor with uint8_t as data type that stores the\n",
      " |      underlying uint8_t values of the given Tensor.\n",
      " |  \n",
      " |  inverse(...)\n",
      " |      inverse() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.inverse`\n",
      " |  \n",
      " |  ipu(...)\n",
      " |      ipu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      Returns a copy of this object in IPU memory.\n",
      " |      \n",
      " |      If this object is already in IPU memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination IPU device.\n",
      " |              Defaults to the current IPU device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  is_coalesced(...)\n",
      " |      is_coalesced() -> bool\n",
      " |      \n",
      " |      Returns ``True`` if :attr:`self` is a :ref:`sparse COO tensor\n",
      " |      <sparse-coo-docs>` that is coalesced, ``False`` otherwise.\n",
      " |      \n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |      \n",
      " |      See :meth:`coalesce` and :ref:`uncoalesced tensors <sparse-uncoalesced-coo-docs>`.\n",
      " |  \n",
      " |  is_complex(...)\n",
      " |      is_complex() -> bool\n",
      " |      \n",
      " |      Returns True if the data type of :attr:`self` is a complex data type.\n",
      " |  \n",
      " |  is_conj(...)\n",
      " |      is_conj() -> bool\n",
      " |      \n",
      " |      Returns True if the conjugate bit of :attr:`self` is set to true.\n",
      " |  \n",
      " |  is_contiguous(...)\n",
      " |      is_contiguous(memory_format=torch.contiguous_format) -> bool\n",
      " |      \n",
      " |      Returns True if :attr:`self` tensor is contiguous in memory in the order specified\n",
      " |      by memory format.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): Specifies memory allocation\n",
      " |              order. Default: ``torch.contiguous_format``.\n",
      " |  \n",
      " |  is_distributed(...)\n",
      " |  \n",
      " |  is_floating_point(...)\n",
      " |      is_floating_point() -> bool\n",
      " |      \n",
      " |      Returns True if the data type of :attr:`self` is a floating point data type.\n",
      " |  \n",
      " |  is_inference(...)\n",
      " |      is_inference() -> bool\n",
      " |      \n",
      " |      See :func:`torch.is_inference`\n",
      " |  \n",
      " |  is_neg(...)\n",
      " |      is_neg() -> bool\n",
      " |      \n",
      " |      Returns True if the negative bit of :attr:`self` is set to true.\n",
      " |  \n",
      " |  is_nonzero(...)\n",
      " |  \n",
      " |  is_pinned(...)\n",
      " |      Returns true if this tensor resides in pinned memory.\n",
      " |  \n",
      " |  is_same_size(...)\n",
      " |  \n",
      " |  is_set_to(...)\n",
      " |      is_set_to(tensor) -> bool\n",
      " |      \n",
      " |      Returns True if both tensors are pointing to the exact same memory (same\n",
      " |      storage, offset, size and stride).\n",
      " |  \n",
      " |  is_signed(...)\n",
      " |      is_signed() -> bool\n",
      " |      \n",
      " |      Returns True if the data type of :attr:`self` is a signed data type.\n",
      " |  \n",
      " |  isclose(...)\n",
      " |      isclose(other, rtol=1e-05, atol=1e-08, equal_nan=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.isclose`\n",
      " |  \n",
      " |  isfinite(...)\n",
      " |      isfinite() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.isfinite`\n",
      " |  \n",
      " |  isinf(...)\n",
      " |      isinf() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.isinf`\n",
      " |  \n",
      " |  isnan(...)\n",
      " |      isnan() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.isnan`\n",
      " |  \n",
      " |  isneginf(...)\n",
      " |      isneginf() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.isneginf`\n",
      " |  \n",
      " |  isposinf(...)\n",
      " |      isposinf() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.isposinf`\n",
      " |  \n",
      " |  isreal(...)\n",
      " |      isreal() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.isreal`\n",
      " |  \n",
      " |  item(...)\n",
      " |      item() -> number\n",
      " |      \n",
      " |      Returns the value of this tensor as a standard Python number. This only works\n",
      " |      for tensors with one element. For other cases, see :meth:`~Tensor.tolist`.\n",
      " |      \n",
      " |      This operation is not differentiable.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([1.0])\n",
      " |          >>> x.item()\n",
      " |          1.0\n",
      " |  \n",
      " |  kron(...)\n",
      " |      kron(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.kron`\n",
      " |  \n",
      " |  kthvalue(...)\n",
      " |      kthvalue(k, dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.kthvalue`\n",
      " |  \n",
      " |  lcm(...)\n",
      " |      lcm(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lcm`\n",
      " |  \n",
      " |  lcm_(...)\n",
      " |      lcm_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.lcm`\n",
      " |  \n",
      " |  ldexp(...)\n",
      " |      ldexp(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ldexp`\n",
      " |  \n",
      " |  ldexp_(...)\n",
      " |      ldexp_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ldexp`\n",
      " |  \n",
      " |  le(...)\n",
      " |      le(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.le`.\n",
      " |  \n",
      " |  le_(...)\n",
      " |      le_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.le`.\n",
      " |  \n",
      " |  lerp(...)\n",
      " |      lerp(end, weight) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lerp`\n",
      " |  \n",
      " |  lerp_(...)\n",
      " |      lerp_(end, weight) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.lerp`\n",
      " |  \n",
      " |  less(...)\n",
      " |      lt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.less`.\n",
      " |  \n",
      " |  less_(...)\n",
      " |      less_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.less`.\n",
      " |  \n",
      " |  less_equal(...)\n",
      " |      less_equal(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.less_equal`.\n",
      " |  \n",
      " |  less_equal_(...)\n",
      " |      less_equal_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.less_equal`.\n",
      " |  \n",
      " |  lgamma(...)\n",
      " |      lgamma() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lgamma`\n",
      " |  \n",
      " |  lgamma_(...)\n",
      " |      lgamma_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.lgamma`\n",
      " |  \n",
      " |  log(...)\n",
      " |      log() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log`\n",
      " |  \n",
      " |  log10(...)\n",
      " |      log10() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log10`\n",
      " |  \n",
      " |  log10_(...)\n",
      " |      log10_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log10`\n",
      " |  \n",
      " |  log1p(...)\n",
      " |      log1p() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log1p`\n",
      " |  \n",
      " |  log1p_(...)\n",
      " |      log1p_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log1p`\n",
      " |  \n",
      " |  log2(...)\n",
      " |      log2() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.log2`\n",
      " |  \n",
      " |  log2_(...)\n",
      " |      log2_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log2`\n",
      " |  \n",
      " |  log_(...)\n",
      " |      log_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.log`\n",
      " |  \n",
      " |  log_normal_(...)\n",
      " |      log_normal_(mean=1, std=2, *, generator=None)\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with numbers samples from the log-normal distribution\n",
      " |      parameterized by the given mean :math:`\\mu` and standard deviation\n",
      " |      :math:`\\sigma`. Note that :attr:`mean` and :attr:`std` are the mean and\n",
      " |      standard deviation of the underlying normal distribution, and not of the\n",
      " |      returned distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |          f(x) = \\dfrac{1}{x \\sigma \\sqrt{2\\pi}}\\ e^{-\\frac{(\\ln x - \\mu)^2}{2\\sigma^2}}\n",
      " |  \n",
      " |  log_softmax(...)\n",
      " |  \n",
      " |  logaddexp(...)\n",
      " |      logaddexp(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logaddexp`\n",
      " |  \n",
      " |  logaddexp2(...)\n",
      " |      logaddexp2(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logaddexp2`\n",
      " |  \n",
      " |  logcumsumexp(...)\n",
      " |      logcumsumexp(dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logcumsumexp`\n",
      " |  \n",
      " |  logdet(...)\n",
      " |      logdet() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logdet`\n",
      " |  \n",
      " |  logical_and(...)\n",
      " |      logical_and() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logical_and`\n",
      " |  \n",
      " |  logical_and_(...)\n",
      " |      logical_and_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.logical_and`\n",
      " |  \n",
      " |  logical_not(...)\n",
      " |      logical_not() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logical_not`\n",
      " |  \n",
      " |  logical_not_(...)\n",
      " |      logical_not_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.logical_not`\n",
      " |  \n",
      " |  logical_or(...)\n",
      " |      logical_or() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logical_or`\n",
      " |  \n",
      " |  logical_or_(...)\n",
      " |      logical_or_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.logical_or`\n",
      " |  \n",
      " |  logical_xor(...)\n",
      " |      logical_xor() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logical_xor`\n",
      " |  \n",
      " |  logical_xor_(...)\n",
      " |      logical_xor_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.logical_xor`\n",
      " |  \n",
      " |  logit(...)\n",
      " |      logit() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logit`\n",
      " |  \n",
      " |  logit_(...)\n",
      " |      logit_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.logit`\n",
      " |  \n",
      " |  logsumexp(...)\n",
      " |      logsumexp(dim, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.logsumexp`\n",
      " |  \n",
      " |  long(...)\n",
      " |      long(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.long()`` is equivalent to ``self.to(torch.int64)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  lt(...)\n",
      " |      lt(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lt`.\n",
      " |  \n",
      " |  lt_(...)\n",
      " |      lt_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.lt`.\n",
      " |  \n",
      " |  lu_solve(...)\n",
      " |      lu_solve(LU_data, LU_pivots) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.lu_solve`\n",
      " |  \n",
      " |  map2_(...)\n",
      " |  \n",
      " |  map_(...)\n",
      " |      map_(tensor, callable)\n",
      " |      \n",
      " |      Applies :attr:`callable` for each element in :attr:`self` tensor and the given\n",
      " |      :attr:`tensor` and stores the results in :attr:`self` tensor. :attr:`self` tensor and\n",
      " |      the given :attr:`tensor` must be :ref:`broadcastable <broadcasting-semantics>`.\n",
      " |      \n",
      " |      The :attr:`callable` should have the signature::\n",
      " |      \n",
      " |          def callable(a, b) -> number\n",
      " |  \n",
      " |  masked_fill(...)\n",
      " |      masked_fill(mask, value) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.masked_fill_`\n",
      " |  \n",
      " |  masked_fill_(...)\n",
      " |      masked_fill_(mask, value)\n",
      " |      \n",
      " |      Fills elements of :attr:`self` tensor with :attr:`value` where :attr:`mask` is\n",
      " |      True. The shape of :attr:`mask` must be\n",
      " |      :ref:`broadcastable <broadcasting-semantics>` with the shape of the underlying\n",
      " |      tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          mask (BoolTensor): the boolean mask\n",
      " |          value (float): the value to fill in with\n",
      " |  \n",
      " |  masked_scatter(...)\n",
      " |      masked_scatter(mask, tensor) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.masked_scatter_`\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The inputs :attr:`self` and :attr:`mask`\n",
      " |          :ref:`broadcast <broadcasting-semantics>`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          >>> self = torch.tensor([0, 0, 0, 0, 0])\n",
      " |          >>> mask = torch.tensor([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]])\n",
      " |          >>> source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
      " |          >>> self.masked_scatter(mask, source)\n",
      " |          tensor([[0, 0, 0, 0, 1],\n",
      " |                  [2, 3, 0, 4, 5]])\n",
      " |  \n",
      " |  masked_scatter_(...)\n",
      " |      masked_scatter_(mask, source)\n",
      " |      \n",
      " |      Copies elements from :attr:`source` into :attr:`self` tensor at positions where\n",
      " |      the :attr:`mask` is True. Elements from :attr:`source` are copied into :attr:`self`\n",
      " |      starting at position 0 of :attr:`source` and continuing in order one-by-one for each\n",
      " |      occurrence of :attr:`mask` being True.\n",
      " |      The shape of :attr:`mask` must be :ref:`broadcastable <broadcasting-semantics>`\n",
      " |      with the shape of the underlying tensor. The :attr:`source` should have at least\n",
      " |      as many elements as the number of ones in :attr:`mask`.\n",
      " |      \n",
      " |      Args:\n",
      " |          mask (BoolTensor): the boolean mask\n",
      " |          source (Tensor): the tensor to copy from\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The :attr:`mask` operates on the :attr:`self` tensor, not on the given\n",
      " |          :attr:`source` tensor.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |          >>> self = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 0, 0]])\n",
      " |          >>> mask = torch.tensor([[0, 0, 0, 1, 1], [1, 1, 0, 1, 1]])\n",
      " |          >>> source = torch.tensor([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])\n",
      " |          >>> self.masked_scatter_(mask, source)\n",
      " |          tensor([[0, 0, 0, 0, 1],\n",
      " |                  [2, 3, 0, 4, 5]])\n",
      " |  \n",
      " |  masked_select(...)\n",
      " |      masked_select(mask) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.masked_select`\n",
      " |  \n",
      " |  matmul(...)\n",
      " |      matmul(tensor2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.matmul`\n",
      " |  \n",
      " |  matrix_exp(...)\n",
      " |      matrix_exp() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.matrix_exp`\n",
      " |  \n",
      " |  matrix_power(...)\n",
      " |      matrix_power(n) -> Tensor\n",
      " |      \n",
      " |      .. note:: :meth:`~Tensor.matrix_power` is deprecated, use :func:`torch.linalg.matrix_power` instead.\n",
      " |      \n",
      " |      Alias for :func:`torch.linalg.matrix_power`\n",
      " |  \n",
      " |  max(...)\n",
      " |      max(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.max`\n",
      " |  \n",
      " |  maximum(...)\n",
      " |      maximum(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.maximum`\n",
      " |  \n",
      " |  mean(...)\n",
      " |      mean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mean`\n",
      " |  \n",
      " |  median(...)\n",
      " |      median(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.median`\n",
      " |  \n",
      " |  min(...)\n",
      " |      min(dim=None, keepdim=False) -> Tensor or (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.min`\n",
      " |  \n",
      " |  minimum(...)\n",
      " |      minimum(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.minimum`\n",
      " |  \n",
      " |  mm(...)\n",
      " |      mm(mat2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mm`\n",
      " |  \n",
      " |  mode(...)\n",
      " |      mode(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.mode`\n",
      " |  \n",
      " |  moveaxis(...)\n",
      " |      moveaxis(source, destination) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.moveaxis`\n",
      " |  \n",
      " |  movedim(...)\n",
      " |      movedim(source, destination) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.movedim`\n",
      " |  \n",
      " |  msort(...)\n",
      " |      msort() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.msort`\n",
      " |  \n",
      " |  mul(...)\n",
      " |      mul(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mul`.\n",
      " |  \n",
      " |  mul_(...)\n",
      " |      mul_(value) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.mul`.\n",
      " |  \n",
      " |  multinomial(...)\n",
      " |      multinomial(num_samples, replacement=False, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.multinomial`\n",
      " |  \n",
      " |  multiply(...)\n",
      " |      multiply(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.multiply`.\n",
      " |  \n",
      " |  multiply_(...)\n",
      " |      multiply_(value) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.multiply`.\n",
      " |  \n",
      " |  mv(...)\n",
      " |      mv(vec) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mv`\n",
      " |  \n",
      " |  mvlgamma(...)\n",
      " |      mvlgamma(p) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.mvlgamma`\n",
      " |  \n",
      " |  mvlgamma_(...)\n",
      " |      mvlgamma_(p) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.mvlgamma`\n",
      " |  \n",
      " |  nan_to_num(...)\n",
      " |      nan_to_num(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.nan_to_num`.\n",
      " |  \n",
      " |  nan_to_num_(...)\n",
      " |      nan_to_num_(nan=0.0, posinf=None, neginf=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.nan_to_num`.\n",
      " |  \n",
      " |  nanmean(...)\n",
      " |      nanmean(dim=None, keepdim=False, *, dtype=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.nanmean`\n",
      " |  \n",
      " |  nanmedian(...)\n",
      " |      nanmedian(dim=None, keepdim=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.nanmedian`\n",
      " |  \n",
      " |  nanquantile(...)\n",
      " |      nanquantile(q, dim=None, keepdim=False, *, interpolation='linear') -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.nanquantile`\n",
      " |  \n",
      " |  nansum(...)\n",
      " |      nansum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.nansum`\n",
      " |  \n",
      " |  narrow(...)\n",
      " |      narrow(dimension, start, length) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.narrow`.\n",
      " |  \n",
      " |  narrow_copy(...)\n",
      " |      narrow_copy(dimension, start, length) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.narrow_copy`.\n",
      " |  \n",
      " |  ndimension(...)\n",
      " |      ndimension() -> int\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |  \n",
      " |  ne(...)\n",
      " |      ne(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ne`.\n",
      " |  \n",
      " |  ne_(...)\n",
      " |      ne_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.ne`.\n",
      " |  \n",
      " |  neg(...)\n",
      " |      neg() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.neg`\n",
      " |  \n",
      " |  neg_(...)\n",
      " |      neg_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.neg`\n",
      " |  \n",
      " |  negative(...)\n",
      " |      negative() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.negative`\n",
      " |  \n",
      " |  negative_(...)\n",
      " |      negative_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.negative`\n",
      " |  \n",
      " |  nelement(...)\n",
      " |      nelement() -> int\n",
      " |      \n",
      " |      Alias for :meth:`~Tensor.numel`\n",
      " |  \n",
      " |  new(...)\n",
      " |  \n",
      " |  new_empty(...)\n",
      " |      new_empty(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |      \n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with uninitialized data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.ones(())\n",
      " |          >>> tensor.new_empty((2, 3))\n",
      " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
      " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
      " |  \n",
      " |  new_empty_strided(...)\n",
      " |      new_empty_strided(size, stride, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |      \n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` and strides :attr:`stride` filled with\n",
      " |      uninitialized data. By default, the returned Tensor has the same\n",
      " |      :class:`torch.dtype` and :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.ones(())\n",
      " |          >>> tensor.new_empty_strided((2, 3), (3, 1))\n",
      " |          tensor([[ 5.8182e-18,  4.5765e-41, -1.0545e+30],\n",
      " |                  [ 3.0949e-41,  4.4842e-44,  0.0000e+00]])\n",
      " |  \n",
      " |  new_full(...)\n",
      " |      new_full(size, fill_value, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |      \n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with :attr:`fill_value`.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          fill_value (scalar): the number to fill the output tensor with.\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.float64)\n",
      " |          >>> tensor.new_full((3, 4), 3.141592)\n",
      " |          tensor([[ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416],\n",
      " |                  [ 3.1416,  3.1416,  3.1416,  3.1416]], dtype=torch.float64)\n",
      " |  \n",
      " |  new_ones(...)\n",
      " |      new_ones(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |      \n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with ``1``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.tensor((), dtype=torch.int32)\n",
      " |          >>> tensor.new_ones((2, 3))\n",
      " |          tensor([[ 1,  1,  1],\n",
      " |                  [ 1,  1,  1]], dtype=torch.int32)\n",
      " |  \n",
      " |  new_tensor(...)\n",
      " |      new_tensor(data, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |      \n",
      " |      \n",
      " |      Returns a new Tensor with :attr:`data` as the tensor data.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          :func:`new_tensor` always copies :attr:`data`. If you have a Tensor\n",
      " |          ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
      " |          or :func:`torch.Tensor.detach`.\n",
      " |          If you have a numpy array and want to avoid a copy, use\n",
      " |          :func:`torch.from_numpy`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          When data is a tensor `x`, :func:`new_tensor()` reads out 'the data' from whatever it is passed,\n",
      " |          and constructs a leaf variable. Therefore ``tensor.new_tensor(x)`` is equivalent to ``x.clone().detach()``\n",
      " |          and ``tensor.new_tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``.\n",
      " |          The equivalents using ``clone()`` and ``detach()`` are recommended.\n",
      " |      \n",
      " |      Args:\n",
      " |          data (array_like): The returned Tensor copies :attr:`data`.\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.ones((2,), dtype=torch.int8)\n",
      " |          >>> data = [[0, 1], [2, 3]]\n",
      " |          >>> tensor.new_tensor(data)\n",
      " |          tensor([[ 0,  1],\n",
      " |                  [ 2,  3]], dtype=torch.int8)\n",
      " |  \n",
      " |  new_zeros(...)\n",
      " |      new_zeros(size, *, dtype=None, device=None, requires_grad=False, layout=torch.strided, pin_memory=False) -> Tensor\n",
      " |      \n",
      " |      \n",
      " |      Returns a Tensor of size :attr:`size` filled with ``0``.\n",
      " |      By default, the returned Tensor has the same :class:`torch.dtype` and\n",
      " |      :class:`torch.device` as this tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (int...): a list, tuple, or :class:`torch.Size` of integers defining the\n",
      " |              shape of the output tensor.\n",
      " |      \n",
      " |      Keyword args:\n",
      " |          dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n",
      " |              Default: if None, same :class:`torch.dtype` as this tensor.\n",
      " |          device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      " |              Default: if None, same :class:`torch.device` as this tensor.\n",
      " |          requires_grad (bool, optional): If autograd should record operations on the\n",
      " |              returned tensor. Default: ``False``.\n",
      " |          layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      " |              Default: ``torch.strided``.\n",
      " |          pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      " |              the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.tensor((), dtype=torch.float64)\n",
      " |          >>> tensor.new_zeros((2, 3))\n",
      " |          tensor([[ 0.,  0.,  0.],\n",
      " |                  [ 0.,  0.,  0.]], dtype=torch.float64)\n",
      " |  \n",
      " |  nextafter(...)\n",
      " |      nextafter(other) -> Tensor\n",
      " |      See :func:`torch.nextafter`\n",
      " |  \n",
      " |  nextafter_(...)\n",
      " |      nextafter_(other) -> Tensor\n",
      " |      In-place version of :meth:`~Tensor.nextafter`\n",
      " |  \n",
      " |  nonzero(...)\n",
      " |      nonzero() -> LongTensor\n",
      " |      \n",
      " |      See :func:`torch.nonzero`\n",
      " |  \n",
      " |  normal_(...)\n",
      " |      normal_(mean=0, std=1, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with elements samples from the normal distribution\n",
      " |      parameterized by :attr:`mean` and :attr:`std`.\n",
      " |  \n",
      " |  not_equal(...)\n",
      " |      not_equal(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.not_equal`.\n",
      " |  \n",
      " |  not_equal_(...)\n",
      " |      not_equal_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.not_equal`.\n",
      " |  \n",
      " |  numel(...)\n",
      " |      numel() -> int\n",
      " |      \n",
      " |      See :func:`torch.numel`\n",
      " |  \n",
      " |  numpy(...)\n",
      " |      numpy(*, force=False) -> numpy.ndarray\n",
      " |      \n",
      " |      Returns the tensor as a NumPy :class:`ndarray`.\n",
      " |      \n",
      " |      If :attr:`force` is ``False`` (the default), the conversion\n",
      " |      is performed only if the tensor is on the CPU, does not require grad,\n",
      " |      does not have its conjugate bit set, and is a dtype and layout that\n",
      " |      NumPy supports. The returned ndarray and the tensor will share their\n",
      " |      storage, so changes to the tensor will be reflected in the ndarray\n",
      " |      and vice versa.\n",
      " |      \n",
      " |      If :attr:`force` is ``True`` this is equivalent to\n",
      " |      calling ``t.detach().cpu().resolve_conj().resolve_neg().numpy()``.\n",
      " |      If the tensor isn't on the CPU or the conjugate or negative bit is set,\n",
      " |      the tensor won't share its storage with the returned ndarray.\n",
      " |      Setting :attr:`force` to ``True`` can be a useful shorthand.\n",
      " |      \n",
      " |      Args:\n",
      " |          force (bool): if ``True``, the ndarray may be a copy of the tensor\n",
      " |                     instead of always sharing memory, defaults to ``False``.\n",
      " |  \n",
      " |  orgqr(...)\n",
      " |      orgqr(input2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.orgqr`\n",
      " |  \n",
      " |  ormqr(...)\n",
      " |      ormqr(input2, input3, left=True, transpose=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.ormqr`\n",
      " |  \n",
      " |  outer(...)\n",
      " |      outer(vec2) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.outer`.\n",
      " |  \n",
      " |  permute(...)\n",
      " |      permute(*dims) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.permute`\n",
      " |  \n",
      " |  pin_memory(...)\n",
      " |      pin_memory() -> Tensor\n",
      " |      \n",
      " |      Copies the tensor to pinned memory, if it's not already pinned.\n",
      " |  \n",
      " |  pinverse(...)\n",
      " |      pinverse() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.pinverse`\n",
      " |  \n",
      " |  polygamma(...)\n",
      " |      polygamma(n) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.polygamma`\n",
      " |  \n",
      " |  polygamma_(...)\n",
      " |      polygamma_(n) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.polygamma`\n",
      " |  \n",
      " |  positive(...)\n",
      " |      positive() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.positive`\n",
      " |  \n",
      " |  pow(...)\n",
      " |      pow(exponent) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.pow`\n",
      " |  \n",
      " |  pow_(...)\n",
      " |      pow_(exponent) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.pow`\n",
      " |  \n",
      " |  prelu(...)\n",
      " |  \n",
      " |  prod(...)\n",
      " |      prod(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.prod`\n",
      " |  \n",
      " |  put(...)\n",
      " |      put(input, index, source, accumulate=False) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.put_`.\n",
      " |      `input` corresponds to `self` in :meth:`torch.Tensor.put_`.\n",
      " |  \n",
      " |  put_(...)\n",
      " |      put_(index, source, accumulate=False) -> Tensor\n",
      " |      \n",
      " |      Copies the elements from :attr:`source` into the positions specified by\n",
      " |      :attr:`index`. For the purpose of indexing, the :attr:`self` tensor is treated as if\n",
      " |      it were a 1-D tensor.\n",
      " |      \n",
      " |      :attr:`index` and :attr:`source` need to have the same number of elements, but not necessarily\n",
      " |      the same shape.\n",
      " |      \n",
      " |      If :attr:`accumulate` is ``True``, the elements in :attr:`source` are added to\n",
      " |      :attr:`self`. If accumulate is ``False``, the behavior is undefined if :attr:`index`\n",
      " |      contain duplicate elements.\n",
      " |      \n",
      " |      Args:\n",
      " |          index (LongTensor): the indices into self\n",
      " |          source (Tensor): the tensor containing values to copy from\n",
      " |          accumulate (bool): whether to accumulate into self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> src = torch.tensor([[4, 3, 5],\n",
      " |          ...                     [6, 7, 8]])\n",
      " |          >>> src.put_(torch.tensor([1, 3]), torch.tensor([9, 10]))\n",
      " |          tensor([[  4,   9,   5],\n",
      " |                  [ 10,   7,   8]])\n",
      " |  \n",
      " |  q_per_channel_axis(...)\n",
      " |      q_per_channel_axis() -> int\n",
      " |      \n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns the index of dimension on which per-channel quantization is applied.\n",
      " |  \n",
      " |  q_per_channel_scales(...)\n",
      " |      q_per_channel_scales() -> Tensor\n",
      " |      \n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns a Tensor of scales of the underlying quantizer. It has the number of\n",
      " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
      " |      the tensor.\n",
      " |  \n",
      " |  q_per_channel_zero_points(...)\n",
      " |      q_per_channel_zero_points() -> Tensor\n",
      " |      \n",
      " |      Given a Tensor quantized by linear (affine) per-channel quantization,\n",
      " |      returns a tensor of zero_points of the underlying quantizer. It has the number of\n",
      " |      elements that matches the corresponding dimensions (from q_per_channel_axis) of\n",
      " |      the tensor.\n",
      " |  \n",
      " |  q_scale(...)\n",
      " |      q_scale() -> float\n",
      " |      \n",
      " |      Given a Tensor quantized by linear(affine) quantization,\n",
      " |      returns the scale of the underlying quantizer().\n",
      " |  \n",
      " |  q_zero_point(...)\n",
      " |      q_zero_point() -> int\n",
      " |      \n",
      " |      Given a Tensor quantized by linear(affine) quantization,\n",
      " |      returns the zero_point of the underlying quantizer().\n",
      " |  \n",
      " |  qr(...)\n",
      " |      qr(some=True) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.qr`\n",
      " |  \n",
      " |  qscheme(...)\n",
      " |      qscheme() -> torch.qscheme\n",
      " |      \n",
      " |      Returns the quantization scheme of a given QTensor.\n",
      " |  \n",
      " |  quantile(...)\n",
      " |      quantile(q, dim=None, keepdim=False, *, interpolation='linear') -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.quantile`\n",
      " |  \n",
      " |  rad2deg(...)\n",
      " |      rad2deg() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.rad2deg`\n",
      " |  \n",
      " |  rad2deg_(...)\n",
      " |      rad2deg_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.rad2deg`\n",
      " |  \n",
      " |  random_(...)\n",
      " |      random_(from=0, to=None, *, generator=None) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with numbers sampled from the discrete uniform\n",
      " |      distribution over ``[from, to - 1]``. If not specified, the values are usually\n",
      " |      only bounded by :attr:`self` tensor's data type. However, for floating point\n",
      " |      types, if unspecified, range will be ``[0, 2^mantissa]`` to ensure that every\n",
      " |      value is representable. For example, `torch.tensor(1, dtype=torch.double).random_()`\n",
      " |      will be uniform in ``[0, 2^53]``.\n",
      " |  \n",
      " |  ravel(...)\n",
      " |      ravel() -> Tensor\n",
      " |      \n",
      " |      see :func:`torch.ravel`\n",
      " |  \n",
      " |  reciprocal(...)\n",
      " |      reciprocal() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.reciprocal`\n",
      " |  \n",
      " |  reciprocal_(...)\n",
      " |      reciprocal_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.reciprocal`\n",
      " |  \n",
      " |  record_stream(...)\n",
      " |      record_stream(stream)\n",
      " |      \n",
      " |      Ensures that the tensor memory is not reused for another tensor until all\n",
      " |      current work queued on :attr:`stream` are complete.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The caching allocator is aware of only the stream where a tensor was\n",
      " |          allocated. Due to the awareness, it already correctly manages the life\n",
      " |          cycle of tensors on only one stream. But if a tensor is used on a stream\n",
      " |          different from the stream of origin, the allocator might reuse the memory\n",
      " |          unexpectedly. Calling this method lets the allocator know which streams\n",
      " |          have used the tensor.\n",
      " |  \n",
      " |  relu(...)\n",
      " |  \n",
      " |  relu_(...)\n",
      " |  \n",
      " |  remainder(...)\n",
      " |      remainder(divisor) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.remainder`\n",
      " |  \n",
      " |  remainder_(...)\n",
      " |      remainder_(divisor) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.remainder`\n",
      " |  \n",
      " |  renorm(...)\n",
      " |      renorm(p, dim, maxnorm) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.renorm`\n",
      " |  \n",
      " |  renorm_(...)\n",
      " |      renorm_(p, dim, maxnorm) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.renorm`\n",
      " |  \n",
      " |  repeat(...)\n",
      " |      repeat(*sizes) -> Tensor\n",
      " |      \n",
      " |      Repeats this tensor along the specified dimensions.\n",
      " |      \n",
      " |      Unlike :meth:`~Tensor.expand`, this function copies the tensor's data.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          :meth:`~Tensor.repeat` behaves differently from\n",
      " |          `numpy.repeat <https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html>`_,\n",
      " |          but is more similar to\n",
      " |          `numpy.tile <https://docs.scipy.org/doc/numpy/reference/generated/numpy.tile.html>`_.\n",
      " |          For the operator similar to `numpy.repeat`, see :func:`torch.repeat_interleave`.\n",
      " |      \n",
      " |      Args:\n",
      " |          sizes (torch.Size or int...): The number of times to repeat this tensor along each\n",
      " |              dimension\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([1, 2, 3])\n",
      " |          >>> x.repeat(4, 2)\n",
      " |          tensor([[ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3],\n",
      " |                  [ 1,  2,  3,  1,  2,  3]])\n",
      " |          >>> x.repeat(4, 2, 1).size()\n",
      " |          torch.Size([4, 2, 3])\n",
      " |  \n",
      " |  repeat_interleave(...)\n",
      " |      repeat_interleave(repeats, dim=None, *, output_size=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.repeat_interleave`.\n",
      " |  \n",
      " |  requires_grad_(...)\n",
      " |      requires_grad_(requires_grad=True) -> Tensor\n",
      " |      \n",
      " |      Change if autograd should record operations on this tensor: sets this tensor's\n",
      " |      :attr:`requires_grad` attribute in-place. Returns this tensor.\n",
      " |      \n",
      " |      :func:`requires_grad_`'s main use case is to tell autograd to begin recording\n",
      " |      operations on a Tensor ``tensor``. If ``tensor`` has ``requires_grad=False``\n",
      " |      (because it was obtained through a DataLoader, or required preprocessing or\n",
      " |      initialization), ``tensor.requires_grad_()`` makes it so that autograd will\n",
      " |      begin to record operations on ``tensor``.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): If autograd should record operations on this tensor.\n",
      " |              Default: ``True``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # Let's say we want to preprocess some saved weights and use\n",
      " |          >>> # the result as new weights.\n",
      " |          >>> saved_weights = [0.1, 0.2, 0.3, 0.25]\n",
      " |          >>> loaded_weights = torch.tensor(saved_weights)\n",
      " |          >>> weights = preprocess(loaded_weights)  # some function\n",
      " |          >>> weights\n",
      " |          tensor([-0.5503,  0.4926, -2.1158, -0.8303])\n",
      " |      \n",
      " |          >>> # Now, start to record operations done to weights\n",
      " |          >>> weights.requires_grad_()\n",
      " |          >>> out = weights.pow(2).sum()\n",
      " |          >>> out.backward()\n",
      " |          >>> weights.grad\n",
      " |          tensor([-1.1007,  0.9853, -4.2316, -1.6606])\n",
      " |  \n",
      " |  reshape(...)\n",
      " |      reshape(*shape) -> Tensor\n",
      " |      \n",
      " |      Returns a tensor with the same data and number of elements as :attr:`self`\n",
      " |      but with the specified shape. This method returns a view if :attr:`shape` is\n",
      " |      compatible with the current shape. See :meth:`torch.Tensor.view` on when it is\n",
      " |      possible to return a view.\n",
      " |      \n",
      " |      See :func:`torch.reshape`\n",
      " |      \n",
      " |      Args:\n",
      " |          shape (tuple of ints or int...): the desired shape\n",
      " |  \n",
      " |  reshape_as(...)\n",
      " |      reshape_as(other) -> Tensor\n",
      " |      \n",
      " |      Returns this tensor as the same shape as :attr:`other`.\n",
      " |      ``self.reshape_as(other)`` is equivalent to ``self.reshape(other.sizes())``.\n",
      " |      This method returns a view if ``other.sizes()`` is compatible with the current\n",
      " |      shape. See :meth:`torch.Tensor.view` on when it is possible to return a view.\n",
      " |      \n",
      " |      Please see :meth:`reshape` for more information about ``reshape``.\n",
      " |      \n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same shape\n",
      " |              as :attr:`other`.\n",
      " |  \n",
      " |  resize_(...)\n",
      " |      resize_(*sizes, memory_format=torch.contiguous_format) -> Tensor\n",
      " |      \n",
      " |      Resizes :attr:`self` tensor to the specified size. If the number of elements is\n",
      " |      larger than the current storage size, then the underlying storage is resized\n",
      " |      to fit the new number of elements. If the number of elements is smaller, the\n",
      " |      underlying storage is not changed. Existing elements are preserved but any new\n",
      " |      memory is uninitialized.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          This is a low-level method. The storage is reinterpreted as C-contiguous,\n",
      " |          ignoring the current strides (unless the target size equals the current\n",
      " |          size, in which case the tensor is left unchanged). For most purposes, you\n",
      " |          will instead want to use :meth:`~Tensor.view()`, which checks for\n",
      " |          contiguity, or :meth:`~Tensor.reshape()`, which copies data if needed. To\n",
      " |          change the size in-place with custom strides, see :meth:`~Tensor.set_()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          sizes (torch.Size or int...): the desired size\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
      " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``sizes``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
      " |          >>> x.resize_(2, 2)\n",
      " |          tensor([[ 1,  2],\n",
      " |                  [ 3,  4]])\n",
      " |  \n",
      " |  resize_as_(...)\n",
      " |      resize_as_(tensor, memory_format=torch.contiguous_format) -> Tensor\n",
      " |      \n",
      " |      Resizes the :attr:`self` tensor to be the same size as the specified\n",
      " |      :attr:`tensor`. This is equivalent to ``self.resize_(tensor.size())``.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              Tensor. Default: ``torch.contiguous_format``. Note that memory format of\n",
      " |              :attr:`self` is going to be unaffected if ``self.size()`` matches ``tensor.size()``.\n",
      " |  \n",
      " |  resize_as_sparse_(...)\n",
      " |  \n",
      " |  resolve_conj(...)\n",
      " |      resolve_conj() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.resolve_conj`\n",
      " |  \n",
      " |  resolve_neg(...)\n",
      " |      resolve_neg() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.resolve_neg`\n",
      " |  \n",
      " |  retain_grad(...)\n",
      " |      retain_grad() -> None\n",
      " |      \n",
      " |      Enables this Tensor to have their :attr:`grad` populated during\n",
      " |      :func:`backward`. This is a no-op for leaf tensors.\n",
      " |  \n",
      " |  roll(...)\n",
      " |      roll(shifts, dims) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.roll`\n",
      " |  \n",
      " |  rot90(...)\n",
      " |      rot90(k, dims) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.rot90`\n",
      " |  \n",
      " |  round(...)\n",
      " |      round(decimals=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.round`\n",
      " |  \n",
      " |  round_(...)\n",
      " |      round_(decimals=0) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.round`\n",
      " |  \n",
      " |  row_indices(...)\n",
      " |  \n",
      " |  rsqrt(...)\n",
      " |      rsqrt() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.rsqrt`\n",
      " |  \n",
      " |  rsqrt_(...)\n",
      " |      rsqrt_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.rsqrt`\n",
      " |  \n",
      " |  scatter(...)\n",
      " |      scatter(dim, index, src) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_`\n",
      " |  \n",
      " |  scatter_(...)\n",
      " |      scatter_(dim, index, src, reduce=None) -> Tensor\n",
      " |      \n",
      " |      Writes all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor. For each value in :attr:`src`, its output\n",
      " |      index is specified by its index in :attr:`src` for ``dimension != dim`` and by\n",
      " |      the corresponding value in :attr:`index` for ``dimension = dim``.\n",
      " |      \n",
      " |      For a 3-D tensor, :attr:`self` is updated as::\n",
      " |      \n",
      " |          self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2\n",
      " |      \n",
      " |      This is the reverse operation of the manner described in :meth:`~Tensor.gather`.\n",
      " |      \n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` (if it is a Tensor) should all have\n",
      " |      the same number of dimensions. It is also required that\n",
      " |      ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
      " |      ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
      " |      Note that ``index`` and ``src`` do not broadcast.\n",
      " |      \n",
      " |      Moreover, as for :meth:`~Tensor.gather`, the values of :attr:`index` must be\n",
      " |      between ``0`` and ``self.size(dim) - 1`` inclusive.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          When indices are not unique, the behavior is non-deterministic (one of the\n",
      " |          values from ``src`` will be picked arbitrarily) and the gradient will be\n",
      " |          incorrect (it will be propagated to all locations in the source that\n",
      " |          correspond to the same index)!\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |      \n",
      " |      Additionally accepts an optional :attr:`reduce` argument that allows\n",
      " |      specification of an optional reduction operation, which is applied to all\n",
      " |      values in the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index`. For each value in :attr:`src`, the reduction\n",
      " |      operation is applied to an index in :attr:`self` which is specified by\n",
      " |      its index in :attr:`src` for ``dimension != dim`` and by the corresponding\n",
      " |      value in :attr:`index` for ``dimension = dim``.\n",
      " |      \n",
      " |      Given a 3-D tensor and reduction using the multiplication operation, :attr:`self`\n",
      " |      is updated as::\n",
      " |      \n",
      " |          self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2\n",
      " |      \n",
      " |      Reducing with the addition operation is the same as using\n",
      " |      :meth:`~torch.Tensor.scatter_add_`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          The reduce argument with Tensor ``src`` is deprecated and will be removed in\n",
      " |          a future PyTorch release. Please use :meth:`~torch.Tensor.scatter_reduce_`\n",
      " |          instead for more reduction options.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter, can be either empty\n",
      " |              or of the same dimensionality as ``src``. When empty, the operation\n",
      " |              returns ``self`` unchanged.\n",
      " |          src (Tensor or float): the source element(s) to scatter.\n",
      " |          reduce (str, optional): reduction operation to apply, can be either\n",
      " |              ``'add'`` or ``'multiply'``.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> src = torch.arange(1, 11).reshape((2, 5))\n",
      " |          >>> src\n",
      " |          tensor([[ 1,  2,  3,  4,  5],\n",
      " |                  [ 6,  7,  8,  9, 10]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)\n",
      " |          tensor([[1, 0, 0, 4, 0],\n",
      " |                  [0, 2, 0, 0, 0],\n",
      " |                  [0, 0, 3, 0, 0]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2], [0, 1, 4]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)\n",
      " |          tensor([[1, 2, 3, 0, 0],\n",
      " |                  [6, 7, 0, 0, 8],\n",
      " |                  [0, 0, 0, 0, 0]])\n",
      " |      \n",
      " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      " |          ...            1.23, reduce='multiply')\n",
      " |          tensor([[2.0000, 2.0000, 2.4600, 2.0000],\n",
      " |                  [2.0000, 2.0000, 2.0000, 2.4600]])\n",
      " |          >>> torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),\n",
      " |          ...            1.23, reduce='add')\n",
      " |          tensor([[2.0000, 2.0000, 3.2300, 2.0000],\n",
      " |                  [2.0000, 2.0000, 2.0000, 3.2300]])\n",
      " |  \n",
      " |  scatter_add(...)\n",
      " |      scatter_add(dim, index, src) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_add_`\n",
      " |  \n",
      " |  scatter_add_(...)\n",
      " |      scatter_add_(dim, index, src) -> Tensor\n",
      " |      \n",
      " |      Adds all values from the tensor :attr:`src` into :attr:`self` at the indices\n",
      " |      specified in the :attr:`index` tensor in a similar fashion as\n",
      " |      :meth:`~torch.Tensor.scatter_`. For each value in :attr:`src`, it is added to\n",
      " |      an index in :attr:`self` which is specified by its index in :attr:`src`\n",
      " |      for ``dimension != dim`` and by the corresponding value in :attr:`index` for\n",
      " |      ``dimension = dim``.\n",
      " |      \n",
      " |      For a 3-D tensor, :attr:`self` is updated as::\n",
      " |      \n",
      " |          self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
      " |      \n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` should have same number of\n",
      " |      dimensions. It is also required that ``index.size(d) <= src.size(d)`` for all\n",
      " |      dimensions ``d``, and that ``index.size(d) <= self.size(d)`` for all dimensions\n",
      " |      ``d != dim``. Note that ``index`` and ``src`` do not broadcast.\n",
      " |      \n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter and add, can be\n",
      " |              either empty or of the same dimensionality as ``src``. When empty, the\n",
      " |              operation returns ``self`` unchanged.\n",
      " |          src (Tensor): the source elements to scatter and add\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> src = torch.ones((2, 5))\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0, 0]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
      " |          tensor([[1., 0., 0., 1., 1.],\n",
      " |                  [0., 1., 0., 0., 0.],\n",
      " |                  [0., 0., 1., 0., 0.]])\n",
      " |          >>> index = torch.tensor([[0, 1, 2, 0, 0], [0, 1, 2, 2, 2]])\n",
      " |          >>> torch.zeros(3, 5, dtype=src.dtype).scatter_add_(0, index, src)\n",
      " |          tensor([[2., 0., 0., 1., 1.],\n",
      " |                  [0., 2., 0., 0., 0.],\n",
      " |                  [0., 0., 2., 1., 1.]])\n",
      " |  \n",
      " |  scatter_reduce(...)\n",
      " |      scatter_reduce(dim, index, src, reduce, *, include_self=True) -> Tensor\n",
      " |      \n",
      " |      Out-of-place version of :meth:`torch.Tensor.scatter_reduce_`\n",
      " |  \n",
      " |  scatter_reduce_(...)\n",
      " |      scatter_reduce_(dim, index, src, reduce, *, include_self=True) -> Tensor\n",
      " |      \n",
      " |      Reduces all values from the :attr:`src` tensor to the indices specified in\n",
      " |      the :attr:`index` tensor in the :attr:`self` tensor using the applied reduction\n",
      " |      defined via the :attr:`reduce` argument (:obj:`\"sum\"`, :obj:`\"prod\"`, :obj:`\"mean\"`,\n",
      " |      :obj:`\"amax\"`, :obj:`\"amin\"`). For each value in :attr:`src`, it is reduced to an\n",
      " |      index in :attr:`self` which is specified by its index in :attr:`src` for\n",
      " |      ``dimension != dim`` and by the corresponding value in :attr:`index` for\n",
      " |      ``dimension = dim``. If :obj:`include_self=\"True\"`, the values in the :attr:`self`\n",
      " |      tensor are included in the reduction.\n",
      " |      \n",
      " |      :attr:`self`, :attr:`index` and :attr:`src` should all have\n",
      " |      the same number of dimensions. It is also required that\n",
      " |      ``index.size(d) <= src.size(d)`` for all dimensions ``d``, and that\n",
      " |      ``index.size(d) <= self.size(d)`` for all dimensions ``d != dim``.\n",
      " |      Note that ``index`` and ``src`` do not broadcast.\n",
      " |      \n",
      " |      For a 3-D tensor with :obj:`reduce=\"sum\"` and :obj:`include_self=True` the\n",
      " |      output is given as::\n",
      " |      \n",
      " |          self[index[i][j][k]][j][k] += src[i][j][k]  # if dim == 0\n",
      " |          self[i][index[i][j][k]][k] += src[i][j][k]  # if dim == 1\n",
      " |          self[i][j][index[i][j][k]] += src[i][j][k]  # if dim == 2\n",
      " |      \n",
      " |      Note:\n",
      " |          This operation may behave nondeterministically when given tensors on a CUDA device. See :doc:`/notes/randomness` for more information.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The backward pass is implemented only for ``src.shape == index.shape``.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          This function is in beta and may change in the near future.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int): the axis along which to index\n",
      " |          index (LongTensor): the indices of elements to scatter and reduce.\n",
      " |          src (Tensor): the source elements to scatter and reduce\n",
      " |          reduce (str): the reduction operation to apply for non-unique indices\n",
      " |              (:obj:`\"sum\"`, :obj:`\"prod\"`, :obj:`\"mean\"`, :obj:`\"amax\"`, :obj:`\"amin\"`)\n",
      " |          include_self (bool): whether elements from the :attr:`self` tensor are\n",
      " |              included in the reduction\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> src = torch.tensor([1., 2., 3., 4., 5., 6.])\n",
      " |          >>> index = torch.tensor([0, 1, 0, 1, 2, 1])\n",
      " |          >>> input = torch.tensor([1., 2., 3., 4.])\n",
      " |          >>> input.scatter_reduce(0, index, src, reduce=\"sum\")\n",
      " |          tensor([5., 14., 8., 4.])\n",
      " |          >>> input.scatter_reduce(0, index, src, reduce=\"sum\", include_self=False)\n",
      " |          tensor([4., 12., 5., 4.])\n",
      " |          >>> input2 = torch.tensor([5., 4., 3., 2.])\n",
      " |          >>> input2.scatter_reduce(0, index, src, reduce=\"amax\")\n",
      " |          tensor([5., 6., 5., 2.])\n",
      " |          >>> input2.scatter_reduce(0, index, src, reduce=\"amax\", include_self=False)\n",
      " |          tensor([3., 6., 5., 2.])\n",
      " |  \n",
      " |  select(...)\n",
      " |      select(dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.select`\n",
      " |  \n",
      " |  select_scatter(...)\n",
      " |      select_scatter(src, dim, index) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.select_scatter`\n",
      " |  \n",
      " |  set_(...)\n",
      " |      set_(source=None, storage_offset=0, size=None, stride=None) -> Tensor\n",
      " |      \n",
      " |      Sets the underlying storage, size, and strides. If :attr:`source` is a tensor,\n",
      " |      :attr:`self` tensor will share the same storage and have the same size and\n",
      " |      strides as :attr:`source`. Changes to elements in one tensor will be reflected\n",
      " |      in the other.\n",
      " |      \n",
      " |      If :attr:`source` is a :class:`~torch.Storage`, the method sets the underlying\n",
      " |      storage, offset, size, and stride.\n",
      " |      \n",
      " |      Args:\n",
      " |          source (Tensor or Storage): the tensor or storage to use\n",
      " |          storage_offset (int, optional): the offset in the storage\n",
      " |          size (torch.Size, optional): the desired size. Defaults to the size of the source.\n",
      " |          stride (tuple, optional): the desired stride. Defaults to C-contiguous strides.\n",
      " |  \n",
      " |  sgn(...)\n",
      " |      sgn() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sgn`\n",
      " |  \n",
      " |  sgn_(...)\n",
      " |      sgn_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sgn`\n",
      " |  \n",
      " |  short(...)\n",
      " |      short(memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      ``self.short()`` is equivalent to ``self.to(torch.int16)``. See :func:`to`.\n",
      " |      \n",
      " |      Args:\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  sigmoid(...)\n",
      " |      sigmoid() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sigmoid`\n",
      " |  \n",
      " |  sigmoid_(...)\n",
      " |      sigmoid_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sigmoid`\n",
      " |  \n",
      " |  sign(...)\n",
      " |      sign() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sign`\n",
      " |  \n",
      " |  sign_(...)\n",
      " |      sign_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sign`\n",
      " |  \n",
      " |  signbit(...)\n",
      " |      signbit() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.signbit`\n",
      " |  \n",
      " |  sin(...)\n",
      " |      sin() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sin`\n",
      " |  \n",
      " |  sin_(...)\n",
      " |      sin_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sin`\n",
      " |  \n",
      " |  sinc(...)\n",
      " |      sinc() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sinc`\n",
      " |  \n",
      " |  sinc_(...)\n",
      " |      sinc_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sinc`\n",
      " |  \n",
      " |  sinh(...)\n",
      " |      sinh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sinh`\n",
      " |  \n",
      " |  sinh_(...)\n",
      " |      sinh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sinh`\n",
      " |  \n",
      " |  size(...)\n",
      " |      size(dim=None) -> torch.Size or int\n",
      " |      \n",
      " |      Returns the size of the :attr:`self` tensor. If ``dim`` is not specified,\n",
      " |      the returned value is a :class:`torch.Size`, a subclass of :class:`tuple`.\n",
      " |      If ``dim`` is specified, returns an int holding the size of that dimension.\n",
      " |      \n",
      " |      Args:\n",
      " |        dim (int, optional): The dimension for which to retrieve the size.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> t = torch.empty(3, 4, 5)\n",
      " |          >>> t.size()\n",
      " |          torch.Size([3, 4, 5])\n",
      " |          >>> t.size(dim=1)\n",
      " |          4\n",
      " |  \n",
      " |  slice_scatter(...)\n",
      " |      slice_scatter(src, dim=0, start=None, end=None, step=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.slice_scatter`\n",
      " |  \n",
      " |  slogdet(...)\n",
      " |      slogdet() -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.slogdet`\n",
      " |  \n",
      " |  smm(...)\n",
      " |      smm(mat) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.smm`\n",
      " |  \n",
      " |  softmax(...)\n",
      " |      softmax(dim) -> Tensor\n",
      " |      \n",
      " |      Alias for :func:`torch.nn.functional.softmax`.\n",
      " |  \n",
      " |  sort(...)\n",
      " |      sort(dim=-1, descending=False) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.sort`\n",
      " |  \n",
      " |  sparse_dim(...)\n",
      " |      sparse_dim() -> int\n",
      " |      \n",
      " |      Return the number of sparse dimensions in a :ref:`sparse tensor <sparse-docs>` :attr:`self`.\n",
      " |      \n",
      " |      .. note::\n",
      " |        Returns ``0`` if :attr:`self` is not a sparse tensor.\n",
      " |      \n",
      " |      See also :meth:`Tensor.dense_dim` and :ref:`hybrid tensors <sparse-hybrid-coo-docs>`.\n",
      " |  \n",
      " |  sparse_mask(...)\n",
      " |      sparse_mask(mask) -> Tensor\n",
      " |      \n",
      " |      Returns a new :ref:`sparse tensor <sparse-docs>` with values from a\n",
      " |      strided tensor :attr:`self` filtered by the indices of the sparse\n",
      " |      tensor :attr:`mask`. The values of :attr:`mask` sparse tensor are\n",
      " |      ignored. :attr:`self` and :attr:`mask` tensors must have the same\n",
      " |      shape.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |        The returned sparse tensor might contain duplicate values if :attr:`mask`\n",
      " |        is not coalesced. It is therefore advisable to pass ``mask.coalesce()``\n",
      " |        if such behavior is not desired.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |        The returned sparse tensor has the same indices as the sparse tensor\n",
      " |        :attr:`mask`, even when the corresponding values in :attr:`self` are\n",
      " |        zeros.\n",
      " |      \n",
      " |      Args:\n",
      " |          mask (Tensor): a sparse tensor whose indices are used as a filter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> nse = 5\n",
      " |          >>> dims = (5, 5, 2, 2)\n",
      " |          >>> I = torch.cat([torch.randint(0, dims[0], size=(nse,)),\n",
      " |          ...                torch.randint(0, dims[1], size=(nse,))], 0).reshape(2, nse)\n",
      " |          >>> V = torch.randn(nse, dims[2], dims[3])\n",
      " |          >>> S = torch.sparse_coo_tensor(I, V, dims).coalesce()\n",
      " |          >>> D = torch.randn(dims)\n",
      " |          >>> D.sparse_mask(S)\n",
      " |          tensor(indices=tensor([[0, 0, 0, 2],\n",
      " |                                 [0, 1, 4, 3]]),\n",
      " |                 values=tensor([[[ 1.6550,  0.2397],\n",
      " |                                 [-0.1611, -0.0779]],\n",
      " |      \n",
      " |                                [[ 0.2326, -1.0558],\n",
      " |                                 [ 1.4711,  1.9678]],\n",
      " |      \n",
      " |                                [[-0.5138, -0.0411],\n",
      " |                                 [ 1.9417,  0.5158]],\n",
      " |      \n",
      " |                                [[ 0.0793,  0.0036],\n",
      " |                                 [-0.2569, -0.1055]]]),\n",
      " |                 size=(5, 5, 2, 2), nnz=4, layout=torch.sparse_coo)\n",
      " |  \n",
      " |  sparse_resize_(...)\n",
      " |      sparse_resize_(size, sparse_dim, dense_dim) -> Tensor\n",
      " |      \n",
      " |      Resizes :attr:`self` :ref:`sparse tensor <sparse-docs>` to the desired\n",
      " |      size and the number of sparse and dense dimensions.\n",
      " |      \n",
      " |      .. note::\n",
      " |        If the number of specified elements in :attr:`self` is zero, then\n",
      " |        :attr:`size`, :attr:`sparse_dim`, and :attr:`dense_dim` can be any\n",
      " |        size and positive integers such that ``len(size) == sparse_dim +\n",
      " |        dense_dim``.\n",
      " |      \n",
      " |        If :attr:`self` specifies one or more elements, however, then each\n",
      " |        dimension in :attr:`size` must not be smaller than the corresponding\n",
      " |        dimension of :attr:`self`, :attr:`sparse_dim` must equal the number\n",
      " |        of sparse dimensions in :attr:`self`, and :attr:`dense_dim` must\n",
      " |        equal the number of dense dimensions in :attr:`self`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (torch.Size): the desired size. If :attr:`self` is non-empty\n",
      " |            sparse tensor, the desired size cannot be smaller than the\n",
      " |            original size.\n",
      " |          sparse_dim (int): the number of sparse dimensions\n",
      " |          dense_dim (int): the number of dense dimensions\n",
      " |  \n",
      " |  sparse_resize_and_clear_(...)\n",
      " |      sparse_resize_and_clear_(size, sparse_dim, dense_dim) -> Tensor\n",
      " |      \n",
      " |      Removes all specified elements from a :ref:`sparse tensor\n",
      " |      <sparse-docs>` :attr:`self` and resizes :attr:`self` to the desired\n",
      " |      size and the number of sparse and dense dimensions.\n",
      " |      \n",
      " |      .. warning:\n",
      " |        Throws an error if :attr:`self` is not a sparse tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (torch.Size): the desired size.\n",
      " |          sparse_dim (int): the number of sparse dimensions\n",
      " |          dense_dim (int): the number of dense dimensions\n",
      " |  \n",
      " |  split_with_sizes(...)\n",
      " |  \n",
      " |  sqrt(...)\n",
      " |      sqrt() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sqrt`\n",
      " |  \n",
      " |  sqrt_(...)\n",
      " |      sqrt_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sqrt`\n",
      " |  \n",
      " |  square(...)\n",
      " |      square() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.square`\n",
      " |  \n",
      " |  square_(...)\n",
      " |      square_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.square`\n",
      " |  \n",
      " |  squeeze(...)\n",
      " |      squeeze(dim=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.squeeze`\n",
      " |  \n",
      " |  squeeze_(...)\n",
      " |      squeeze_(dim=None) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.squeeze`\n",
      " |  \n",
      " |  sspaddmm(...)\n",
      " |      sspaddmm(mat1, mat2, *, beta=1, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sspaddmm`\n",
      " |  \n",
      " |  std(...)\n",
      " |      std(dim=None, *, correction=1, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.std`\n",
      " |  \n",
      " |  storage_offset(...)\n",
      " |      storage_offset() -> int\n",
      " |      \n",
      " |      Returns :attr:`self` tensor's offset in the underlying storage in terms of\n",
      " |      number of storage elements (not bytes).\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([1, 2, 3, 4, 5])\n",
      " |          >>> x.storage_offset()\n",
      " |          0\n",
      " |          >>> x[3:].storage_offset()\n",
      " |          3\n",
      " |  \n",
      " |  stride(...)\n",
      " |      stride(dim) -> tuple or int\n",
      " |      \n",
      " |      Returns the stride of :attr:`self` tensor.\n",
      " |      \n",
      " |      Stride is the jump necessary to go from one element to the next one in the\n",
      " |      specified dimension :attr:`dim`. A tuple of all strides is returned when no\n",
      " |      argument is passed in. Otherwise, an integer value is returned as the stride in\n",
      " |      the particular dimension :attr:`dim`.\n",
      " |      \n",
      " |      Args:\n",
      " |          dim (int, optional): the desired dimension in which stride is required\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]])\n",
      " |          >>> x.stride()\n",
      " |          (5, 1)\n",
      " |          >>> x.stride(0)\n",
      " |          5\n",
      " |          >>> x.stride(-1)\n",
      " |          1\n",
      " |  \n",
      " |  sub(...)\n",
      " |      sub(other, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sub`.\n",
      " |  \n",
      " |  sub_(...)\n",
      " |      sub_(other, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.sub`\n",
      " |  \n",
      " |  subtract(...)\n",
      " |      subtract(other, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.subtract`.\n",
      " |  \n",
      " |  subtract_(...)\n",
      " |      subtract_(other, *, alpha=1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.subtract`.\n",
      " |  \n",
      " |  sum(...)\n",
      " |      sum(dim=None, keepdim=False, dtype=None) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.sum`\n",
      " |  \n",
      " |  sum_to_size(...)\n",
      " |      sum_to_size(*size) -> Tensor\n",
      " |      \n",
      " |      Sum ``this`` tensor to :attr:`size`.\n",
      " |      :attr:`size` must be broadcastable to ``this`` tensor size.\n",
      " |      \n",
      " |      Args:\n",
      " |          size (int...): a sequence of integers defining the shape of the output tensor.\n",
      " |  \n",
      " |  svd(...)\n",
      " |      svd(some=True, compute_uv=True) -> (Tensor, Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.svd`\n",
      " |  \n",
      " |  swapaxes(...)\n",
      " |      swapaxes(axis0, axis1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.swapaxes`\n",
      " |  \n",
      " |  swapaxes_(...)\n",
      " |      swapaxes_(axis0, axis1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.swapaxes`\n",
      " |  \n",
      " |  swapdims(...)\n",
      " |      swapdims(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.swapdims`\n",
      " |  \n",
      " |  swapdims_(...)\n",
      " |      swapdims_(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.swapdims`\n",
      " |  \n",
      " |  t(...)\n",
      " |      t() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.t`\n",
      " |  \n",
      " |  t_(...)\n",
      " |      t_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.t`\n",
      " |  \n",
      " |  take(...)\n",
      " |      take(indices) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.take`\n",
      " |  \n",
      " |  take_along_dim(...)\n",
      " |      take_along_dim(indices, dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.take_along_dim`\n",
      " |  \n",
      " |  tan(...)\n",
      " |      tan() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.tan`\n",
      " |  \n",
      " |  tan_(...)\n",
      " |      tan_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.tan`\n",
      " |  \n",
      " |  tanh(...)\n",
      " |      tanh() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.tanh`\n",
      " |  \n",
      " |  tanh_(...)\n",
      " |      tanh_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.tanh`\n",
      " |  \n",
      " |  tensor_split(...)\n",
      " |      tensor_split(indices_or_sections, dim=0) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.tensor_split`\n",
      " |  \n",
      " |  tile(...)\n",
      " |      tile(*reps) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.tile`\n",
      " |  \n",
      " |  to(...)\n",
      " |      to(*args, **kwargs) -> Tensor\n",
      " |      \n",
      " |      Performs Tensor dtype and/or device conversion. A :class:`torch.dtype` and :class:`torch.device` are\n",
      " |      inferred from the arguments of ``self.to(*args, **kwargs)``.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          If the ``self`` Tensor already\n",
      " |          has the correct :class:`torch.dtype` and :class:`torch.device`, then ``self`` is returned.\n",
      " |          Otherwise, the returned tensor is a copy of ``self`` with the desired\n",
      " |          :class:`torch.dtype` and :class:`torch.device`.\n",
      " |      \n",
      " |      Here are the ways to call ``to``:\n",
      " |      \n",
      " |      .. method:: to(dtype, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |         :noindex:\n",
      " |      \n",
      " |          Returns a Tensor with the specified :attr:`dtype`\n",
      " |      \n",
      " |          Args:\n",
      " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |      \n",
      " |      .. method:: to(device=None, dtype=None, non_blocking=False, copy=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |         :noindex:\n",
      " |      \n",
      " |          Returns a Tensor with the specified :attr:`device` and (optional)\n",
      " |          :attr:`dtype`. If :attr:`dtype` is ``None`` it is inferred to be ``self.dtype``.\n",
      " |          When :attr:`non_blocking`, tries to convert asynchronously with respect to\n",
      " |          the host if possible, e.g., converting a CPU Tensor with pinned memory to a\n",
      " |          CUDA Tensor.\n",
      " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
      " |          already matches the desired conversion.\n",
      " |      \n",
      " |          Args:\n",
      " |              memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |      \n",
      " |      .. method:: to(other, non_blocking=False, copy=False) -> Tensor\n",
      " |         :noindex:\n",
      " |      \n",
      " |          Returns a Tensor with same :class:`torch.dtype` and :class:`torch.device` as\n",
      " |          the Tensor :attr:`other`. When :attr:`non_blocking`, tries to convert\n",
      " |          asynchronously with respect to the host if possible, e.g., converting a CPU\n",
      " |          Tensor with pinned memory to a CUDA Tensor.\n",
      " |          When :attr:`copy` is set, a new Tensor is created even when the Tensor\n",
      " |          already matches the desired conversion.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> tensor = torch.randn(2, 2)  # Initially dtype=float32, device=cpu\n",
      " |          >>> tensor.to(torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64)\n",
      " |      \n",
      " |          >>> cuda0 = torch.device('cuda:0')\n",
      " |          >>> tensor.to(cuda0)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], device='cuda:0')\n",
      " |      \n",
      " |          >>> tensor.to(cuda0, dtype=torch.float64)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |      \n",
      " |          >>> other = torch.randn((), dtype=torch.float64, device=cuda0)\n",
      " |          >>> tensor.to(other, non_blocking=True)\n",
      " |          tensor([[-0.5044,  0.0005],\n",
      " |                  [ 0.3310, -0.0584]], dtype=torch.float64, device='cuda:0')\n",
      " |  \n",
      " |  to_dense(...)\n",
      " |      to_dense() -> Tensor\n",
      " |      \n",
      " |      Creates a strided copy of :attr:`self` if :attr:`self` is not a strided tensor, otherwise returns :attr:`self`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> s = torch.sparse_coo_tensor(\n",
      " |          ...        torch.tensor([[1, 1],\n",
      " |          ...                      [0, 2]]),\n",
      " |          ...        torch.tensor([9, 10]),\n",
      " |          ...        size=(3, 3))\n",
      " |          >>> s.to_dense()\n",
      " |          tensor([[ 0,  0,  0],\n",
      " |                  [ 9,  0, 10],\n",
      " |                  [ 0,  0,  0]])\n",
      " |  \n",
      " |  to_mkldnn(...)\n",
      " |      to_mkldnn() -> Tensor\n",
      " |      Returns a copy of the tensor in ``torch.mkldnn`` layout.\n",
      " |  \n",
      " |  to_padded_tensor(...)\n",
      " |      to_padded_tensor(padding, output_size=None) -> Tensor\n",
      " |      See :func:`to_padded_tensor`\n",
      " |  \n",
      " |  to_sparse(...)\n",
      " |      to_sparse(sparseDims) -> Tensor\n",
      " |      \n",
      " |      Returns a sparse copy of the tensor.  PyTorch supports sparse tensors in\n",
      " |      :ref:`coordinate format <sparse-coo-docs>`.\n",
      " |      \n",
      " |      Args:\n",
      " |          sparseDims (int, optional): the number of sparse dimensions to include in the new sparse tensor\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> d = torch.tensor([[0, 0, 0], [9, 0, 10], [0, 0, 0]])\n",
      " |          >>> d\n",
      " |          tensor([[ 0,  0,  0],\n",
      " |                  [ 9,  0, 10],\n",
      " |                  [ 0,  0,  0]])\n",
      " |          >>> d.to_sparse()\n",
      " |          tensor(indices=tensor([[1, 1],\n",
      " |                                 [0, 2]]),\n",
      " |                 values=tensor([ 9, 10]),\n",
      " |                 size=(3, 3), nnz=2, layout=torch.sparse_coo)\n",
      " |          >>> d.to_sparse(1)\n",
      " |          tensor(indices=tensor([[1]]),\n",
      " |                 values=tensor([[ 9,  0, 10]]),\n",
      " |                 size=(3, 3), nnz=1, layout=torch.sparse_coo)\n",
      " |      \n",
      " |      .. method:: to_sparse(*, layout=None, blocksize=None, dense_dim=None) -> Tensor\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Returns a sparse tensor with the specified layout and blocksize.  If\n",
      " |      the :attr:`self` is strided, the number of dense dimensions could be\n",
      " |      specified, and a hybrid sparse tensor will be created, with\n",
      " |      `dense_dim` dense dimensions and `self.dim() - 2 - dense_dim` batch\n",
      " |      dimension.\n",
      " |      \n",
      " |      .. note:: If the :attr:`self` layout and blocksize parameters match\n",
      " |                with the specified layout and blocksize, return\n",
      " |                :attr:`self`. Otherwise, return a sparse tensor copy of\n",
      " |                :attr:`self`.\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          layout (:class:`torch.layout`, optional): The desired sparse\n",
      " |            layout. One of ``torch.sparse_coo``, ``torch.sparse_csr``,\n",
      " |            ``torch.sparse_csc``, ``torch.sparse_bsr``, or\n",
      " |            ``torch.sparse_bsc``. Default: if ``None``,\n",
      " |            ``torch.sparse_coo``.\n",
      " |      \n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSR or BSC tensor. For other layouts,\n",
      " |            specifying the block size that is not ``None`` will result in a\n",
      " |            RuntimeError exception.  A block size must be a tuple of length\n",
      " |            two such that its items evenly divide the two sparse dimensions.\n",
      " |      \n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSR, CSC, BSR or BSC tensor.  This argument should be\n",
      " |            used only if :attr:`self` is a strided tensor, and must be a\n",
      " |            value between 0 and dimension of :attr:`self` tensor minus two.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.tensor([[1, 0], [0, 0], [2, 3]])\n",
      " |          >>> x.to_sparse(layout=torch.sparse_coo)\n",
      " |          tensor(indices=tensor([[0, 2, 2],\n",
      " |                                 [0, 0, 1]]),\n",
      " |                 values=tensor([1, 2, 3]),\n",
      " |                 size=(3, 2), nnz=3, layout=torch.sparse_coo)\n",
      " |          >>> x.to_sparse(layout=torch.sparse_bsr, blocksize=(1, 2))\n",
      " |          tensor(crow_indices=tensor([0, 1, 1, 2]),\n",
      " |                 col_indices=tensor([0, 0]),\n",
      " |                 values=tensor([[[1, 0]],\n",
      " |                                [[2, 3]]]), size=(3, 2), nnz=2, layout=torch.sparse_bsr)\n",
      " |          >>> x.to_sparse(layout=torch.sparse_bsr, blocksize=(2, 1))\n",
      " |          RuntimeError: Tensor size(-2) 3 needs to be divisible by blocksize[0] 2\n",
      " |          >>> x.to_sparse(layout=torch.sparse_csr, blocksize=(3, 1))\n",
      " |          RuntimeError: to_sparse for Strided to SparseCsr conversion does not use specified blocksize\n",
      " |      \n",
      " |          >>> x = torch.tensor([[[1], [0]], [[0], [0]], [[2], [3]]])\n",
      " |          >>> x.to_sparse(layout=torch.sparse_csr, dense_dim=1)\n",
      " |          tensor(crow_indices=tensor([0, 1, 1, 3]),\n",
      " |                 col_indices=tensor([0, 0, 1]),\n",
      " |                 values=tensor([[1],\n",
      " |                                [2],\n",
      " |                                [3]]), size=(3, 2, 1), nnz=3, layout=torch.sparse_csr)\n",
      " |  \n",
      " |  to_sparse_bsc(...)\n",
      " |      to_sparse_bsc(blocksize, dense_dim) -> Tensor\n",
      " |      \n",
      " |      Convert a tensor to a block sparse column (BSC) storage format of\n",
      " |      given blocksize.  If the :attr:`self` is strided, then the number of\n",
      " |      dense dimensions could be specified, and a hybrid BSC tensor will be\n",
      " |      created, with `dense_dim` dense dimensions and `self.dim() - 2 -\n",
      " |      dense_dim` batch dimension.\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSC tensor. A block size must be a tuple of\n",
      " |            length two such that its items evenly divide the two sparse\n",
      " |            dimensions.\n",
      " |      \n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting BSC tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> dense = torch.randn(10, 10)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse_bsc = sparse.to_sparse_bsc((5, 5))\n",
      " |          >>> sparse_bsc.row_indices()\n",
      " |          tensor([0, 1, 0, 1])\n",
      " |      \n",
      " |          >>> dense = torch.zeros(4, 3, 1)\n",
      " |          >>> dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1\n",
      " |          >>> dense.to_sparse_bsc((2, 1), 1)\n",
      " |          tensor(ccol_indices=tensor([0, 1, 2, 3]),\n",
      " |                 row_indices=tensor([0, 1, 0]),\n",
      " |                 values=tensor([[[[1.]],\n",
      " |      \n",
      " |                                 [[1.]]],\n",
      " |      \n",
      " |      \n",
      " |                                [[[1.]],\n",
      " |      \n",
      " |                                 [[1.]]],\n",
      " |      \n",
      " |      \n",
      " |                                [[[1.]],\n",
      " |      \n",
      " |                                 [[1.]]]]), size=(4, 3, 1), nnz=3,\n",
      " |                 layout=torch.sparse_bsc)\n",
      " |  \n",
      " |  to_sparse_bsr(...)\n",
      " |      to_sparse_bsr(blocksize, dense_dim) -> Tensor\n",
      " |      \n",
      " |      Convert a tensor to a block sparse row (BSR) storage format of given\n",
      " |      blocksize.  If the :attr:`self` is strided, then the number of dense\n",
      " |      dimensions could be specified, and a hybrid BSR tensor will be\n",
      " |      created, with `dense_dim` dense dimensions and `self.dim() - 2 -\n",
      " |      dense_dim` batch dimension.\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          blocksize (list, tuple, :class:`torch.Size`, optional): Block size\n",
      " |            of the resulting BSR tensor. A block size must be a tuple of\n",
      " |            length two such that its items evenly divide the two sparse\n",
      " |            dimensions.\n",
      " |      \n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting BSR tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> dense = torch.randn(10, 10)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse_bsr = sparse.to_sparse_bsr((5, 5))\n",
      " |          >>> sparse_bsr.col_indices()\n",
      " |          tensor([0, 1, 0, 1])\n",
      " |      \n",
      " |          >>> dense = torch.zeros(4, 3, 1)\n",
      " |          >>> dense[0:2, 0] = dense[0:2, 2] = dense[2:4, 1] = 1\n",
      " |          >>> dense.to_sparse_bsr((2, 1), 1)\n",
      " |          tensor(crow_indices=tensor([0, 2, 3]),\n",
      " |                 col_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[[1.]],\n",
      " |      \n",
      " |                                 [[1.]]],\n",
      " |      \n",
      " |      \n",
      " |                                [[[1.]],\n",
      " |      \n",
      " |                                 [[1.]]],\n",
      " |      \n",
      " |      \n",
      " |                                [[[1.]],\n",
      " |      \n",
      " |                                 [[1.]]]]), size=(4, 3, 1), nnz=3,\n",
      " |                 layout=torch.sparse_bsr)\n",
      " |  \n",
      " |  to_sparse_csc(...)\n",
      " |      to_sparse_csc() -> Tensor\n",
      " |      \n",
      " |      Convert a tensor to compressed column storage (CSC) format.  Except\n",
      " |      for strided tensors, only works with 2D tensors.  If the :attr:`self`\n",
      " |      is strided, then the number of dense dimensions could be specified,\n",
      " |      and a hybrid CSC tensor will be created, with `dense_dim` dense\n",
      " |      dimensions and `self.dim() - 2 - dense_dim` batch dimension.\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSC tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> dense = torch.randn(5, 5)\n",
      " |          >>> sparse = dense.to_sparse_csc()\n",
      " |          >>> sparse._nnz()\n",
      " |          25\n",
      " |      \n",
      " |          >>> dense = torch.zeros(3, 3, 1, 1)\n",
      " |          >>> dense[0, 0] = dense[1, 2] = dense[2, 1] = 1\n",
      " |          >>> dense.to_sparse_csc(dense_dim=2)\n",
      " |          tensor(ccol_indices=tensor([0, 1, 2, 3]),\n",
      " |                 row_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[1.]],\n",
      " |      \n",
      " |                                [[1.]],\n",
      " |      \n",
      " |                                [[1.]]]), size=(3, 3, 1, 1), nnz=3,\n",
      " |                 layout=torch.sparse_csc)\n",
      " |  \n",
      " |  to_sparse_csr(...)\n",
      " |      to_sparse_csr(dense_dim=None) -> Tensor\n",
      " |      \n",
      " |      Convert a tensor to compressed row storage format (CSR).  Except for\n",
      " |      strided tensors, only works with 2D tensors.  If the :attr:`self` is\n",
      " |      strided, then the number of dense dimensions could be specified, and a\n",
      " |      hybrid CSR tensor will be created, with `dense_dim` dense dimensions\n",
      " |      and `self.dim() - 2 - dense_dim` batch dimension.\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          dense_dim (int, optional): Number of dense dimensions of the\n",
      " |            resulting CSR tensor.  This argument should be used only if\n",
      " |            :attr:`self` is a strided tensor, and must be a value between 0\n",
      " |            and dimension of :attr:`self` tensor minus two.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> dense = torch.randn(5, 5)\n",
      " |          >>> sparse = dense.to_sparse_csr()\n",
      " |          >>> sparse._nnz()\n",
      " |          25\n",
      " |      \n",
      " |          >>> dense = torch.zeros(3, 3, 1, 1)\n",
      " |          >>> dense[0, 0] = dense[1, 2] = dense[2, 1] = 1\n",
      " |          >>> dense.to_sparse_csr(dense_dim=2)\n",
      " |          tensor(crow_indices=tensor([0, 1, 2, 3]),\n",
      " |                 col_indices=tensor([0, 2, 1]),\n",
      " |                 values=tensor([[[1.]],\n",
      " |      \n",
      " |                                [[1.]],\n",
      " |      \n",
      " |                                [[1.]]]), size=(3, 3, 1, 1), nnz=3,\n",
      " |                 layout=torch.sparse_csr)\n",
      " |  \n",
      " |  tolist(...)\n",
      " |      tolist() -> list or number\n",
      " |      \n",
      " |      Returns the tensor as a (nested) list. For scalars, a standard\n",
      " |      Python number is returned, just like with :meth:`~Tensor.item`.\n",
      " |      Tensors are automatically moved to the CPU first if necessary.\n",
      " |      \n",
      " |      This operation is not differentiable.\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> a = torch.randn(2, 2)\n",
      " |          >>> a.tolist()\n",
      " |          [[0.012766935862600803, 0.5415473580360413],\n",
      " |           [-0.08909505605697632, 0.7729271650314331]]\n",
      " |          >>> a[0,0].tolist()\n",
      " |          0.012766935862600803\n",
      " |  \n",
      " |  topk(...)\n",
      " |      topk(k, dim=None, largest=True, sorted=True) -> (Tensor, LongTensor)\n",
      " |      \n",
      " |      See :func:`torch.topk`\n",
      " |  \n",
      " |  trace(...)\n",
      " |      trace() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.trace`\n",
      " |  \n",
      " |  transpose(...)\n",
      " |      transpose(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.transpose`\n",
      " |  \n",
      " |  transpose_(...)\n",
      " |      transpose_(dim0, dim1) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.transpose`\n",
      " |  \n",
      " |  triangular_solve(...)\n",
      " |      triangular_solve(A, upper=True, transpose=False, unitriangular=False) -> (Tensor, Tensor)\n",
      " |      \n",
      " |      See :func:`torch.triangular_solve`\n",
      " |  \n",
      " |  tril(...)\n",
      " |      tril(diagonal=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.tril`\n",
      " |  \n",
      " |  tril_(...)\n",
      " |      tril_(diagonal=0) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.tril`\n",
      " |  \n",
      " |  triu(...)\n",
      " |      triu(diagonal=0) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.triu`\n",
      " |  \n",
      " |  triu_(...)\n",
      " |      triu_(diagonal=0) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.triu`\n",
      " |  \n",
      " |  true_divide(...)\n",
      " |      true_divide(value) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.true_divide`\n",
      " |  \n",
      " |  true_divide_(...)\n",
      " |      true_divide_(value) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.true_divide_`\n",
      " |  \n",
      " |  trunc(...)\n",
      " |      trunc() -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.trunc`\n",
      " |  \n",
      " |  trunc_(...)\n",
      " |      trunc_() -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.trunc`\n",
      " |  \n",
      " |  type(...)\n",
      " |      type(dtype=None, non_blocking=False, **kwargs) -> str or Tensor\n",
      " |      Returns the type if `dtype` is not provided, else casts this object to\n",
      " |      the specified type.\n",
      " |      \n",
      " |      If this is already of the correct type, no copy is performed and the\n",
      " |      original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          dtype (dtype or string): The desired type\n",
      " |          non_blocking (bool): If ``True``, and the source is in pinned memory\n",
      " |              and destination is on the GPU or vice versa, the copy is performed\n",
      " |              asynchronously with respect to the host. Otherwise, the argument\n",
      " |              has no effect.\n",
      " |          **kwargs: For compatibility, may contain the key ``async`` in place of\n",
      " |              the ``non_blocking`` argument. The ``async`` arg is deprecated.\n",
      " |  \n",
      " |  type_as(...)\n",
      " |      type_as(tensor) -> Tensor\n",
      " |      \n",
      " |      Returns this tensor cast to the type of the given tensor.\n",
      " |      \n",
      " |      This is a no-op if the tensor is already of the correct type. This is\n",
      " |      equivalent to ``self.type(tensor.type())``\n",
      " |      \n",
      " |      Args:\n",
      " |          tensor (Tensor): the tensor which has the desired type\n",
      " |  \n",
      " |  unbind(...)\n",
      " |      unbind(dim=0) -> seq\n",
      " |      \n",
      " |      See :func:`torch.unbind`\n",
      " |  \n",
      " |  unfold(...)\n",
      " |      unfold(dimension, size, step) -> Tensor\n",
      " |      \n",
      " |      Returns a view of the original tensor which contains all slices of size :attr:`size` from\n",
      " |      :attr:`self` tensor in the dimension :attr:`dimension`.\n",
      " |      \n",
      " |      Step between two slices is given by :attr:`step`.\n",
      " |      \n",
      " |      If `sizedim` is the size of dimension :attr:`dimension` for :attr:`self`, the size of\n",
      " |      dimension :attr:`dimension` in the returned tensor will be\n",
      " |      `(sizedim - size) / step + 1`.\n",
      " |      \n",
      " |      An additional dimension of size :attr:`size` is appended in the returned tensor.\n",
      " |      \n",
      " |      Args:\n",
      " |          dimension (int): dimension in which unfolding happens\n",
      " |          size (int): the size of each slice that is unfolded\n",
      " |          step (int): the step between each slice\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.arange(1., 8)\n",
      " |          >>> x\n",
      " |          tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.])\n",
      " |          >>> x.unfold(0, 2, 1)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 2.,  3.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 4.,  5.],\n",
      " |                  [ 5.,  6.],\n",
      " |                  [ 6.,  7.]])\n",
      " |          >>> x.unfold(0, 2, 2)\n",
      " |          tensor([[ 1.,  2.],\n",
      " |                  [ 3.,  4.],\n",
      " |                  [ 5.,  6.]])\n",
      " |  \n",
      " |  uniform_(...)\n",
      " |      uniform_(from=0, to=1) -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with numbers sampled from the continuous uniform\n",
      " |      distribution:\n",
      " |      \n",
      " |      .. math::\n",
      " |          P(x) = \\dfrac{1}{\\text{to} - \\text{from}}\n",
      " |  \n",
      " |  unsafe_chunk(...)\n",
      " |      unsafe_chunk(chunks, dim=0) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.unsafe_chunk`\n",
      " |  \n",
      " |  unsafe_split(...)\n",
      " |      unsafe_split(split_size, dim=0) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.unsafe_split`\n",
      " |  \n",
      " |  unsafe_split_with_sizes(...)\n",
      " |  \n",
      " |  unsqueeze(...)\n",
      " |      unsqueeze(dim) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.unsqueeze`\n",
      " |  \n",
      " |  unsqueeze_(...)\n",
      " |      unsqueeze_(dim) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.unsqueeze`\n",
      " |  \n",
      " |  untyped_storage(...)\n",
      " |      untyped_storage() -> torch.UntypedStorage\n",
      " |      \n",
      " |      Returns the underlying :class:`UntypedStorage`.\n",
      " |  \n",
      " |  values(...)\n",
      " |      values() -> Tensor\n",
      " |      \n",
      " |      Return the values tensor of a :ref:`sparse COO tensor <sparse-coo-docs>`.\n",
      " |      \n",
      " |      .. warning::\n",
      " |        Throws an error if :attr:`self` is not a sparse COO tensor.\n",
      " |      \n",
      " |      See also :meth:`Tensor.indices`.\n",
      " |      \n",
      " |      .. note::\n",
      " |        This method can only be called on a coalesced sparse tensor. See\n",
      " |        :meth:`Tensor.coalesce` for details.\n",
      " |  \n",
      " |  var(...)\n",
      " |      var(dim=None, *, correction=1, keepdim=False) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.var`\n",
      " |  \n",
      " |  vdot(...)\n",
      " |      vdot(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.vdot`\n",
      " |  \n",
      " |  view(...)\n",
      " |      view(*shape) -> Tensor\n",
      " |      \n",
      " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      " |      different :attr:`shape`.\n",
      " |      \n",
      " |      The returned tensor shares the same data and must have the same number\n",
      " |      of elements, but may have a different size. For a tensor to be viewed, the new\n",
      " |      view size must be compatible with its original size and stride, i.e., each new\n",
      " |      view dimension must either be a subspace of an original dimension, or only span\n",
      " |      across original dimensions :math:`d, d+1, \\dots, d+k` that satisfy the following\n",
      " |      contiguity-like condition that :math:`\\forall i = d, \\dots, d+k-1`,\n",
      " |      \n",
      " |      .. math::\n",
      " |      \n",
      " |        \\text{stride}[i] = \\text{stride}[i+1] \\times \\text{size}[i+1]\n",
      " |      \n",
      " |      Otherwise, it will not be possible to view :attr:`self` tensor as :attr:`shape`\n",
      " |      without copying it (e.g., via :meth:`contiguous`). When it is unclear whether a\n",
      " |      :meth:`view` can be performed, it is advisable to use :meth:`reshape`, which\n",
      " |      returns a view if the shapes are compatible, and copies (equivalent to calling\n",
      " |      :meth:`contiguous`) otherwise.\n",
      " |      \n",
      " |      Args:\n",
      " |          shape (torch.Size or int...): the desired size\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x.size()\n",
      " |          torch.Size([4, 4])\n",
      " |          >>> y = x.view(16)\n",
      " |          >>> y.size()\n",
      " |          torch.Size([16])\n",
      " |          >>> z = x.view(-1, 8)  # the size -1 is inferred from other dimensions\n",
      " |          >>> z.size()\n",
      " |          torch.Size([2, 8])\n",
      " |      \n",
      " |          >>> a = torch.randn(1, 2, 3, 4)\n",
      " |          >>> a.size()\n",
      " |          torch.Size([1, 2, 3, 4])\n",
      " |          >>> b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension\n",
      " |          >>> b.size()\n",
      " |          torch.Size([1, 3, 2, 4])\n",
      " |          >>> c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory\n",
      " |          >>> c.size()\n",
      " |          torch.Size([1, 3, 2, 4])\n",
      " |          >>> torch.equal(b, c)\n",
      " |          False\n",
      " |      \n",
      " |      \n",
      " |      .. method:: view(dtype) -> Tensor\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Returns a new tensor with the same data as the :attr:`self` tensor but of a\n",
      " |      different :attr:`dtype`.\n",
      " |      \n",
      " |      If the element size of :attr:`dtype` is different than that of ``self.dtype``,\n",
      " |      then the size of the last dimension of the output will be scaled\n",
      " |      proportionally.  For instance, if :attr:`dtype` element size is twice that of\n",
      " |      ``self.dtype``, then each pair of elements in the last dimension of\n",
      " |      :attr:`self` will be combined, and the size of the last dimension of the output\n",
      " |      will be half that of :attr:`self`. If :attr:`dtype` element size is half that\n",
      " |      of ``self.dtype``, then each element in the last dimension of :attr:`self` will\n",
      " |      be split in two, and the size of the last dimension of the output will be\n",
      " |      double that of :attr:`self`. For this to be possible, the following conditions\n",
      " |      must be true:\n",
      " |      \n",
      " |          * ``self.dim()`` must be greater than 0.\n",
      " |          * ``self.stride(-1)`` must be 1.\n",
      " |      \n",
      " |      Additionally, if the element size of :attr:`dtype` is greater than that of\n",
      " |      ``self.dtype``, the following conditions must be true as well:\n",
      " |      \n",
      " |          * ``self.size(-1)`` must be divisible by the ratio between the element\n",
      " |            sizes of the dtypes.\n",
      " |          * ``self.storage_offset()`` must be divisible by the ratio between the\n",
      " |            element sizes of the dtypes.\n",
      " |          * The strides of all dimensions, except the last dimension, must be\n",
      " |            divisible by the ratio between the element sizes of the dtypes.\n",
      " |      \n",
      " |      If any of the above conditions are not met, an error is thrown.\n",
      " |      \n",
      " |      .. warning::\n",
      " |      \n",
      " |          This overload is not supported by TorchScript, and using it in a Torchscript\n",
      " |          program will cause undefined behavior.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |          dtype (:class:`torch.dtype`): the desired dtype\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> x = torch.randn(4, 4)\n",
      " |          >>> x\n",
      " |          tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],\n",
      " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      " |          >>> x.dtype\n",
      " |          torch.float32\n",
      " |      \n",
      " |          >>> y = x.view(torch.int32)\n",
      " |          >>> y\n",
      " |          tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],\n",
      " |                  [-1105482831,  1061112040,  1057999968, -1084397505],\n",
      " |                  [-1071760287, -1123489973, -1097310419, -1084649136],\n",
      " |                  [-1101533110,  1073668768, -1082790149, -1088634448]],\n",
      " |              dtype=torch.int32)\n",
      " |          >>> y[0, 0] = 1000000000\n",
      " |          >>> x\n",
      " |          tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],\n",
      " |                  [-0.1520,  0.7472,  0.5617, -0.8649],\n",
      " |                  [-2.4724, -0.0334, -0.2976, -0.8499],\n",
      " |                  [-0.2109,  1.9913, -0.9607, -0.6123]])\n",
      " |      \n",
      " |          >>> x.view(torch.cfloat)\n",
      " |          tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],\n",
      " |                  [-0.1520+0.7472j,  0.5617-0.8649j],\n",
      " |                  [-2.4724-0.0334j, -0.2976-0.8499j],\n",
      " |                  [-0.2109+1.9913j, -0.9607-0.6123j]])\n",
      " |          >>> x.view(torch.cfloat).size()\n",
      " |          torch.Size([4, 2])\n",
      " |      \n",
      " |          >>> x.view(torch.uint8)\n",
      " |          tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,\n",
      " |                     8, 191],\n",
      " |                  [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,\n",
      " |                    93, 191],\n",
      " |                  [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,\n",
      " |                    89, 191],\n",
      " |                  [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,\n",
      " |                    28, 191]], dtype=torch.uint8)\n",
      " |          >>> x.view(torch.uint8).size()\n",
      " |          torch.Size([4, 16])\n",
      " |  \n",
      " |  view_as(...)\n",
      " |      view_as(other) -> Tensor\n",
      " |      \n",
      " |      View this tensor as the same size as :attr:`other`.\n",
      " |      ``self.view_as(other)`` is equivalent to ``self.view(other.size())``.\n",
      " |      \n",
      " |      Please see :meth:`~Tensor.view` for more information about ``view``.\n",
      " |      \n",
      " |      Args:\n",
      " |          other (:class:`torch.Tensor`): The result tensor has the same size\n",
      " |              as :attr:`other`.\n",
      " |  \n",
      " |  vsplit(...)\n",
      " |      vsplit(split_size_or_sections) -> List of Tensors\n",
      " |      \n",
      " |      See :func:`torch.vsplit`\n",
      " |  \n",
      " |  where(...)\n",
      " |      where(condition, y) -> Tensor\n",
      " |      \n",
      " |      ``self.where(condition, y)`` is equivalent to ``torch.where(condition, self, y)``.\n",
      " |      See :func:`torch.where`\n",
      " |  \n",
      " |  xlogy(...)\n",
      " |      xlogy(other) -> Tensor\n",
      " |      \n",
      " |      See :func:`torch.xlogy`\n",
      " |  \n",
      " |  xlogy_(...)\n",
      " |      xlogy_(other) -> Tensor\n",
      " |      \n",
      " |      In-place version of :meth:`~Tensor.xlogy`\n",
      " |  \n",
      " |  xpu(...)\n",
      " |      xpu(device=None, non_blocking=False, memory_format=torch.preserve_format) -> Tensor\n",
      " |      \n",
      " |      Returns a copy of this object in XPU memory.\n",
      " |      \n",
      " |      If this object is already in XPU memory and on the correct device,\n",
      " |      then no copy is performed and the original object is returned.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The destination XPU device.\n",
      " |              Defaults to the current XPU device.\n",
      " |          non_blocking (bool): If ``True`` and the source is in pinned memory,\n",
      " |              the copy will be asynchronous with respect to the host.\n",
      " |              Otherwise, the argument has no effect. Default: ``False``.\n",
      " |          memory_format (:class:`torch.memory_format`, optional): the desired memory format of\n",
      " |              returned Tensor. Default: ``torch.preserve_format``.\n",
      " |  \n",
      " |  zero_(...)\n",
      " |      zero_() -> Tensor\n",
      " |      \n",
      " |      Fills :attr:`self` tensor with zeros.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch._C._TensorBase:\n",
      " |  \n",
      " |  H\n",
      " |      Returns a view of a matrix (2-D tensor) conjugated and transposed.\n",
      " |      \n",
      " |      ``x.H`` is equivalent to ``x.transpose(0, 1).conj()`` for complex matrices and\n",
      " |      ``x.transpose(0, 1)`` for real matrices.\n",
      " |      \n",
      " |      .. seealso::\n",
      " |      \n",
      " |              :attr:`~.Tensor.mH`: An attribute that also works on batches of matrices.\n",
      " |  \n",
      " |  T\n",
      " |      Returns a view of this tensor with its dimensions reversed.\n",
      " |      \n",
      " |      If ``n`` is the number of dimensions in ``x``,\n",
      " |      ``x.T`` is equivalent to ``x.permute(n-1, n-2, ..., 0)``.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          The use of :func:`Tensor.T` on tensors of dimension other than 2 to reverse their shape\n",
      " |          is deprecated and it will throw an error in a future release. Consider :attr:`~.Tensor.mT`\n",
      " |          to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse\n",
      " |          the dimensions of a tensor.\n",
      " |  \n",
      " |  data\n",
      " |  \n",
      " |  device\n",
      " |      Is the :class:`torch.device` where this Tensor is.\n",
      " |  \n",
      " |  dtype\n",
      " |  \n",
      " |  grad\n",
      " |      This attribute is ``None`` by default and becomes a Tensor the first time a call to\n",
      " |      :func:`backward` computes gradients for ``self``.\n",
      " |      The attribute will then contain the gradients computed and future calls to\n",
      " |      :func:`backward` will accumulate (add) gradients into it.\n",
      " |  \n",
      " |  grad_fn\n",
      " |  \n",
      " |  imag\n",
      " |      Returns a new tensor containing imaginary values of the :attr:`self` tensor.\n",
      " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          :func:`imag` is only supported for tensors with complex dtypes.\n",
      " |      \n",
      " |      Example::\n",
      " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
      " |          >>> x\n",
      " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
      " |          >>> x.imag\n",
      " |          tensor([ 0.3553, -0.7896, -0.0633, -0.8119])\n",
      " |  \n",
      " |  is_cpu\n",
      " |      Is ``True`` if the Tensor is stored on the CPU, ``False`` otherwise.\n",
      " |  \n",
      " |  is_cuda\n",
      " |      Is ``True`` if the Tensor is stored on the GPU, ``False`` otherwise.\n",
      " |  \n",
      " |  is_ipu\n",
      " |      Is ``True`` if the Tensor is stored on the IPU, ``False`` otherwise.\n",
      " |  \n",
      " |  is_leaf\n",
      " |      All Tensors that have :attr:`requires_grad` which is ``False`` will be leaf Tensors by convention.\n",
      " |      \n",
      " |      For Tensors that have :attr:`requires_grad` which is ``True``, they will be leaf Tensors if they were\n",
      " |      created by the user. This means that they are not the result of an operation and so\n",
      " |      :attr:`grad_fn` is None.\n",
      " |      \n",
      " |      Only leaf Tensors will have their :attr:`grad` populated during a call to :func:`backward`.\n",
      " |      To get :attr:`grad` populated for non-leaf Tensors, you can use :func:`retain_grad`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> a = torch.rand(10, requires_grad=True)\n",
      " |          >>> a.is_leaf\n",
      " |          True\n",
      " |          >>> b = torch.rand(10, requires_grad=True).cuda()\n",
      " |          >>> b.is_leaf\n",
      " |          False\n",
      " |          # b was created by the operation that cast a cpu Tensor into a cuda Tensor\n",
      " |          >>> c = torch.rand(10, requires_grad=True) + 2\n",
      " |          >>> c.is_leaf\n",
      " |          False\n",
      " |          # c was created by the addition operation\n",
      " |          >>> d = torch.rand(10).cuda()\n",
      " |          >>> d.is_leaf\n",
      " |          True\n",
      " |          # d does not require gradients and so has no operation creating it (that is tracked by the autograd engine)\n",
      " |          >>> e = torch.rand(10).cuda().requires_grad_()\n",
      " |          >>> e.is_leaf\n",
      " |          True\n",
      " |          # e requires gradients and has no operations creating it\n",
      " |          >>> f = torch.rand(10, requires_grad=True, device=\"cuda\")\n",
      " |          >>> f.is_leaf\n",
      " |          True\n",
      " |          # f requires grad, has no operation creating it\n",
      " |  \n",
      " |  is_meta\n",
      " |      Is ``True`` if the Tensor is a meta tensor, ``False`` otherwise.  Meta tensors\n",
      " |      are like normal tensors, but they carry no data.\n",
      " |  \n",
      " |  is_mkldnn\n",
      " |  \n",
      " |  is_mps\n",
      " |      Is ``True`` if the Tensor is stored on the MPS device, ``False`` otherwise.\n",
      " |  \n",
      " |  is_nested\n",
      " |  \n",
      " |  is_ort\n",
      " |  \n",
      " |  is_quantized\n",
      " |      Is ``True`` if the Tensor is quantized, ``False`` otherwise.\n",
      " |  \n",
      " |  is_sparse\n",
      " |      Is ``True`` if the Tensor uses sparse storage layout, ``False`` otherwise.\n",
      " |  \n",
      " |  is_sparse_csr\n",
      " |      Is ``True`` if the Tensor uses sparse CSR storage layout, ``False`` otherwise.\n",
      " |  \n",
      " |  is_vulkan\n",
      " |  \n",
      " |  is_xpu\n",
      " |      Is ``True`` if the Tensor is stored on the XPU, ``False`` otherwise.\n",
      " |  \n",
      " |  layout\n",
      " |  \n",
      " |  mH\n",
      " |      Accessing this property is equivalent to calling :func:`adjoint`.\n",
      " |  \n",
      " |  mT\n",
      " |      Returns a view of this tensor with the last two dimensions transposed.\n",
      " |      \n",
      " |      ``x.mT`` is equivalent to ``x.transpose(-2, -1)``.\n",
      " |  \n",
      " |  name\n",
      " |  \n",
      " |  names\n",
      " |      Stores names for each of this tensor's dimensions.\n",
      " |      \n",
      " |      ``names[idx]`` corresponds to the name of tensor dimension ``idx``.\n",
      " |      Names are either a string if the dimension is named or ``None`` if the\n",
      " |      dimension is unnamed.\n",
      " |      \n",
      " |      Dimension names may contain characters or underscore. Furthermore, a dimension\n",
      " |      name must be a valid Python variable name (i.e., does not start with underscore).\n",
      " |      \n",
      " |      Tensors may not have two named dimensions with the same name.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          The named tensor API is experimental and subject to change.\n",
      " |  \n",
      " |  ndim\n",
      " |      Alias for :meth:`~Tensor.dim()`\n",
      " |  \n",
      " |  output_nr\n",
      " |  \n",
      " |  real\n",
      " |      Returns a new tensor containing real values of the :attr:`self` tensor for a complex-valued input tensor.\n",
      " |      The returned tensor and :attr:`self` share the same underlying storage.\n",
      " |      \n",
      " |      Returns :attr:`self` if :attr:`self` is a real-valued tensor tensor.\n",
      " |      \n",
      " |      Example::\n",
      " |          >>> x=torch.randn(4, dtype=torch.cfloat)\n",
      " |          >>> x\n",
      " |          tensor([(0.3100+0.3553j), (-0.5445-0.7896j), (-1.6492-0.0633j), (-0.0638-0.8119j)])\n",
      " |          >>> x.real\n",
      " |          tensor([ 0.3100, -0.5445, -1.6492, -0.0638])\n",
      " |  \n",
      " |  requires_grad\n",
      " |      Is ``True`` if gradients need to be computed for this Tensor, ``False`` otherwise.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |          The fact that gradients need to be computed for a Tensor do not mean that the :attr:`grad`\n",
      " |          attribute will be populated, see :attr:`is_leaf` for more details.\n",
      " |  \n",
      " |  retains_grad\n",
      " |      Is ``True`` if this Tensor is non-leaf and its :attr:`grad` is enabled to be\n",
      " |      populated during :func:`backward`, ``False`` otherwise.\n",
      " |  \n",
      " |  shape\n",
      " |  \n",
      " |  volatile\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(temp_model.layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([20, 10]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp_model.layer1.weight.shape),temp_model.layer1.weight.shape,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, str)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "import numpy\n",
    "\n",
    "weights = temp_model.layer1.weight.data\n",
    "block1 = temp_model.layer1.weight.data.numpy()\n",
    "block2 = loaded_model_state['layer1.weight'].numpy()\n",
    "\n",
    "block_hash1 = hashlib.sha256(block1.tobytes()).hexdigest()\n",
    "block_hash2 = hashlib.sha256(block2.tobytes()).hexdigest()\n",
    "\n",
    "block_hash1 == block_hash2, type(block_hash2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dir create.\n",
      "File create.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "hdf5_file_path = block_hash1\n",
    "dirs = \"./block_files/TEST\"\n",
    "file_name = os.path.join(dirs,hdf5_file_path+\".h5\")\n",
    "\n",
    "if not os.path.exists(dirs):\n",
    "    os.makedirs(dirs)\n",
    "    print(\"Dir create.\")\n",
    "else:\n",
    "    print(\"Dir exist.\")  \n",
    "    \n",
    "if not os.path.exists(file_name):\n",
    "    with h5py.File(file_name, 'w') as hf:\n",
    "        hf.create_dataset('weights', data=weights)\n",
    "    print(\"File create.\")\n",
    "else:\n",
    "    print(\"File exist.\")\n",
    "    \n",
    "            \n",
    "            \n",
    "# if os.path.exists(dirs) and os.path.exists(file_name):\n",
    "#     print(\"Dir exists.\")\n",
    "# else:\n",
    "#     if not os.path.exists(file_name):\n",
    "#         with h5py.File(file_name, 'w') as hf:\n",
    "#             hf.create_dataset('weights', data=weights) \n",
    "#         print(\"File creates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0657, -0.1457, -0.0153,  0.2022, -0.0098, -0.2434, -0.1064,  0.1861,\n",
       "        -0.3156, -0.0242])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model.layer1.weight.data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n",
      "torch.Size([20])\n",
      "torch.Size([30, 20])\n",
      "torch.Size([30])\n",
      "torch.Size([5, 30])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "for k,v in temp_model_state.items():\n",
    "    print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 2, 16):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model.layer1.weight.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2426]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0:1,9:19492]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_model'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = \"./adad/adada/my_model.pt\"\n",
    "file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "file_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'layer2'\n",
    "b = temp_model.state_dict()['layer1.weight'].shape\n",
    "with open('hash_info.txt', 'a') as file:\n",
    "    file.write(\"[s]\")\n",
    "    file.write(f\"{a}:{b}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer name is layer1.\n",
      "hash value are ['fdfsfwe23', 'fef33'].\n",
      "layer name is layer2.\n",
      "hash value are ['adsada', 'fef33'].\n",
      "layer name is layer2.\n",
      "hash value are ['adsada', 'fef33'].\n"
     ]
    }
   ],
   "source": [
    "with open('hash_info.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        parts = line.split(':')\n",
    "        layer = parts[0]\n",
    "        print(f\"layer name is {layer}.\")\n",
    "        has_value = parts[1][1:-1].split(',')\n",
    "        for i in range(len(has_value)):\n",
    "            has_value[i] = has_value[i][2:-2]\n",
    "        print(f\"hash value are {has_value}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'sfdfsfwe232', 'fef332'\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = \"['sfdfsfwe232', 'fef332']\"\n",
    "str = str[1:-1]\n",
    "str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('layer1.weight',\n",
       "              tensor([[-0.3043, -0.1405,  0.1828, -0.1613, -0.1103, -0.1103, -0.0647,  0.2471,\n",
       "                       -0.0250, -0.0297],\n",
       "                      [ 0.1087, -0.2892,  0.0645,  0.1057,  0.1449,  0.2218, -0.1186,  0.2042,\n",
       "                       -0.2256,  0.1482],\n",
       "                      [-0.1018,  0.0414,  0.1842, -0.2359,  0.2814, -0.2790, -0.2442,  0.1207,\n",
       "                        0.0629,  0.1218],\n",
       "                      [-0.1345,  0.0481,  0.0255, -0.0806, -0.1713, -0.2509, -0.0230, -0.0381,\n",
       "                       -0.2611, -0.2364],\n",
       "                      [-0.0640,  0.1308, -0.2923, -0.1276,  0.2030, -0.0780, -0.0863, -0.0105,\n",
       "                       -0.2465, -0.3130],\n",
       "                      [ 0.2388, -0.2534,  0.2532, -0.3075, -0.0874,  0.1945, -0.0466, -0.0656,\n",
       "                       -0.0473, -0.2508],\n",
       "                      [-0.2468,  0.1568,  0.1116, -0.0136,  0.1595, -0.1011,  0.1885,  0.1895,\n",
       "                        0.3063, -0.0671],\n",
       "                      [-0.1310, -0.0613,  0.2123,  0.2174,  0.0429,  0.2266,  0.0005, -0.1755,\n",
       "                       -0.0132,  0.2872],\n",
       "                      [ 0.1597, -0.2958, -0.1711,  0.2935,  0.2341, -0.0812,  0.2751,  0.2193,\n",
       "                        0.2476, -0.1913],\n",
       "                      [ 0.0623, -0.1759, -0.2225, -0.2161, -0.0792,  0.0907,  0.2708, -0.2632,\n",
       "                        0.0271, -0.2851],\n",
       "                      [-0.2871, -0.1971, -0.0785, -0.2908,  0.1201, -0.1728, -0.1260,  0.2470,\n",
       "                        0.0697, -0.2519],\n",
       "                      [-0.2424, -0.1421,  0.0051,  0.1878, -0.2581, -0.1477, -0.0416, -0.0287,\n",
       "                        0.0948,  0.3076],\n",
       "                      [-0.0736, -0.0987, -0.1207,  0.2934, -0.2641, -0.0367,  0.1581,  0.2564,\n",
       "                        0.2768,  0.1311],\n",
       "                      [ 0.1874, -0.0815,  0.2233, -0.2568,  0.1304,  0.2970, -0.1437,  0.2480,\n",
       "                       -0.2154, -0.0932],\n",
       "                      [ 0.2730, -0.1168,  0.2123, -0.3087,  0.0797,  0.2368,  0.1304,  0.0417,\n",
       "                        0.1831, -0.1695],\n",
       "                      [-0.0800,  0.0288, -0.1927,  0.2499, -0.0104, -0.0199,  0.3108, -0.1588,\n",
       "                       -0.0020, -0.1335],\n",
       "                      [ 0.1200,  0.1681, -0.2190,  0.0144,  0.1884, -0.0726,  0.1208,  0.0307,\n",
       "                        0.2287, -0.0079],\n",
       "                      [-0.0418,  0.2642, -0.0756, -0.1534, -0.1416,  0.2862,  0.0018,  0.2052,\n",
       "                        0.1246, -0.1760],\n",
       "                      [ 0.0795,  0.1534,  0.1753,  0.2667, -0.2928, -0.2747, -0.0737, -0.1750,\n",
       "                       -0.2223, -0.0699],\n",
       "                      [-0.0593, -0.2365, -0.2682, -0.2438, -0.2597, -0.1585, -0.0648, -0.1763,\n",
       "                        0.2391,  0.0148]])),\n",
       "             ('layer1.bias',\n",
       "              tensor([-0.2464,  0.1146, -0.0766, -0.1257, -0.0492, -0.2676, -0.1328,  0.2497,\n",
       "                      -0.2535, -0.1606, -0.0786,  0.0381,  0.0216, -0.2997, -0.0164,  0.1954,\n",
       "                      -0.0863,  0.0064, -0.0329,  0.0881])),\n",
       "             ('layer2.weight',\n",
       "              tensor([[ 1.2627e-01,  2.2189e-01,  1.1112e-01, -1.5570e-01, -4.8542e-02,\n",
       "                        8.4769e-02, -2.1164e-01,  2.1957e-01, -2.1178e-01, -1.8703e-01,\n",
       "                        4.3175e-02,  1.7754e-01,  1.1732e-01,  3.5934e-02,  1.5101e-02,\n",
       "                       -2.1207e-01, -4.4739e-02, -7.6392e-02, -4.3298e-02,  1.9029e-01],\n",
       "                      [ 6.8214e-02,  2.1261e-01,  3.6947e-02,  9.2854e-02, -9.5580e-02,\n",
       "                        1.8530e-01,  9.3286e-02,  1.2177e-01,  1.9131e-01, -1.8038e-01,\n",
       "                        2.9181e-02, -5.2813e-02,  1.5744e-01, -1.2679e-02,  1.2097e-01,\n",
       "                        1.0470e-01,  3.2376e-02, -8.4969e-02,  8.2885e-02, -1.0146e-01],\n",
       "                      [ 7.4613e-02,  1.1598e-01,  8.2787e-02, -2.1923e-01,  1.6968e-01,\n",
       "                       -1.0874e-01, -1.3013e-01,  1.7536e-01, -1.9208e-01, -1.3591e-01,\n",
       "                        5.2652e-02, -4.5504e-02,  2.2298e-01,  1.9997e-01,  4.3312e-02,\n",
       "                        5.8314e-02,  1.4259e-01, -5.6239e-02, -9.2740e-02, -2.3311e-02],\n",
       "                      [-4.3734e-02,  4.0986e-02, -2.0868e-01,  2.0155e-01, -3.2980e-02,\n",
       "                       -5.3518e-02,  2.0563e-01, -5.0777e-02,  3.7773e-02,  1.0279e-01,\n",
       "                       -9.2190e-02, -9.3686e-02, -1.2838e-01,  1.7589e-01, -2.4636e-02,\n",
       "                       -1.7875e-01,  1.2945e-01, -1.0098e-02, -2.0484e-01,  1.9401e-01],\n",
       "                      [ 8.9313e-03, -1.0424e-01,  1.9656e-01,  1.3335e-01, -1.8362e-01,\n",
       "                       -1.7010e-01,  5.3391e-02,  8.1706e-02,  5.6856e-02, -1.9184e-01,\n",
       "                       -6.0178e-02,  8.5864e-02, -1.1078e-01,  1.5559e-01,  9.6113e-02,\n",
       "                       -6.1419e-02,  2.1295e-02, -6.9854e-02,  4.5333e-02,  1.6211e-01],\n",
       "                      [ 1.5048e-01, -1.6515e-01, -2.1882e-01, -1.2502e-01, -3.4249e-02,\n",
       "                        4.0294e-02, -1.8844e-01,  2.0945e-01, -2.0796e-01, -7.0833e-03,\n",
       "                       -1.9144e-01, -7.5077e-02,  2.0713e-01,  2.1149e-01, -1.0310e-01,\n",
       "                       -7.5997e-03,  1.9326e-01, -6.0069e-02,  1.5278e-01, -1.5707e-01],\n",
       "                      [ 3.8676e-02,  6.4843e-02,  1.7767e-02, -1.8773e-01, -1.3430e-01,\n",
       "                       -2.1236e-01, -2.1178e-02, -2.7322e-02,  1.4960e-03,  1.2941e-01,\n",
       "                       -1.3942e-01, -6.1069e-02,  1.6364e-02,  4.4624e-02, -1.0013e-01,\n",
       "                        2.0560e-01,  6.3615e-02, -1.0902e-01, -4.1851e-02,  1.6281e-01],\n",
       "                      [ 2.7842e-02, -2.1612e-01,  1.0246e-01,  5.1756e-02,  1.2372e-01,\n",
       "                        1.9037e-01, -1.5693e-01,  2.0969e-01,  1.2704e-01, -1.2746e-01,\n",
       "                       -1.2986e-01,  2.2257e-01,  8.1657e-02, -1.8497e-01,  9.7611e-02,\n",
       "                       -3.1400e-03,  5.8986e-02,  1.1808e-01,  2.1401e-01, -3.4539e-02],\n",
       "                      [-1.4436e-01,  6.6000e-02, -1.8757e-01,  6.5860e-02,  8.0064e-02,\n",
       "                        8.1243e-02,  1.9538e-01, -2.1805e-01, -5.9540e-02,  7.5290e-02,\n",
       "                       -1.3154e-01, -4.2055e-02,  2.9019e-02, -1.1675e-01, -1.0761e-01,\n",
       "                       -1.4887e-02, -1.7025e-02,  8.6265e-02, -1.2164e-02, -5.3294e-02],\n",
       "                      [-2.0632e-01, -1.3057e-01, -2.1677e-01, -1.1842e-01,  1.1781e-02,\n",
       "                       -3.0722e-03,  1.5641e-01,  6.2656e-02, -1.7530e-01, -1.5906e-01,\n",
       "                        2.1968e-01, -4.0365e-02, -1.6817e-01,  6.4646e-02, -1.8539e-02,\n",
       "                       -4.3749e-02,  6.1747e-02, -2.6904e-03, -5.4039e-02,  2.1309e-01],\n",
       "                      [ 8.9345e-02,  1.2757e-01,  1.2703e-01, -1.5688e-01, -1.6211e-01,\n",
       "                       -1.7506e-03, -1.9788e-01, -1.5197e-01, -9.6741e-03,  6.5480e-02,\n",
       "                       -1.6944e-01,  8.3370e-02,  1.6858e-01,  2.0947e-01, -3.3187e-02,\n",
       "                       -1.7084e-02, -1.4878e-01,  5.6851e-02,  9.9655e-02, -1.4209e-02],\n",
       "                      [-8.5748e-03, -8.4113e-02, -2.5664e-02, -1.0187e-02, -1.7911e-01,\n",
       "                        1.2735e-01, -9.7017e-02,  1.1162e-01,  9.8900e-02, -1.7405e-01,\n",
       "                       -1.6987e-01,  7.1528e-02, -7.5878e-02, -2.1291e-01, -7.6915e-02,\n",
       "                       -1.2683e-01,  8.4714e-02, -9.4461e-03, -1.0815e-01,  5.1438e-02],\n",
       "                      [ 8.9400e-02, -2.2062e-02, -1.1327e-01,  1.3362e-01, -4.1498e-02,\n",
       "                       -6.6469e-03, -4.9049e-02,  1.4740e-01,  1.5805e-02, -2.8627e-02,\n",
       "                       -1.0992e-01,  1.6348e-01,  1.3842e-01,  1.1140e-01, -8.3107e-02,\n",
       "                        5.7800e-02,  2.0924e-01, -6.7385e-02, -6.8806e-02,  2.0515e-01],\n",
       "                      [-2.0512e-01, -2.9330e-02, -1.1969e-01,  1.5765e-01, -5.8953e-02,\n",
       "                        1.3059e-01,  1.9347e-01,  6.6114e-02,  2.0823e-01, -6.7303e-02,\n",
       "                       -1.5523e-02,  6.1942e-02,  8.0640e-02, -1.4831e-01, -1.7031e-02,\n",
       "                       -1.3413e-02,  4.9968e-02,  4.0673e-02, -1.5521e-01,  1.7534e-01],\n",
       "                      [-1.4266e-01,  1.9708e-01,  5.7336e-02,  1.3580e-01, -1.9093e-01,\n",
       "                        4.5385e-02,  1.2343e-01, -1.1069e-01, -2.0367e-01, -1.7843e-01,\n",
       "                       -1.8635e-01,  9.4840e-02,  8.6661e-02, -3.8228e-02, -1.5516e-01,\n",
       "                       -5.7098e-02, -2.5244e-02, -9.9784e-02, -1.8173e-01,  1.8835e-02],\n",
       "                      [ 1.5627e-01, -1.3789e-01,  6.5454e-02,  1.5655e-01,  1.9287e-02,\n",
       "                       -1.9369e-01, -1.6396e-01, -5.4534e-02, -1.9533e-02, -1.7593e-01,\n",
       "                        1.6194e-01, -1.6811e-01,  1.9198e-01,  2.2024e-01, -9.6717e-02,\n",
       "                        1.4273e-01,  2.1315e-01,  2.9920e-02, -1.9860e-01,  2.0542e-02],\n",
       "                      [-3.2931e-03, -1.0572e-01, -1.3690e-01, -2.7332e-02, -2.0774e-03,\n",
       "                       -7.8788e-02,  1.9073e-01, -1.1732e-01,  1.1944e-01, -1.0359e-01,\n",
       "                       -3.6339e-02, -7.4552e-02,  5.4640e-02,  2.2218e-01, -9.1836e-02,\n",
       "                       -3.5816e-03,  9.5056e-02,  8.9077e-03, -6.5322e-02, -1.2120e-04],\n",
       "                      [-1.6877e-01,  5.2964e-02,  1.8614e-01, -1.2856e-01,  1.7209e-01,\n",
       "                        1.4421e-01,  1.6598e-01, -1.4221e-01,  1.9606e-01,  7.4432e-02,\n",
       "                        1.4943e-01, -1.2270e-01,  1.6490e-01, -8.2606e-02,  1.1438e-01,\n",
       "                        1.9126e-01, -3.9041e-02, -1.8318e-01,  7.6002e-02,  1.4276e-01],\n",
       "                      [ 8.6861e-02,  2.2355e-01, -1.9367e-01,  1.4286e-03, -2.1611e-01,\n",
       "                       -2.1445e-01, -3.3513e-02, -9.2587e-03, -2.1000e-01,  5.4115e-02,\n",
       "                        1.5250e-01,  1.8282e-01,  2.1722e-01, -1.7488e-01,  1.7458e-01,\n",
       "                       -4.1364e-02, -5.3315e-02,  4.5019e-02,  2.1466e-01,  9.4334e-03],\n",
       "                      [ 1.0050e-01, -6.9329e-02, -1.1442e-02,  1.9282e-01, -2.0574e-01,\n",
       "                       -8.6316e-03, -2.1197e-01,  1.5350e-01,  6.8640e-02, -2.2463e-02,\n",
       "                       -1.2720e-01, -4.7129e-02,  5.0073e-02, -1.8309e-01,  1.7512e-01,\n",
       "                        5.0897e-02, -1.1390e-01,  1.2511e-01,  2.6232e-03,  8.2565e-02],\n",
       "                      [-6.1875e-02,  1.4388e-01, -5.8590e-02, -9.9928e-03,  1.9122e-01,\n",
       "                       -1.0542e-01,  1.4550e-01,  1.9338e-01,  1.4411e-01,  1.2009e-01,\n",
       "                       -2.1203e-01, -1.6971e-01, -8.8512e-02,  1.5368e-01,  1.6920e-01,\n",
       "                        1.2762e-02,  8.6274e-02, -1.6364e-01, -8.9030e-02,  3.3485e-02],\n",
       "                      [ 1.2844e-01, -1.5532e-01, -1.9886e-01, -3.5452e-02,  1.4786e-02,\n",
       "                        5.9721e-02, -1.7453e-01,  1.9982e-01,  1.8928e-01, -1.2088e-01,\n",
       "                        1.7136e-01, -2.7363e-02,  9.3507e-02, -5.0892e-02, -1.1715e-01,\n",
       "                        2.2179e-01, -1.6264e-01, -9.4717e-02,  2.5327e-02, -1.2192e-01],\n",
       "                      [-1.6258e-02, -1.4881e-01, -1.5315e-01, -3.1142e-02, -1.6877e-01,\n",
       "                       -1.8694e-01,  2.1042e-02, -1.9639e-03,  9.2836e-02,  5.7801e-02,\n",
       "                       -1.3253e-02,  4.7403e-02,  1.7105e-01, -1.8528e-01,  1.9437e-01,\n",
       "                       -1.8547e-01,  4.6922e-03, -1.0312e-01, -1.8628e-01,  1.1654e-01],\n",
       "                      [-8.0524e-02, -3.0574e-02, -1.8943e-01, -6.6947e-02, -1.0113e-01,\n",
       "                        1.3231e-01, -1.2220e-01,  1.4655e-01,  2.0959e-01,  1.0099e-02,\n",
       "                       -1.6586e-01,  1.4843e-01, -7.4506e-03,  1.2790e-01,  9.2980e-02,\n",
       "                        2.1521e-01,  1.7845e-01, -1.0301e-02,  4.1050e-02, -2.0542e-01],\n",
       "                      [-1.2609e-01, -1.6210e-01, -4.7657e-02, -1.8580e-01, -1.4446e-01,\n",
       "                        2.6588e-02,  8.4436e-02,  1.6409e-01, -7.3092e-02, -1.7015e-01,\n",
       "                       -3.0041e-02,  4.3747e-02, -1.0909e-01, -1.6607e-01,  7.1276e-02,\n",
       "                        5.4841e-02,  1.6402e-01,  1.3860e-02, -1.6104e-02, -1.2348e-01],\n",
       "                      [ 1.3954e-01, -1.3785e-01, -2.8819e-02, -1.9256e-02, -3.3588e-02,\n",
       "                        1.8573e-01,  4.0425e-03, -1.0994e-01,  3.4550e-02, -2.1838e-01,\n",
       "                       -2.0672e-01,  1.0894e-01,  3.8112e-02,  2.0199e-01, -1.9403e-02,\n",
       "                        1.3590e-01,  6.0857e-02,  4.9084e-02, -1.8994e-01,  1.3509e-01],\n",
       "                      [ 1.2221e-01,  5.4719e-02, -3.3005e-02, -1.4725e-01,  1.4723e-01,\n",
       "                       -1.4027e-01,  4.8757e-02,  1.2916e-02, -2.0748e-01,  2.3584e-03,\n",
       "                       -2.0183e-02,  1.7798e-01,  1.0589e-01, -1.8436e-01, -2.5054e-02,\n",
       "                        1.4085e-01, -3.6055e-03,  3.4574e-02, -8.4805e-02, -9.8966e-02],\n",
       "                      [-1.3662e-01, -3.5971e-02, -2.0877e-01,  1.7761e-01,  2.1783e-01,\n",
       "                        4.1259e-02, -1.4086e-01, -4.3967e-02, -9.3567e-03, -9.6743e-02,\n",
       "                        2.2198e-01,  2.0161e-01, -2.4063e-02, -8.6718e-02,  1.8441e-01,\n",
       "                       -5.0608e-03,  1.3436e-01,  1.8260e-02, -1.1290e-01,  1.6764e-01],\n",
       "                      [ 1.1283e-01,  1.0680e-01,  1.8544e-01,  1.1274e-01, -2.0463e-01,\n",
       "                       -1.7722e-01,  2.1261e-02,  8.4822e-02, -1.6041e-01, -1.2699e-01,\n",
       "                       -1.0010e-01,  1.9514e-01, -5.3211e-02,  1.1087e-01,  9.6334e-03,\n",
       "                       -4.3798e-03,  8.2692e-02, -1.2453e-01, -1.4052e-01,  1.5914e-01],\n",
       "                      [-5.9451e-02, -1.1152e-01,  9.3960e-02, -2.0015e-01, -1.7577e-01,\n",
       "                        6.3778e-02,  2.1946e-01,  2.1303e-01,  8.7645e-02,  5.3810e-03,\n",
       "                        1.4179e-01, -1.9594e-01, -1.8803e-01, -2.0220e-01,  1.8064e-01,\n",
       "                       -2.0317e-01,  2.0835e-01,  8.1254e-02,  8.6184e-02,  8.6981e-02]])),\n",
       "             ('layer2.bias',\n",
       "              tensor([ 0.0814,  0.1611,  0.0684,  0.1677, -0.1810,  0.1262, -0.1104,  0.0483,\n",
       "                       0.2180,  0.1222, -0.2127, -0.2060, -0.0681,  0.1970,  0.1971, -0.0915,\n",
       "                      -0.1077,  0.1684, -0.1819,  0.1366, -0.1403,  0.1031,  0.0176, -0.1499,\n",
       "                      -0.1406,  0.0745,  0.0565, -0.0896, -0.1649,  0.1593])),\n",
       "             ('layer3.weight',\n",
       "              tensor([[-0.0803, -0.0326,  0.1210,  0.0931,  0.0256, -0.0215,  0.0642, -0.0281,\n",
       "                        0.0763,  0.1369,  0.1700,  0.1730,  0.0203, -0.0196, -0.0435,  0.1499,\n",
       "                       -0.0095, -0.1054, -0.0927, -0.0695, -0.1610,  0.0372,  0.1197, -0.0682,\n",
       "                        0.1483,  0.1252, -0.1800, -0.0140, -0.0082, -0.1713],\n",
       "                      [ 0.0752, -0.0144, -0.1419, -0.0428, -0.0717,  0.0934,  0.1577, -0.0396,\n",
       "                        0.0584,  0.0269, -0.0251,  0.0967, -0.0315, -0.0241, -0.0024,  0.0778,\n",
       "                        0.0748,  0.0886,  0.0593, -0.0366,  0.1627, -0.0570,  0.1301,  0.0864,\n",
       "                        0.1103,  0.1049, -0.1112,  0.0169, -0.0809,  0.0437],\n",
       "                      [ 0.1002, -0.1298,  0.1591,  0.0442,  0.0601,  0.1147, -0.0231, -0.1351,\n",
       "                        0.0758, -0.1513,  0.0763,  0.0506,  0.1490, -0.1065, -0.1467,  0.0916,\n",
       "                       -0.1616,  0.0183, -0.1094,  0.1290, -0.1164, -0.0115, -0.1245,  0.1167,\n",
       "                        0.1207, -0.0881,  0.0649, -0.1067, -0.1649, -0.0855],\n",
       "                      [ 0.0932, -0.0039, -0.1100,  0.0774, -0.1537, -0.0166,  0.1015, -0.1564,\n",
       "                       -0.0634, -0.1546, -0.1290, -0.0436,  0.1111,  0.0917,  0.0467, -0.0046,\n",
       "                       -0.1700,  0.1305, -0.0953,  0.1017, -0.1377, -0.0014,  0.0243, -0.0037,\n",
       "                        0.0681,  0.0718,  0.0544, -0.0775,  0.0003,  0.0692],\n",
       "                      [ 0.0997,  0.0683,  0.0400, -0.0777, -0.0217,  0.0367,  0.0583,  0.1017,\n",
       "                       -0.1663,  0.0673, -0.0026, -0.0234, -0.0733,  0.1216, -0.0715, -0.0858,\n",
       "                       -0.0699,  0.0898, -0.0516,  0.0952,  0.0581, -0.0564, -0.0578, -0.0396,\n",
       "                        0.1672, -0.1802, -0.0701, -0.1598,  0.1294,  0.0991]])),\n",
       "             ('layer3.bias',\n",
       "              tensor([-0.0879, -0.1647, -0.0303,  0.0997, -0.0768]))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[20, 10]\n",
      "1\n",
      "[20]\n",
      "2\n",
      "[30, 20]\n",
      "1\n",
      "[30]\n",
      "2\n",
      "[5, 30]\n",
      "1\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "for k,v in temp_model.state_dict().items():\n",
    "    print(len(v.data.shape))\n",
    "    print((list(v.shape)))\n",
    "    # print(type(k))\n",
    "    # print(f\"{k},{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in temp_model.state_dict()['layer1.weight'].shape:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "print(temp_model.state_dict()['layer1.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 10]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(temp_model.state_dict()['layer1.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, [12, 323])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [12,323]\n",
    "type(a),a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer name is layer1.weight.\n",
      "layer1.weight : [20, 10].\n",
      "layer name is layer1.bias.\n",
      "layer1.bias : [20].\n",
      "layer name is layer2.weight.\n",
      "layer2.weight : [30, 20].\n",
      "layer name is layer2.bias.\n",
      "layer2.bias : [30].\n",
      "layer name is layer3.weight.\n",
      "layer3.weight : [5, 30].\n",
      "layer name is layer3.bias.\n",
      "layer3.bias : [5].\n"
     ]
    }
   ],
   "source": [
    "model_shape = {}\n",
    "with open('my_model.txt','r') as file:\n",
    "    \n",
    "    for line in file:\n",
    "        #print(line[3:])\n",
    "        if line[1] == 's':\n",
    "            line = line[3:]\n",
    "            line = line.strip()\n",
    "            parts = line.split(':')\n",
    "            layer = parts[0]\n",
    "            print(f\"layer name is {layer}.\")\n",
    "            data = parts[1][1:-1].split(',')\n",
    "            layer_shape = []\n",
    "            for i in data:\n",
    "                # i = i.strip()\n",
    "                layer_shape.append(int(i))\n",
    "            model_shape[layer] = layer_shape\n",
    "            print(f\"{layer} : {layer_shape}.\")\n",
    "            # for i in range(len(has_value)):\n",
    "            #     has_value[i] = has_value[i][2:-2]\n",
    "            #print(f\"hash value are {has_value}.\")\n",
    "            # print(type(line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer name is layer1.weight.\n",
      "layer1.weight : 49a1bd60405f7ced4579f456bf8f5be7b8f6e96e17cd0b68c0272ba9d676919a.\n",
      "layer name is layer1.bias.\n",
      "layer1.bias : 3dabdefacea5de81bc5ab876106e967ba270c75d73b8d0a1054e890e882033db.\n",
      "layer name is layer2.weight.\n",
      "layer2.weight : c88df49ba9805e92c85c7f5ad4e460c9a18514d39373ec67f65a2f6ac889daca.\n",
      "layer name is layer2.bias.\n",
      "layer2.bias : 13d078649dd685acfcc8535154d8f247859154d52136a34a47b1fad017b57c83.\n",
      "layer name is layer3.weight.\n",
      "layer3.weight : d1398b89a7eaf80667f59340c816ea757adabdffd5fdf4dc6826e3aa923fe950.\n",
      "layer name is layer3.bias.\n",
      "layer3.bias : b2eb4ce261a762e739c6d6f773d5478363b74c4eb85f6250f9457abe67b29212.\n"
     ]
    }
   ],
   "source": [
    "model_shape = {}\n",
    "with open('my_model.txt','r') as file:\n",
    "    \n",
    "    for line in file:\n",
    "        # print(line[3:])\n",
    "        if line[1] == 'p':\n",
    "            line = line[3:]\n",
    "            line = line.strip()\n",
    "            parts = line.split(':')\n",
    "            layer = parts[0]\n",
    "            print(f\"layer name is {layer}.\")\n",
    "            data = parts[1][1:-1].split(',')\n",
    "            hash_value = []\n",
    "            for i in data:\n",
    "                hash_value.append(i.strip()[1:-1])\n",
    "                #print(i[1:-1])\n",
    "            print(f\"{layer} : {hash_value[0]}.\")\n",
    "            # model_state[layer] = hash_value\n",
    "            # for i in range(len(has_value)):\n",
    "            #     has_value[i] = has_value[i][2:-2]\n",
    "            #print(f\"hash value are {has_value}.\")\n",
    "            # print(type(line[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load(\"pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('wav2vec2.masked_spec_embed',\n",
       "              tensor([ 0.5507,  0.4647, -0.8060,  ...,  0.5205, -0.4617, -0.2501],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.0.conv.weight',\n",
       "              tensor([[[-0.0301,  0.0601, -0.0824,  ...,  0.1017, -0.0714,  0.0293]],\n",
       "              \n",
       "                      [[-0.0608,  0.2048, -0.3384,  ..., -0.2467,  0.1772, -0.0568]],\n",
       "              \n",
       "                      [[-0.0760,  0.1932, -0.2710,  ...,  0.1458, -0.0731,  0.0205]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.0909, -0.0269, -0.0012,  ...,  0.2131, -0.2964, -0.1357]],\n",
       "              \n",
       "                      [[-0.0822,  0.0698,  0.0471,  ...,  0.0196, -0.0282,  0.0362]],\n",
       "              \n",
       "                      [[ 0.0054, -0.0195,  0.0404,  ...,  0.0238,  0.0038, -0.0026]]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.0.conv.bias',\n",
       "              tensor([-0.0115, -0.0240, -0.0242, -0.0116, -0.0116, -0.0115, -0.0226, -0.0405,\n",
       "                      -0.0252, -0.0504, -0.0115, -0.0115, -0.0113, -0.0119, -0.0114, -0.0120,\n",
       "                      -0.0186, -0.0115, -0.0113, -0.0115, -0.0119, -0.0117, -0.0217, -0.0231,\n",
       "                      -0.0044, -0.0116, -0.0122, -0.0116, -0.0117, -0.0440, -0.0117, -0.0105,\n",
       "                      -0.0052, -0.0637, -0.0079, -0.0114, -0.0314, -0.0117, -0.0029, -0.0120,\n",
       "                      -0.0118, -0.0116, -0.0116, -0.0119, -0.0114, -0.0114, -0.0113, -0.0116,\n",
       "                      -0.0654, -0.0123, -0.0384, -0.0128, -0.0457, -0.0120, -0.0408, -0.0116,\n",
       "                      -0.0117, -0.0157, -0.0122, -0.0251, -0.0117, -0.0229, -0.0115, -0.0064,\n",
       "                      -0.0117, -0.0731, -0.0151, -0.0114, -0.0117, -0.0116, -0.0113, -0.0119,\n",
       "                      -0.0115, -0.0114, -0.0732, -0.0381, -0.0010, -0.0113, -0.0117, -0.0114,\n",
       "                      -0.0112, -0.0117, -0.0122, -0.0213, -0.0116, -0.0115, -0.0222, -0.0121,\n",
       "                      -0.0121, -0.0417, -0.0118, -0.0114, -0.0114, -0.0120, -0.0117, -0.0116,\n",
       "                      -0.0114, -0.0116, -0.0122, -0.0117, -0.0120, -0.0388, -0.0117, -0.0119,\n",
       "                      -0.0116, -0.0117, -0.0115, -0.0225, -0.0116, -0.0117, -0.0110, -0.0118,\n",
       "                      -0.0117, -0.0045, -0.0119, -0.0120, -0.0117, -0.0118, -0.0121, -0.0323,\n",
       "                      -0.0233, -0.0123, -0.0063, -0.0117, -0.0116, -0.0114, -0.0116, -0.0124,\n",
       "                      -0.0114, -0.0118, -0.0116, -0.0608, -0.0117, -0.0117, -0.0115, -0.0120,\n",
       "                      -0.0211, -0.0120, -0.0076, -0.0120, -0.0112, -0.0243, -0.0119, -0.0119,\n",
       "                      -0.0120, -0.0120, -0.0116, -0.0118, -0.0118, -0.0241, -0.0117, -0.0120,\n",
       "                      -0.0120, -0.0379, -0.0327, -0.0116, -0.0117, -0.0119, -0.0114, -0.0115,\n",
       "                      -0.0116, -0.0117, -0.0510, -0.0088, -0.0120, -0.0118, -0.0120, -0.0120,\n",
       "                      -0.0117, -0.0070, -0.0151, -0.0117, -0.0116, -0.0074, -0.0704, -0.0118,\n",
       "                      -0.0115, -0.0242, -0.0119, -0.0116, -0.0120, -0.0118, -0.0421, -0.0136,\n",
       "                      -0.0117, -0.0457, -0.0117, -0.0122, -0.0418, -0.0115, -0.0119, -0.0115,\n",
       "                      -0.0120, -0.0118, -0.0248, -0.0121, -0.0448, -0.0598, -0.1759, -0.0115,\n",
       "                      -0.0120, -0.0143, -0.0120, -0.0118, -0.0221, -0.0109, -0.0118, -0.0100,\n",
       "                      -0.0043, -0.0117, -0.0114, -0.0119, -0.0120, -0.0098, -0.0114, -0.0067,\n",
       "                      -0.0164, -0.0116, -0.0371, -0.0118, -0.0117, -0.0240, -0.0120, -0.0121,\n",
       "                      -0.0444, -0.0116, -0.0116, -0.0115, -0.0352, -0.0116, -0.0117, -0.0251,\n",
       "                      -0.0115, -0.0119, -0.0241, -0.0054, -0.0115, -0.0118, -0.0118, -0.0109,\n",
       "                      -0.0119, -0.0157, -0.0115, -0.0654, -0.0117, -0.0102, -0.0116, -0.0119,\n",
       "                      -0.0115, -0.0116, -0.0115, -0.0113, -0.0114, -0.0118, -0.0469, -0.0116,\n",
       "                      -0.0116, -0.0639, -0.0114, -0.0115, -0.0119, -0.0116, -0.0109, -0.0125,\n",
       "                      -0.0119, -0.0545, -0.0116, -0.0516, -0.0122, -0.0119, -0.0117, -0.0332,\n",
       "                      -0.0118, -0.0115, -0.0352, -0.0117, -0.0080, -0.0408, -0.0114, -0.0420,\n",
       "                      -0.0126, -0.0022, -0.0116, -0.0117, -0.0237, -0.0118, -0.0116, -0.0039,\n",
       "                      -0.0116, -0.0053, -0.0115, -0.0121, -0.0564, -0.0117, -0.0466, -0.0117,\n",
       "                      -0.0113, -0.0117, -0.0118, -0.0031, -0.0024, -0.0109, -0.0118, -0.0115,\n",
       "                      -0.0116, -0.0118, -0.0119, -0.0324, -0.0119, -0.0117, -0.0870, -0.0225,\n",
       "                      -0.0120, -0.0477, -0.0115, -0.0116, -0.0115, -0.0118, -0.0122, -0.0984,\n",
       "                      -0.0120, -0.0113, -0.0119, -0.0100, -0.0114, -0.0119, -0.0505, -0.0116,\n",
       "                      -0.0116, -0.0115, -0.0117, -0.0118, -0.0115, -0.0116, -0.0588, -0.0117,\n",
       "                      -0.0428, -0.0131, -0.0409, -0.0440, -0.0122, -0.0118, -0.0115, -0.0119,\n",
       "                      -0.0113, -0.0117, -0.0038, -0.0116, -0.0117, -0.0121, -0.0116, -0.0115,\n",
       "                      -0.0115, -0.0119, -0.0115, -0.0115, -0.0116, -0.0115, -0.0117, -0.0117,\n",
       "                      -0.0117, -0.0121, -0.0116, -0.0762, -0.0118, -0.0116, -0.0120, -0.0284,\n",
       "                      -0.0042, -0.0208, -0.0118, -0.0865, -0.0226, -0.0118, -0.0117, -0.0118,\n",
       "                      -0.0118, -0.0117, -0.0118, -0.0431, -0.0162, -0.0116, -0.0118, -0.0109,\n",
       "                      -0.0381, -0.0116, -0.0116, -0.0410, -0.0447, -0.0116, -0.0264, -0.0254,\n",
       "                      -0.0118, -0.0107, -0.0113, -0.0117, -0.0121, -0.0228, -0.0118, -0.0119,\n",
       "                      -0.0118, -0.0116, -0.0117, -0.0117, -0.0241, -0.0117, -0.0034, -0.0072,\n",
       "                      -0.0081, -0.0113, -0.0117, -0.0673, -0.0067, -0.0117, -0.0336, -0.0120,\n",
       "                      -0.0241, -0.0870, -0.0771, -0.0106, -0.0117, -0.0115, -0.0689, -0.0117,\n",
       "                      -0.0422, -0.0117, -0.0113, -0.0425, -0.0118, -0.0116, -0.0117, -0.0580,\n",
       "                      -0.0118, -0.0120, -0.0042, -0.0116, -0.0962, -0.0114, -0.0117, -0.0115,\n",
       "                      -0.0305, -0.0108, -0.0116, -0.0115, -0.0115, -0.0118, -0.0119, -0.0118,\n",
       "                      -0.0439, -0.0115, -0.0122, -0.0114, -0.0117, -0.0410, -0.0095, -0.0116,\n",
       "                      -0.0113, -0.0121, -0.0122, -0.0068, -0.0178, -0.0387, -0.0117, -0.0116,\n",
       "                      -0.0129, -0.0117, -0.0119, -0.0090, -0.0118, -0.0116, -0.0237, -0.0118,\n",
       "                      -0.0116, -0.0121, -0.0117, -0.0276, -0.0238, -0.0111, -0.0116, -0.0115,\n",
       "                      -0.0117, -0.0117, -0.0126, -0.0116, -0.0110, -0.0115, -0.0115, -0.0119,\n",
       "                      -0.0113, -0.0378, -0.0049, -0.0113, -0.0081, -0.0120, -0.0118, -0.0377,\n",
       "                      -0.0117, -0.0116, -0.0118, -0.0120, -0.0113, -0.0506, -0.0116, -0.0115,\n",
       "                      -0.0118, -0.0115, -0.0118, -0.0118, -0.0080, -0.0046, -0.0119, -0.0119],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight',\n",
       "              tensor([-4.4608e-04,  1.7529e+00,  2.0762e+00,  4.3259e-03,  1.8101e-03,\n",
       "                      -1.5535e-03,  9.8730e-01,  8.2861e-01,  6.6992e-01,  1.1426e+00,\n",
       "                       1.7433e-03,  4.0855e-03, -2.0230e-04,  6.3934e-03,  4.5280e-03,\n",
       "                      -8.2350e-04,  4.3677e-01,  6.6376e-03,  2.5711e-03,  2.8343e-03,\n",
       "                       4.0321e-03,  4.5929e-03,  4.9390e-01,  1.5654e+00,  3.6963e-01,\n",
       "                       7.6675e-04,  1.8396e-03,  5.2185e-03,  1.3039e-02,  1.0625e+00,\n",
       "                       1.3103e-03,  3.7134e-01,  4.3604e-01,  8.9990e-01,  4.0063e-01,\n",
       "                       4.2229e-03,  1.2129e+00,  8.8120e-03,  3.7720e-01,  2.3384e-03,\n",
       "                       8.1024e-03, -1.0242e-03,  4.4060e-03, -5.3167e-05,  2.0599e-03,\n",
       "                       2.4147e-03,  3.1357e-03,  5.1422e-03,  7.7441e-01, -2.0962e-03,\n",
       "                       7.6807e-01,  2.9316e-03,  8.8672e-01,  3.6221e-03,  1.0312e+00,\n",
       "                       9.0179e-03,  1.9178e-03,  4.9438e-01, -2.2340e-04,  1.4346e+00,\n",
       "                       2.5482e-03,  9.8975e-01,  5.9843e-04,  4.0820e-01,  7.4539e-03,\n",
       "                       6.6260e-01,  4.3823e-01,  6.1321e-04,  4.6301e-04,  2.9106e-03,\n",
       "                       2.1114e-03,  1.5404e-02,  9.4299e-03,  8.8453e-04,  7.7002e-01,\n",
       "                       9.9463e-01,  3.3398e-01,  1.7920e-03,  1.9348e-04,  1.8044e-03,\n",
       "                       3.3340e-03,  4.4174e-03,  3.3722e-03,  1.4922e+00,  1.4572e-03,\n",
       "                       5.1422e-03,  2.7129e+00,  5.6028e-04,  1.1292e-03,  9.7900e-01,\n",
       "                       3.5534e-03,  3.5419e-03,  4.2648e-03,  1.5783e-03,  5.3120e-04,\n",
       "                       1.3992e-02,  1.0315e-02,  7.8487e-04, -8.6355e-04,  1.3702e-02,\n",
       "                       1.4889e-04,  1.1738e+00,  8.2397e-03,  7.8058e-04,  5.3177e-03,\n",
       "                      -5.4455e-04,  1.7958e-03,  1.2939e+00,  8.0338e-03,  5.5122e-03,\n",
       "                      -4.8804e-04,  1.0166e-03,  1.8654e-03,  3.4546e-01,  6.0616e-03,\n",
       "                       1.4508e-04,  5.2299e-03,  4.9095e-03, -6.7329e-04,  1.2041e+00,\n",
       "                       1.6396e+00, -2.1458e-04,  4.2676e-01,  1.9684e-03,  1.0147e-02,\n",
       "                       3.5229e-03, -9.0790e-04,  1.3566e-04,  1.0315e-02,  3.3455e-03,\n",
       "                       3.8700e-03,  5.0928e-01, -9.1400e-03,  2.2054e-04,  1.7441e-02,\n",
       "                       1.5850e-03,  1.4424e+00, -1.7014e-03,  2.9224e-01, -3.6573e-04,\n",
       "                       2.4204e-03,  6.0596e-01,  4.3030e-03,  1.3483e-04, -1.1120e-03,\n",
       "                       7.4625e-04,  1.5535e-03,  1.5831e-03,  1.2894e-03,  7.6074e-01,\n",
       "                       1.7929e-03,  3.1490e-03,  4.3106e-03,  1.0098e+00,  5.2490e-01,\n",
       "                       3.4924e-03,  2.4376e-03, -7.0095e-04,  1.0729e-03,  7.3612e-05,\n",
       "                       1.3412e-02,  3.2558e-03,  8.5645e-01,  2.9932e-01,  4.1914e-04,\n",
       "                       5.3482e-03,  3.0041e-03,  2.2564e-03,  4.9973e-03,  4.2432e-01,\n",
       "                       8.9600e-02,  3.9177e-03,  2.9812e-03,  3.5645e-01,  7.5830e-01,\n",
       "                       7.3013e-03,  9.5901e-03,  1.7451e+00,  5.7373e-03,  2.3556e-03,\n",
       "                       1.5211e-03,  1.0672e-03,  9.3652e-01,  3.7500e-01,  5.8508e-04,\n",
       "                       1.1553e+00,  3.7994e-03, -4.1461e-04,  1.1387e+00,  1.0612e-02,\n",
       "                       2.1362e-03,  2.1000e-03,  4.5729e-04,  5.3062e-03,  9.6484e-01,\n",
       "                       1.9627e-03,  1.0410e+00,  9.5068e-01,  6.2451e-01,  1.0815e-03,\n",
       "                       5.4884e-04,  2.5220e-01, -3.7360e-04,  5.9242e-03,  1.8242e+00,\n",
       "                       2.4462e-04,  3.4771e-03,  4.3042e-01,  4.1162e-01,  1.4847e-02,\n",
       "                       1.3952e-03,  4.4899e-03, -2.6512e-04,  2.8711e-01,  1.1559e-03,\n",
       "                       2.0850e-01,  2.6489e-01,  8.2779e-03,  1.0244e+00,  7.9727e-03,\n",
       "                       7.1466e-05,  9.0771e-01, -8.8120e-04,  2.1324e-03,  9.9414e-01,\n",
       "                       1.0414e-02,  3.0351e-04,  4.4098e-03,  8.4717e-01,  7.5042e-05,\n",
       "                       8.9798e-03,  1.4463e+00,  7.7133e-03,  3.3817e-03,  1.4248e+00,\n",
       "                       3.3765e-01,  3.3627e-03,  6.8359e-03,  8.0061e-04,  1.2197e-03,\n",
       "                       2.8312e-05,  3.7549e-01,  2.1496e-03,  7.3193e-01,  3.6087e-03,\n",
       "                       3.9062e-01,  4.5443e-04,  8.3685e-05,  5.5504e-04,  4.5509e-03,\n",
       "                       7.1220e-03,  1.0939e-03,  5.4359e-03,  3.6449e-03,  6.4209e-01,\n",
       "                       1.1168e-03,  1.5282e-02,  3.3887e-01,  2.9907e-03,  3.8700e-03,\n",
       "                       4.9210e-03,  1.2712e-03,  2.2812e-03, -2.1160e-05,  1.1616e-03,\n",
       "                       9.3945e-01,  1.0628e-02,  1.0420e+00,  6.4850e-04,  1.0214e-03,\n",
       "                       6.4774e-03,  6.1230e-01,  2.1248e-03,  4.0550e-03,  1.1787e+00,\n",
       "                       7.0038e-03,  4.3164e-01,  1.1016e+00,  8.6670e-03,  1.1074e+00,\n",
       "                       1.5440e-03,  4.0210e-01,  2.4567e-03,  9.5062e-03,  2.1914e+00,\n",
       "                       7.8869e-04,  2.2373e-03,  4.1479e-01,  5.6686e-03,  3.4131e-01,\n",
       "                       1.0557e-03,  2.5883e-03,  1.0059e+00,  7.5989e-03,  9.6973e-01,\n",
       "                       2.1219e-05,  1.5602e-03,  1.4000e-02,  4.8752e-03,  4.1504e-01,\n",
       "                       3.4131e-01,  9.7656e-04,  7.0343e-03,  9.7427e-03,  1.0193e-02,\n",
       "                       4.2496e-03,  8.6212e-03,  6.2207e-01,  7.0190e-03,  1.7853e-03,\n",
       "                       5.7861e-01,  1.0430e+00,  2.2278e-03,  1.1025e+00,  2.2316e-03,\n",
       "                       6.3591e-03,  5.0697e-03,  7.3195e-04,  5.2185e-03,  4.6826e-01,\n",
       "                       2.8667e-03,  3.1986e-03,  1.1263e-03,  3.2373e-01,  2.3594e-03,\n",
       "                       5.3825e-03,  1.1582e+00,  1.6769e-02,  7.0953e-03,  1.8940e-03,\n",
       "                       9.5654e-04,  4.5776e-03,  6.8817e-03,  1.6220e-02,  5.1318e-01,\n",
       "                       5.6114e-03,  8.6426e-01,  9.9219e-01,  9.3506e-01,  9.6875e-01,\n",
       "                       4.8375e-04,  2.0332e-03,  2.2373e-03,  1.5869e-03,  2.4462e-04,\n",
       "                       1.8711e-03,  3.6670e-01,  7.9918e-04,  7.4768e-04,  1.3695e-03,\n",
       "                       3.4399e-01,  1.3256e-03,  4.4670e-03,  4.5052e-03,  1.7214e-03,\n",
       "                       3.4218e-03,  2.9812e-03,  3.0155e-03,  5.4836e-04, -3.4833e-04,\n",
       "                       7.8278e-03,  1.9608e-03,  5.2528e-03,  4.7559e-01,  3.3998e-04,\n",
       "                       1.8738e-02,  1.7405e-03,  5.6738e-01,  3.5767e-01,  1.7988e+00,\n",
       "                      -7.5161e-05,  5.9180e-01,  1.8184e+00,  9.7351e-03,  8.7662e-03,\n",
       "                       4.2343e-03,  4.5395e-03,  2.5940e-03,  8.0719e-03,  1.0605e+00,\n",
       "                       4.8340e-01,  8.2493e-04, -9.0837e-05,  1.4997e-04,  9.9561e-01,\n",
       "                       9.8114e-03,  1.8358e-03,  8.6426e-01,  1.1445e+00,  2.8400e-03,\n",
       "                       8.0078e-01,  6.5771e-01, -2.8610e-04,  2.2351e-01, -3.4642e-04,\n",
       "                      -4.3917e-04,  1.5268e-03,  1.8105e+00,  3.7098e-03,  7.4959e-04,\n",
       "                       1.1711e-03,  9.6560e-04,  5.8861e-03,  1.7142e-04,  7.9980e-01,\n",
       "                       3.7479e-04,  2.9858e-01,  4.2212e-01,  2.8931e-01,  4.5815e-03,\n",
       "                       3.1052e-03,  8.5742e-01,  3.5132e-01,  3.7155e-03,  9.5557e-01,\n",
       "                       2.2697e-03,  8.2910e-01,  7.6807e-01,  7.2266e-01,  6.1913e-03,\n",
       "                       4.2343e-03,  7.3493e-05,  5.4785e-01,  5.2404e-04,  9.2139e-01,\n",
       "                       4.1733e-03, -5.1689e-04,  1.0273e+00,  5.0697e-03,  8.9264e-03,\n",
       "                       3.0251e-03,  9.3848e-01,  6.6996e-04,  6.3956e-05,  3.6035e-01,\n",
       "                      -3.0470e-04,  4.8926e-01, -7.0953e-04,  3.9825e-03,  5.1212e-04,\n",
       "                       1.0713e+00,  3.8208e-01,  8.9931e-04,  1.3016e-02,  3.2215e-03,\n",
       "                       3.2082e-03,  9.0456e-04,  6.8207e-03,  1.2061e+00,  1.1597e-03,\n",
       "                       1.2016e-03,  7.5388e-04,  5.1270e-03,  9.1064e-01,  7.5244e-01,\n",
       "                       2.6932e-03,  5.4312e-04, -5.3740e-04,  2.1591e-03,  3.4106e-01,\n",
       "                       4.9048e-01,  9.0918e-01,  3.3150e-03,  4.6349e-03, -1.0002e-04,\n",
       "                       2.5177e-03, -2.2078e-04,  3.3936e-01, -3.6621e-04,  1.5114e-02,\n",
       "                       3.3105e-01,  3.1204e-03,  1.1568e-03,  9.7132e-04,  9.9869e-03,\n",
       "                       3.8696e-01,  1.3369e+00, -4.7565e-05,  1.1162e-02,  1.6623e-03,\n",
       "                       3.4237e-03,  1.5450e-03, -8.9931e-04,  1.3313e-03,  5.4741e-04,\n",
       "                       1.2474e-02,  2.5902e-03, -2.8014e-04,  4.7565e-04,  5.9082e-01,\n",
       "                       4.2334e-01,  2.1706e-03,  2.6318e-01,  3.9148e-04,  4.1604e-04,\n",
       "                       1.1338e+00,  3.9940e-03,  9.2239e-03,  8.6823e-03,  2.4652e-04,\n",
       "                       1.3771e-03,  9.1797e-01,  8.1100e-03,  9.9659e-04,  9.1553e-03,\n",
       "                       1.1091e-03,  3.2120e-03,  2.3174e-03,  3.4741e-01,  2.6318e-01,\n",
       "                      -1.0481e-03,  4.5776e-03], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias',\n",
       "              tensor([-1.6251e-03,  6.4844e-01,  8.1348e-01,  1.1545e-04, -8.6606e-05,\n",
       "                       5.8031e-04,  3.0371e-01, -8.8916e-01,  3.6328e-01, -5.7178e-01,\n",
       "                      -1.9522e-03, -3.3784e-04, -1.1816e-03, -2.4548e-03, -9.8419e-04,\n",
       "                      -2.1095e-03, -4.3068e-03, -1.4734e-03,  1.2636e-03, -9.4128e-04,\n",
       "                       5.3406e-04, -3.4504e-03,  1.3574e-01,  5.4297e-01, -3.2178e-01,\n",
       "                      -2.0638e-03, -2.4366e-04, -1.0223e-03, -5.8784e-03, -1.2305e+00,\n",
       "                       1.0900e-03, -1.6003e-01, -3.1909e-01,  5.2344e-01, -2.3389e-01,\n",
       "                      -1.7929e-03, -1.0967e+00, -2.3918e-03, -3.2397e-01, -1.0271e-03,\n",
       "                      -3.1338e-03, -1.6718e-03, -1.3962e-03, -8.5306e-04, -1.7512e-04,\n",
       "                      -2.7847e-03, -1.6270e-03, -1.0185e-03,  6.2939e-01, -1.0157e-03,\n",
       "                      -1.2412e+00, -1.3475e-03, -1.0957e+00,  2.1152e-03, -1.1709e+00,\n",
       "                      -4.1656e-03, -1.3542e-03, -8.3496e-02, -7.5340e-04, -1.0605e+00,\n",
       "                      -1.4043e-04,  3.1079e-01, -9.7084e-04, -2.9443e-01, -2.2926e-03,\n",
       "                       1.0425e-01, -8.5815e-02, -8.7547e-04, -1.5459e-03, -3.0766e-03,\n",
       "                       1.9813e-04, -6.0616e-03, -2.0752e-03, -1.0309e-03,  7.0496e-02,\n",
       "                      -1.0293e+00, -3.1909e-01, -1.8358e-05, -4.9067e-04, -2.2850e-03,\n",
       "                      -1.3819e-03, -1.4153e-03,  2.5415e-04,  3.0664e-01, -8.4639e-06,\n",
       "                      -2.4681e-03,  9.4922e-01,  2.0444e-04,  1.0481e-03, -8.3350e-01,\n",
       "                       3.8695e-04, -1.4853e-04,  1.3294e-03,  2.2209e-04, -9.6464e-04,\n",
       "                      -3.7022e-03, -1.5621e-03, -1.1501e-03, -1.1263e-03, -6.0692e-03,\n",
       "                      -4.2844e-04, -7.8369e-01, -4.1351e-03, -1.0433e-03, -1.0052e-03,\n",
       "                      -1.6222e-03,  6.7854e-04,  3.4961e-01, -2.1973e-03, -2.8915e-03,\n",
       "                      -1.0910e-03, -1.4162e-03, -9.2125e-04, -2.5171e-01, -9.2173e-04,\n",
       "                      -1.1044e-03, -1.8749e-03, -4.2992e-03, -1.5821e-03, -1.0469e+00,\n",
       "                       6.0400e-01,  7.9453e-05, -2.8247e-01, -2.3308e-03, -5.0011e-03,\n",
       "                       5.9843e-04, -1.8072e-03,  2.8801e-04, -1.6966e-03,  1.0614e-03,\n",
       "                      -2.5101e-03,  4.3018e-01,  3.5553e-03,  9.1136e-05, -6.4812e-03,\n",
       "                      -4.2844e-04,  2.9492e-01, -1.7729e-03, -1.8555e-01, -2.8801e-04,\n",
       "                      -9.3079e-04,  3.9087e-01, -2.5330e-03, -2.5201e-04, -1.5717e-03,\n",
       "                       1.5998e-04, -1.0014e-03,  2.6226e-04, -5.5981e-04, -1.1533e+00,\n",
       "                       7.4148e-04, -2.0027e-03, -1.4505e-03, -9.4824e-01,  4.7729e-01,\n",
       "                      -2.5425e-03, -8.6689e-04, -8.1444e-04, -7.2908e-04, -2.0924e-03,\n",
       "                      -3.9368e-03, -1.3075e-03, -6.3184e-01, -1.5491e-01, -1.9627e-03,\n",
       "                      -1.6603e-03, -7.7963e-04, -1.3580e-03, -6.2847e-04, -2.9370e-01,\n",
       "                      -1.2535e-02, -1.8415e-03,  1.7653e-03, -2.3523e-01, -2.3022e-01,\n",
       "                      -2.9087e-03, -4.9095e-03,  6.5869e-01, -1.7414e-03, -2.1591e-03,\n",
       "                      -1.2445e-03, -8.8596e-04, -1.2412e+00, -9.9915e-02, -3.7098e-04,\n",
       "                      -4.9512e-01, -1.6088e-03, -8.4734e-04, -1.0186e+00, -3.4771e-03,\n",
       "                      -1.4277e-03,  4.7708e-04, -1.1148e-03, -2.2793e-03,  3.9429e-01,\n",
       "                      -4.4417e-04, -1.1240e+00,  3.3508e-02,  4.7095e-01, -1.4219e-03,\n",
       "                      -1.8091e-03, -6.2927e-02, -1.5087e-03, -2.9812e-03,  4.1992e-01,\n",
       "                      -2.2089e-04,  1.5488e-03, -2.3792e-01, -3.4009e-01, -5.9738e-03,\n",
       "                      -4.4465e-05, -3.1376e-03, -5.6124e-04, -1.4014e-01, -1.6947e-03,\n",
       "                      -1.4246e-01, -4.3488e-02, -3.4466e-03, -9.8340e-01, -4.0321e-03,\n",
       "                      -8.6689e-04,  3.1274e-01, -5.1022e-04,  1.2283e-03, -1.0117e+00,\n",
       "                      -2.6722e-03,  3.8052e-04, -2.5311e-03, -1.2109e+00, -2.8276e-04,\n",
       "                      -2.7828e-03, -1.2305e+00, -2.0332e-03, -1.3189e-03,  6.0693e-01,\n",
       "                      -2.7588e-01, -1.7052e-03, -3.4351e-03, -2.5673e-03, -2.7370e-03,\n",
       "                      -2.7390e-03, -3.9185e-02,  3.6669e-04,  5.4980e-01, -1.6384e-03,\n",
       "                      -1.8762e-01, -1.0204e-03, -6.9618e-04, -3.1567e-04,  9.9659e-04,\n",
       "                      -2.5330e-03, -2.5120e-03, -2.7943e-03, -1.4048e-03, -1.3457e+00,\n",
       "                       1.7762e-04, -6.9618e-03,  2.1301e-01, -1.0242e-03, -8.9550e-04,\n",
       "                      -3.5214e-04, -1.1978e-03, -2.0580e-03, -1.5984e-03, -1.4210e-03,\n",
       "                      -6.0742e-01, -3.8681e-03, -2.8467e-01, -1.4181e-03,  1.2760e-03,\n",
       "                      -1.2932e-03,  5.8789e-01, -1.4324e-03, -2.3022e-03, -1.1699e+00,\n",
       "                      -1.1005e-03, -2.5415e-01, -1.1592e+00, -1.6661e-03, -8.9600e-01,\n",
       "                       8.6212e-04, -3.5107e-01, -1.3292e-04, -2.7142e-03,  7.8223e-01,\n",
       "                      -1.0719e-03, -1.2159e-03, -3.6719e-01, -2.8515e-04, -2.4829e-01,\n",
       "                      -1.1402e-04, -8.6164e-04,  3.0060e-02, -1.9217e-03, -7.6953e-01,\n",
       "                      -1.8015e-03, -1.2388e-03, -5.3101e-03, -1.6546e-03, -3.8965e-01,\n",
       "                      -3.1470e-01, -2.7347e-04, -3.9673e-03, -2.2755e-03, -4.3259e-03,\n",
       "                      -2.9202e-03, -4.1237e-03,  5.5322e-01, -2.5101e-03, -7.0477e-04,\n",
       "                       3.0957e-01,  2.6221e-01,  1.3173e-05, -4.2700e-01,  6.9237e-04,\n",
       "                      -3.8338e-03, -2.5196e-03, -7.0953e-04, -1.7366e-03,  3.4009e-01,\n",
       "                      -1.5182e-03, -2.6188e-03, -2.0957e-04, -1.5173e-01,  8.6737e-04,\n",
       "                      -3.1090e-03, -2.4890e-01, -7.1220e-03, -1.6890e-03,  2.5511e-04,\n",
       "                       1.5998e-04, -2.7866e-03, -1.1120e-03, -7.9422e-03,  4.1718e-02,\n",
       "                      -2.0428e-03, -8.9648e-01, -1.1133e+00, -1.1660e+00, -1.1162e+00,\n",
       "                      -3.3307e-04, -1.0128e-03, -7.1478e-04,  8.5306e-04, -6.7377e-04,\n",
       "                      -5.0545e-05, -2.9517e-01,  4.4799e-04, -1.2541e-03, -1.8587e-03,\n",
       "                      -1.3489e-01, -1.6439e-04, -1.6413e-03, -2.7142e-03, -8.9741e-04,\n",
       "                      -1.3995e-04, -2.2335e-03, -2.0714e-03, -4.5919e-04, -1.7595e-03,\n",
       "                      -3.4046e-03,  9.4271e-04, -3.2120e-03,  5.9521e-01, -6.3944e-04,\n",
       "                      -6.1569e-03,  4.5252e-04,  3.7866e-01, -3.0176e-01,  3.2056e-01,\n",
       "                      -9.1076e-04,  3.2593e-01,  5.0000e-01, -5.2071e-03, -2.2125e-03,\n",
       "                      -8.4734e-04, -2.9640e-03, -2.6798e-04, -2.3842e-03, -7.3340e-01,\n",
       "                      -1.0223e-01,  1.0812e-04, -2.2430e-03,  8.6641e-04, -1.0400e+00,\n",
       "                      -5.3482e-03,  1.2779e-03, -1.2393e+00, -8.6523e-01,  9.7466e-04,\n",
       "                      -1.1260e+00,  3.7842e-01, -1.1654e-03, -8.7585e-02, -1.0926e-04,\n",
       "                      -1.8682e-03, -2.7704e-04,  6.5967e-01, -1.8854e-03, -1.5965e-03,\n",
       "                      -1.1790e-04, -4.2319e-06, -2.5158e-03, -2.4796e-04,  3.3984e-01,\n",
       "                       5.4169e-04, -2.3584e-01, -2.3657e-01, -1.6467e-01, -2.2850e-03,\n",
       "                      -2.6703e-03,  2.8760e-01, -2.3633e-01, -1.8072e-03, -1.0977e+00,\n",
       "                      -9.9182e-05,  3.4131e-01,  4.2603e-01,  5.8533e-02, -4.1885e-03,\n",
       "                      -6.1369e-04, -4.0550e-03,  3.9429e-01, -1.9109e-04, -1.3154e+00,\n",
       "                       8.7118e-04, -1.5717e-03, -1.2490e+00, -3.1567e-03, -3.0079e-03,\n",
       "                      -8.6880e-04, -6.6016e-01,  7.8201e-05, -3.4618e-04, -2.8101e-01,\n",
       "                      -6.1131e-04,  3.3105e-01, -1.1253e-03, -3.9911e-04, -1.3218e-03,\n",
       "                      -1.3447e+00, -1.5808e-01, -6.3801e-04, -4.3716e-03,  1.3180e-03,\n",
       "                      -9.9564e-04, -4.2558e-04, -5.7840e-04, -6.4600e-01, -6.2609e-04,\n",
       "                      -7.1573e-04, -1.1367e-04, -2.8229e-03, -1.2666e+00, -6.9287e-01,\n",
       "                       5.5742e-04, -8.8882e-04, -7.8201e-04, -2.2793e-03, -2.2461e-01,\n",
       "                       1.5833e-01, -1.3320e+00, -1.1978e-03, -2.8152e-03, -2.7037e-04,\n",
       "                      -1.3852e-04, -1.8339e-03, -1.8433e-01, -1.1005e-03, -6.0539e-03,\n",
       "                       1.2018e-01, -2.1076e-03, -7.5912e-04,  9.7752e-04, -2.8782e-03,\n",
       "                       2.2583e-01,  4.9902e-01, -9.5558e-04, -3.1986e-03, -5.5885e-04,\n",
       "                      -1.2093e-03, -4.3511e-04, -1.2360e-03, -7.0095e-05, -5.5730e-05,\n",
       "                      -4.2343e-03, -2.3785e-03, -2.3785e-03, -2.9135e-04, -1.1777e+00,\n",
       "                      -3.7378e-01, -4.3869e-05, -1.5198e-01, -7.4100e-04, -1.6336e-03,\n",
       "                      -9.7656e-01, -3.3779e-03, -3.0785e-03, -2.6054e-03, -6.9189e-04,\n",
       "                      -2.5330e-03, -5.8643e-01, -1.3618e-03, -3.2377e-04, -3.2005e-03,\n",
       "                       1.5879e-04,  1.0662e-03, -3.6645e-04, -2.3120e-01, -2.1167e-01,\n",
       "                      -7.0858e-04, -2.4242e-03], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.1.conv.weight',\n",
       "              tensor([[[ 9.3765e-03,  2.3621e-02, -8.5373e-03],\n",
       "                       [ 1.3745e-01,  1.7627e-01, -3.8770e-01],\n",
       "                       [-2.2827e-01,  8.8574e-01, -8.3057e-01],\n",
       "                       ...,\n",
       "                       [ 6.6284e-02,  1.1725e-01, -5.8929e-02],\n",
       "                       [ 7.7744e-03,  4.9591e-03,  2.3899e-03],\n",
       "                       [ 1.0345e-02,  2.6112e-03, -2.3537e-03]],\n",
       "              \n",
       "                      [[ 2.6107e-04,  1.8396e-03,  3.0270e-03],\n",
       "                       [ 2.8290e-02,  1.7419e-01, -2.3941e-02],\n",
       "                       [ 3.3630e-02, -6.1691e-05,  1.0974e-01],\n",
       "                       ...,\n",
       "                       [ 8.6304e-02, -9.0881e-02,  1.2817e-01],\n",
       "                       [-3.7193e-03, -3.5858e-03, -8.3876e-04],\n",
       "                       [ 2.5916e-04,  3.2616e-03,  1.3962e-03]],\n",
       "              \n",
       "                      [[ 1.3603e-02,  5.0888e-03,  1.1997e-03],\n",
       "                       [ 5.4871e-02,  5.1361e-02, -8.4152e-03],\n",
       "                       [ 6.4270e-02,  5.2887e-02,  4.8553e-02],\n",
       "                       ...,\n",
       "                       [ 1.0400e-01,  1.3867e-01, -1.5833e-01],\n",
       "                       [ 1.7456e-02,  9.1019e-03,  6.9389e-03],\n",
       "                       [ 1.6006e-02, -2.0809e-03, -3.7689e-02]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 1.4664e-02,  1.1581e-02,  1.4572e-02],\n",
       "                       [ 1.9150e-02,  1.5063e-01,  7.0312e-02],\n",
       "                       [ 4.3511e-06,  1.0004e-01,  5.0873e-02],\n",
       "                       ...,\n",
       "                       [ 3.5248e-02, -8.6914e-02,  1.0406e-01],\n",
       "                       [ 2.6646e-03,  1.0538e-03,  5.8784e-03],\n",
       "                       [ 6.3705e-03, -7.4692e-03, -1.4015e-02]],\n",
       "              \n",
       "                      [[-3.0518e-03, -1.5306e-03,  2.9969e-04],\n",
       "                       [ 5.4382e-02,  7.1716e-02,  3.8025e-02],\n",
       "                       [ 6.6772e-02,  5.9143e-02,  4.4189e-02],\n",
       "                       ...,\n",
       "                       [-2.2839e-01,  1.4917e-01,  2.2812e-02],\n",
       "                       [ 9.0332e-03,  2.8419e-03,  6.0158e-03],\n",
       "                       [-2.5192e-02, -9.2149e-05,  2.3865e-02]],\n",
       "              \n",
       "                      [[ 2.8061e-02,  2.1545e-02,  1.6296e-02],\n",
       "                       [ 7.1716e-02,  7.9407e-02,  4.4891e-02],\n",
       "                       [ 7.8186e-02,  5.4352e-02,  6.3965e-02],\n",
       "                       ...,\n",
       "                       [-1.3000e-01,  2.9932e-01, -3.2837e-01],\n",
       "                       [-2.0905e-02, -2.8229e-02, -2.5208e-02],\n",
       "                       [ 3.2898e-02,  2.5116e-02,  8.9550e-04]]], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.1.conv.bias',\n",
       "              tensor([ 1.2164e-01,  2.0618e-01,  1.1530e-01,  6.6833e-02,  7.7271e-02,\n",
       "                       2.3083e-01,  1.4746e-01,  2.5439e-01,  1.3855e-01,  2.1375e-01,\n",
       "                       1.1102e-01,  3.1567e-03,  1.3721e-01,  2.5708e-01,  1.3757e-01,\n",
       "                      -7.5806e-02,  7.1899e-02,  3.3765e-01,  2.0508e-01,  3.8757e-02,\n",
       "                       2.3633e-01,  1.5308e-01,  2.1240e-01,  1.9275e-01,  1.3367e-01,\n",
       "                      -3.9520e-02,  1.5857e-01,  1.2646e-01,  1.7310e-01,  3.4253e-01,\n",
       "                       2.6514e-01,  2.1570e-01,  1.6223e-01,  2.2388e-01, -1.0992e-01,\n",
       "                       1.1047e-01,  1.6003e-01,  1.6370e-01,  2.9053e-01,  2.3083e-01,\n",
       "                       6.4392e-02,  1.0590e-01,  2.9126e-01,  1.1053e-01,  1.6821e-01,\n",
       "                       2.6660e-01,  1.9885e-01,  1.9312e-01,  1.8823e-01, -1.6333e-01,\n",
       "                       2.5635e-01,  1.1603e-01,  1.5210e-01,  1.7338e-03,  9.9304e-02,\n",
       "                       1.1090e-01,  1.3208e-01,  2.4451e-01,  1.6614e-01, -5.6793e-02,\n",
       "                       1.8835e-01,  2.6581e-02,  1.8591e-01,  1.9617e-01, -1.2146e-01,\n",
       "                       1.7017e-01, -1.1047e-01,  2.8003e-01,  1.2231e-01, -7.4585e-02,\n",
       "                      -5.4779e-02,  1.7407e-01,  2.4121e-01,  2.1826e-01,  2.5659e-01,\n",
       "                       2.3950e-01,  2.6270e-01,  3.3398e-01,  2.3242e-01,  2.7710e-01,\n",
       "                       2.4036e-01, -2.5781e-01,  1.6101e-01,  1.5063e-01,  2.5806e-01,\n",
       "                       2.5195e-01,  1.5022e-02, -3.6072e-02, -3.3276e-01,  1.6968e-01,\n",
       "                      -7.8552e-02,  1.3098e-01,  1.0217e-01,  2.4683e-01,  1.7505e-01,\n",
       "                       2.0837e-01,  1.0651e-01,  1.5576e-01,  1.7993e-01,  1.3831e-01,\n",
       "                      -2.6047e-02,  3.2617e-01,  2.4304e-01,  1.9690e-01,  2.5171e-01,\n",
       "                       1.7151e-01,  2.2449e-01,  1.8188e-01,  1.0651e-01,  3.7201e-02,\n",
       "                       2.4780e-01,  4.7729e-02,  6.7261e-02,  1.6296e-01,  1.2842e-01,\n",
       "                       2.1228e-01,  7.5134e-02,  1.2488e-01,  1.9507e-01,  4.1772e-01,\n",
       "                       1.8799e-01,  2.5513e-01,  1.8896e-01,  1.5540e-01,  1.5430e-01,\n",
       "                       1.6040e-01,  8.2214e-02,  3.3008e-01,  1.5295e-01, -1.1383e-02,\n",
       "                       1.7908e-01,  1.4124e-01,  2.0166e-01,  2.6367e-01,  2.0581e-01,\n",
       "                       1.1407e-01,  1.2512e-01,  1.4502e-01,  1.2756e-01,  2.0630e-01,\n",
       "                       1.7249e-01,  7.6172e-02,  2.0764e-01,  2.7832e-01,  1.5906e-01,\n",
       "                       1.5955e-01,  2.0972e-01,  1.1102e-01,  1.5601e-01,  2.2064e-02,\n",
       "                       1.1554e-01,  2.5366e-01,  2.3450e-01, -4.5471e-02,  1.3281e-01,\n",
       "                       1.3782e-01,  5.1208e-02,  2.3914e-01,  2.1948e-01,  3.2886e-01,\n",
       "                       1.8274e-01,  2.6855e-01,  1.8628e-01,  2.0215e-01,  2.6147e-01,\n",
       "                       1.2354e-01,  1.3806e-01,  1.6998e-02,  2.6318e-01, -3.0176e-01,\n",
       "                       2.3596e-01,  1.5161e-01,  1.8408e-01,  1.4258e-01,  1.2903e-01,\n",
       "                       1.1945e-01,  2.8381e-03, -2.5513e-01,  1.9214e-01,  1.9824e-01,\n",
       "                       1.9116e-01,  1.4392e-01,  1.4636e-01,  2.1204e-01,  2.4219e-01,\n",
       "                       6.6772e-02,  2.3120e-01,  2.5781e-01,  2.0837e-01,  2.5000e-01,\n",
       "                       1.3550e-01,  6.5857e-02,  1.5405e-01,  2.4353e-01,  1.1438e-01,\n",
       "                       1.2634e-01,  1.0529e-01,  1.4380e-01,  2.3962e-01,  2.2205e-01,\n",
       "                      -7.1960e-02,  2.8687e-01,  2.3486e-01,  2.0776e-01, -6.5308e-02,\n",
       "                       2.3413e-01,  6.3782e-02,  1.9067e-01,  1.9128e-01,  2.3071e-01,\n",
       "                       2.0239e-01, -3.2749e-03,  3.6084e-01,  2.6099e-01,  2.0813e-01,\n",
       "                       2.5317e-01,  9.7229e-02,  2.0764e-01,  1.6992e-01,  1.1176e-01,\n",
       "                       1.7249e-01,  3.6743e-01,  1.3977e-01,  2.0984e-01,  1.4392e-01,\n",
       "                       2.7417e-01,  7.0374e-02, -2.1899e-01,  2.3425e-01,  2.0251e-01,\n",
       "                       8.6670e-02,  2.4683e-01,  1.7651e-01,  1.2352e-02,  1.8591e-01,\n",
       "                       2.7490e-01,  1.9995e-01,  1.6797e-01,  1.9287e-01,  6.0463e-03,\n",
       "                       2.7808e-01,  1.5088e-01,  4.4441e-03,  1.1945e-01,  2.0520e-01,\n",
       "                       4.3915e-02, -1.4722e-01,  1.9653e-01,  1.9690e-01,  2.0911e-01,\n",
       "                       2.5131e-02,  2.1912e-01,  1.9092e-01,  2.6709e-01,  1.9409e-01,\n",
       "                       2.4841e-01, -1.9324e-01,  2.4866e-01,  2.1045e-01,  1.6846e-01,\n",
       "                       2.4109e-01,  1.5515e-01, -7.0984e-02,  2.0947e-01, -1.7151e-01,\n",
       "                       1.4893e-01,  2.0886e-01,  2.5879e-01,  1.5442e-01, -1.4014e-01,\n",
       "                       2.8760e-01,  7.4341e-02,  2.0178e-01,  2.3584e-01,  2.6099e-01,\n",
       "                      -1.9196e-02,  1.9006e-01,  2.0984e-01,  1.8896e-01,  7.2937e-02,\n",
       "                      -3.5858e-02,  2.5684e-01, -7.4707e-02,  1.4551e-01,  1.8103e-01,\n",
       "                       4.0466e-02,  1.0565e-01,  3.1372e-01,  1.3940e-01,  1.4233e-01,\n",
       "                       2.1777e-01,  1.9629e-01,  1.4539e-01,  2.5659e-01,  1.6211e-01,\n",
       "                       2.0654e-01,  2.0007e-01, -6.9031e-02,  1.9409e-01,  1.2610e-01,\n",
       "                      -5.0293e-02,  1.3501e-01,  3.1372e-01,  9.4788e-02,  2.1057e-01,\n",
       "                       1.9775e-01,  2.1802e-01,  8.8928e-02,  1.9495e-01,  1.1307e-02,\n",
       "                       1.4636e-01,  1.8262e-01,  2.7930e-01,  1.8030e-01,  1.6418e-01,\n",
       "                       2.0581e-01,  1.3745e-01, -3.1952e-02,  1.5405e-01,  5.2100e-01,\n",
       "                       2.4976e-01,  2.6416e-01,  1.2189e-01,  2.2961e-01,  6.1768e-02,\n",
       "                       3.0200e-01,  1.6919e-01,  1.2115e-01, -1.3831e-01,  2.1973e-01,\n",
       "                       2.0520e-01,  1.1707e-01,  2.4390e-01,  2.1191e-01,  1.1505e-01,\n",
       "                       2.1948e-01,  8.0109e-03, -1.8884e-01,  1.8225e-01,  2.1484e-01,\n",
       "                       1.0712e-01,  1.7432e-01,  1.5430e-01, -5.6725e-03,  3.0869e-02,\n",
       "                       1.4526e-01, -4.2334e-01,  1.1835e-01,  2.0068e-01,  2.0276e-01,\n",
       "                       1.4331e-01,  3.0640e-01,  9.7534e-02,  2.3779e-01, -6.9214e-02,\n",
       "                       3.2178e-01,  1.3708e-01,  1.5430e-01,  2.2034e-01,  1.9946e-01,\n",
       "                       6.1523e-02,  1.3245e-01,  1.9727e-01,  2.7441e-01,  2.7075e-01,\n",
       "                       2.1133e-02,  1.5820e-01,  1.7542e-01,  2.7173e-01,  3.1445e-01,\n",
       "                      -3.4973e-02,  1.5723e-01,  6.9214e-02,  1.2158e-01,  2.3657e-01,\n",
       "                       2.9883e-01,  2.5781e-01,  2.0642e-01,  2.1948e-01,  1.3464e-01,\n",
       "                       2.7734e-01,  3.1787e-01,  2.9761e-01,  1.4539e-01,  1.2445e-01,\n",
       "                       1.7566e-01,  1.8518e-01,  2.1094e-01,  4.0576e-01,  4.5410e-02,\n",
       "                       4.4189e-01,  2.2083e-01,  2.9663e-01,  3.9795e-02,  3.3325e-01,\n",
       "                       2.3425e-01,  1.2036e-01,  1.3818e-01,  2.7246e-01,  2.1985e-01,\n",
       "                       2.1716e-01,  1.2268e-01,  1.0394e-01,  2.8247e-01, -4.0474e-03,\n",
       "                       2.9370e-01,  1.6248e-01,  1.4514e-01,  4.7424e-02,  1.4429e-01,\n",
       "                       2.0593e-01,  1.5271e-01,  1.1798e-01,  2.5513e-01,  1.0333e-01,\n",
       "                       3.2544e-01, -2.1423e-01,  1.0223e-01,  2.0435e-01,  3.5156e-02,\n",
       "                       7.7393e-02, -6.2805e-02,  1.6846e-01,  2.0251e-01,  1.7480e-01,\n",
       "                       2.0850e-01,  2.7734e-01,  1.6528e-01,  3.7445e-02,  2.5024e-01,\n",
       "                       1.4966e-01,  1.2207e-01,  2.8223e-01,  1.6443e-01,  1.6699e-01,\n",
       "                       6.4468e-03,  2.6660e-01,  2.9883e-01,  2.5659e-01,  1.2817e-01,\n",
       "                       3.2007e-01,  1.8909e-01,  1.5308e-01,  1.8481e-01,  7.5439e-02,\n",
       "                       1.7261e-01,  1.5869e-01,  1.0559e-01, -2.8229e-02,  5.8136e-03,\n",
       "                      -2.4854e-01,  1.2024e-01,  2.0032e-01,  9.9915e-02,  1.6821e-01,\n",
       "                       2.2229e-01,  2.4231e-01, -1.4807e-01,  1.0516e-01,  3.4404e-04,\n",
       "                      -2.9053e-01,  1.0590e-02,  1.4990e-01,  2.0569e-01,  1.7896e-01,\n",
       "                       3.6011e-01,  2.9272e-01, -1.3281e-01,  2.3718e-01,  1.5576e-01,\n",
       "                      -6.9214e-02,  9.6436e-02, -1.9385e-01,  1.5564e-01,  2.6733e-01,\n",
       "                       3.3545e-01,  2.8198e-01,  2.8662e-01,  1.6052e-01,  4.3427e-02,\n",
       "                       8.0078e-02,  2.2839e-01,  1.2262e-01,  1.3879e-01,  9.9976e-02,\n",
       "                       1.6614e-01, -9.6008e-02,  2.0691e-01,  1.9116e-01,  2.2314e-01,\n",
       "                       1.6577e-01,  1.9653e-01,  5.5542e-03,  2.4426e-01,  1.3782e-01,\n",
       "                       7.8369e-02, -8.2932e-03,  3.4497e-01,  8.5571e-02,  1.0144e-01,\n",
       "                       1.3329e-02,  1.7786e-01,  2.7515e-01,  2.2559e-01,  3.6353e-01,\n",
       "                       1.5222e-01,  1.3611e-01,  2.1313e-01, -2.0618e-01,  2.4646e-01,\n",
       "                       1.8066e-01,  1.7871e-01], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight',\n",
       "              tensor([ 2.0078e+00,  7.6485e-03,  9.4434e-01,  1.0400e+00,  1.0879e+00,\n",
       "                       6.8604e-01,  7.9346e-01,  1.3242e+00,  8.1445e-01,  6.7676e-01,\n",
       "                       1.2988e+00,  9.0430e-01,  8.0762e-01,  7.0996e-01,  9.3799e-01,\n",
       "                       1.2881e+00,  9.8926e-01,  1.2637e+00,  1.1250e+00,  7.4609e-01,\n",
       "                       6.0352e-01,  1.3525e+00,  2.1035e+00,  9.8096e-01,  7.5195e-01,\n",
       "                       6.2012e-01,  7.0381e-03,  2.0256e-03,  6.4404e-01,  9.5557e-01,\n",
       "                       1.3828e+00,  1.3475e-03,  5.7666e-01,  5.1074e-01,  5.0635e-01,\n",
       "                       1.9502e+00,  6.2598e-01,  1.1221e+00,  9.3848e-01,  6.8359e-01,\n",
       "                       8.6823e-03,  8.8562e-02,  5.7910e-01,  8.1711e-03,  7.5830e-01,\n",
       "                       2.6270e+00,  9.4482e-01,  7.6123e-01,  6.8213e-01,  5.4199e-01,\n",
       "                       1.2852e+00,  7.5635e-01,  6.8799e-01,  4.2554e-01,  5.6006e-01,\n",
       "                       6.7383e-01,  6.3281e-01,  6.7261e-02,  1.2119e+00,  1.0107e+00,\n",
       "                       8.1201e-01,  7.0801e-01,  7.9102e-01,  9.2773e-02,  9.6973e-01,\n",
       "                       7.1826e-01,  9.6289e-01,  9.6094e-01,  8.1238e-02,  1.0244e+00,\n",
       "                       1.1045e+00,  2.5225e-04,  6.2598e-01,  8.5791e-01,  5.2032e-02,\n",
       "                       9.4287e-01,  5.1611e-01,  1.4287e+00,  9.4238e-01,  1.3711e+00,\n",
       "                       5.8301e-01,  2.6309e+00,  9.2041e-01,  3.4981e-03,  6.7920e-01,\n",
       "                       6.4502e-01,  1.1895e+00,  4.0991e-01,  4.3506e-01,  9.9564e-04,\n",
       "                       1.0615e+00,  9.3506e-01,  6.5283e-01,  1.3994e+00,  6.7139e-01,\n",
       "                       7.0557e-01,  6.6846e-01,  7.7588e-01,  1.2090e+00,  9.4092e-01,\n",
       "                       1.2002e+00,  7.7246e-01,  9.8047e-01,  1.1357e+00,  1.0414e-02,\n",
       "                       6.4160e-01,  7.4072e-01,  1.0371e+00,  7.3047e-01,  4.3457e-01,\n",
       "                       1.3047e+00,  6.1865e-01,  1.4229e+00,  5.1123e-01,  4.2700e-01,\n",
       "                       1.4612e-01,  5.6396e-01,  9.4629e-01,  2.3770e+00,  2.4883e+00,\n",
       "                       4.6045e-01,  1.0186e+00,  8.1787e-01,  8.8086e-01,  8.6865e-01,\n",
       "                       8.6084e-01,  5.7764e-01,  4.3457e-01,  1.0431e-01,  1.1260e+00,\n",
       "                       7.5244e-01,  8.9600e-01,  9.6045e-01,  9.9951e-01,  1.0272e-01,\n",
       "                       4.9683e-01,  1.0400e+00,  5.7568e-01,  9.7266e-01,  7.5293e-01,\n",
       "                       7.5342e-01,  1.9658e+00,  3.0322e-01,  6.4160e-01,  1.6621e+00,\n",
       "                       6.4941e-01,  8.6523e-01,  4.4629e-01,  7.1680e-01,  6.5625e-01,\n",
       "                       1.5195e+00,  1.1835e-01,  9.1016e-01,  7.2656e-01,  1.4395e+00,\n",
       "                       1.0283e+00,  9.6289e-01,  8.7402e-01,  7.5195e-01,  8.4814e-01,\n",
       "                       7.9443e-01,  5.3558e-03,  2.5234e+00,  1.9153e-01,  6.6064e-01,\n",
       "                       4.4653e-01,  5.9326e-01,  1.2588e+00,  7.3340e-01,  4.8169e-01,\n",
       "                       1.9551e+00,  4.7363e-02,  3.6182e-01,  7.6660e-01,  6.8164e-01,\n",
       "                       1.4893e-01,  8.7939e-01,  4.5239e-01,  1.7538e-03,  1.1123e+00,\n",
       "                       7.0654e-01,  5.8105e-01,  9.0039e-01,  1.9727e-01,  5.9668e-01,\n",
       "                       2.3572e-01,  7.9785e-01,  1.7188e+00,  7.2607e-01,  8.4277e-01,\n",
       "                       1.1787e-02,  4.4482e-01,  8.8525e-01,  8.4814e-01,  9.0332e-02,\n",
       "                       1.8591e-01,  5.9937e-02,  6.9153e-02,  1.2969e+00,  5.4199e-01,\n",
       "                       1.0508e+00,  1.1543e+00,  8.4619e-01,  5.2979e-01,  6.9336e-01,\n",
       "                       5.8447e-01,  1.0312e+00,  6.4795e-01,  8.8501e-02,  8.0078e-01,\n",
       "                       5.5664e-01,  7.6318e-01,  1.1533e+00,  9.0551e-04,  8.9844e-01,\n",
       "                       5.6494e-01,  6.7529e-01,  7.1143e-01,  3.3752e-02,  1.1885e+00,\n",
       "                       5.7471e-01,  1.0615e+00,  4.4946e-01,  1.2158e+00,  7.6709e-01,\n",
       "                       5.0391e-01,  1.3145e+00,  5.1416e-01,  5.5762e-01,  1.4639e+00,\n",
       "                       8.0664e-01,  2.5508e+00,  8.3936e-01,  3.4155e-01,  6.5918e-01,\n",
       "                       7.8320e-01,  8.2471e-01,  8.3350e-01,  1.2510e+00,  5.5078e-01,\n",
       "                       4.4434e-01,  7.5684e-01,  8.0762e-01,  1.7734e+00,  9.4678e-01,\n",
       "                       4.6313e-01,  9.6680e-01,  6.4453e-01,  7.8857e-02,  9.9121e-01,\n",
       "                       7.5049e-01,  1.1045e+00,  6.0107e-01,  5.5322e-01,  6.8506e-01,\n",
       "                       6.3818e-01,  4.0527e-01,  5.7568e-01,  7.5342e-01,  1.0297e-01,\n",
       "                       8.2825e-02,  1.3203e+00,  9.8340e-01,  7.2656e-01,  8.3008e-01,\n",
       "                       3.9209e-01,  7.8003e-02,  8.9258e-01,  7.9004e-01,  1.1934e+00,\n",
       "                       1.5732e+00,  5.5908e-01,  7.0850e-01,  5.9814e-01,  2.0187e-02,\n",
       "                       8.7939e-01,  8.9258e-01,  8.5498e-01,  6.9043e-01,  5.1660e-01,\n",
       "                       9.7656e-01,  8.6328e-01,  4.2749e-01,  4.9756e-01,  1.1426e+00,\n",
       "                       1.0596e+00,  7.0215e-01,  9.8022e-02,  7.2998e-01,  6.8311e-01,\n",
       "                       5.9277e-01,  9.7266e-01,  4.5703e-01,  7.0508e-01,  2.5273e+00,\n",
       "                       7.4902e-01,  6.3379e-01,  1.8594e+00,  6.3379e-01,  1.3652e+00,\n",
       "                       1.0674e+00,  6.4600e-01,  2.7661e-01,  7.3059e-02,  6.5430e-01,\n",
       "                       5.3711e-01,  8.4961e-01,  1.1953e+00,  6.2646e-01,  1.2861e+00,\n",
       "                       8.8525e-01,  8.7842e-01,  4.0503e-01,  1.0248e-01,  6.5869e-01,\n",
       "                       5.5322e-01,  9.1992e-01,  1.4951e+00,  1.0898e+00,  1.3701e+00,\n",
       "                       9.2383e-01,  8.9404e-01,  4.0137e-01,  5.9375e-01,  5.0391e-01,\n",
       "                       1.1006e+00,  8.3057e-01,  1.3857e+00,  4.4238e-01,  7.4829e-02,\n",
       "                       9.3018e-01,  1.1660e+00,  7.1240e-01,  1.2373e+00,  3.7598e-02,\n",
       "                       7.6611e-01,  1.0605e+00,  1.6455e+00,  5.5420e-01,  6.3037e-01,\n",
       "                       6.1279e-01,  7.6904e-01,  1.2549e-01,  9.6973e-01,  3.9819e-01,\n",
       "                       1.1006e+00,  5.3223e-01,  5.0635e-01,  7.3096e-01,  2.5425e-03,\n",
       "                       2.0984e-01,  7.2168e-01,  9.3628e-02,  1.8994e+00,  6.5674e-01,\n",
       "                       1.2793e+00,  1.3586e-01,  7.6953e-01,  2.2681e-01,  6.7725e-01,\n",
       "                       7.8027e-01,  1.8044e-03,  7.9395e-01,  7.8662e-01, -1.9121e-03,\n",
       "                       8.7012e-01,  9.2236e-01,  7.7539e-01,  7.5623e-02,  5.7715e-01,\n",
       "                       6.5283e-01,  6.8701e-01,  4.2432e-01,  1.7114e-01,  9.0430e-01,\n",
       "                       1.0498e+00,  7.4756e-01,  5.4834e-01,  1.2236e+00,  9.0625e-01,\n",
       "                       4.6501e-03,  8.3643e-01,  6.6699e-01,  4.1211e-01,  7.4023e-01,\n",
       "                       8.2471e-01,  6.8213e-01,  7.0654e-01,  5.1165e-04,  7.8418e-01,\n",
       "                       1.4121e+00,  5.3802e-02,  6.1963e-01,  1.4600e+00,  5.2100e-01,\n",
       "                       6.3232e-01,  4.4775e-01,  8.3691e-01,  6.6504e-01,  8.6328e-01,\n",
       "                       4.8364e-01,  7.5732e-01,  1.0342e+00,  1.0625e+00,  1.0537e+00,\n",
       "                       1.2344e+00,  3.0396e-01,  4.6704e-01,  6.9531e-01,  6.7578e-01,\n",
       "                       1.1172e+00,  9.0869e-01,  7.3193e-01,  6.7480e-01,  1.0272e-01,\n",
       "                       5.7422e-01,  8.8281e-01,  7.3535e-01,  1.0859e+00,  2.1924e-01,\n",
       "                       1.1602e+00,  1.0322e+00,  1.1871e-01,  5.2765e-02,  6.8213e-01,\n",
       "                       9.2627e-01,  4.4263e-01,  7.9053e-01,  1.0391e+00,  2.1836e+00,\n",
       "                       4.1089e-01,  3.2153e-01,  7.6233e-02,  1.0889e-01,  9.0576e-01,\n",
       "                       1.4150e+00,  7.6514e-01,  1.0342e+00,  1.1064e+00,  1.6738e+00,\n",
       "                       1.0537e+00,  1.1133e+00,  2.7173e-01,  2.5269e-01,  1.1445e+00,\n",
       "                       7.6514e-01,  1.6284e-01,  6.7932e-02,  3.7378e-01,  1.1639e-01,\n",
       "                       1.6973e+00,  1.5198e-01,  6.4014e-01,  4.9219e-01,  7.3779e-01,\n",
       "                       7.5488e-01,  6.9189e-01,  4.8779e-01,  6.1188e-02,  6.5479e-01,\n",
       "                       4.4067e-01,  9.4775e-01,  5.0684e-01,  4.5967e-04,  2.9712e-01,\n",
       "                       9.5215e-01,  6.1719e-01,  4.2896e-01,  2.3474e-01,  5.9180e-01,\n",
       "                       1.0762e+00,  9.5459e-01,  8.6328e-01,  4.7583e-01,  7.4365e-01,\n",
       "                       8.3057e-01,  5.6152e-01,  1.1475e+00,  7.9688e-01,  4.5654e-01,\n",
       "                       6.9775e-01,  6.4795e-01,  7.8906e-01,  7.2876e-02,  8.1445e-01,\n",
       "                       8.8232e-01,  9.2480e-01,  5.7861e-01,  6.5576e-01,  1.2539e+00,\n",
       "                       7.6611e-01,  2.2119e-01,  8.7207e-01,  2.2109e+00,  6.1035e-01,\n",
       "                       5.1514e-01,  7.5977e-01,  4.6753e-01,  6.2561e-02,  7.8174e-01,\n",
       "                       5.6738e-01,  6.4258e-01,  1.4512e+00,  9.2334e-01,  1.1895e+00,\n",
       "                       5.2832e-01,  7.8418e-01, -1.3313e-03,  1.7988e+00,  8.1592e-01,\n",
       "                       1.1309e+00,  9.2285e-01], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias',\n",
       "              tensor([-2.7637e-01, -4.8637e-03, -4.6021e-01, -6.6797e-01, -6.6650e-01,\n",
       "                      -6.9380e-04,  1.3354e-01, -3.5669e-01, -3.4058e-01, -3.0200e-01,\n",
       "                       6.4026e-02, -1.0828e-01, -2.4194e-01, -3.2056e-01, -3.3228e-01,\n",
       "                      -5.0621e-03, -6.9824e-01, -3.0518e-01, -1.4209e-01, -2.1875e-01,\n",
       "                      -3.9575e-01, -8.3203e-01, -2.6367e-01, -7.3926e-01, -3.4943e-02,\n",
       "                      -5.2887e-02, -9.2773e-03, -2.1515e-03, -4.1284e-01, -4.1748e-01,\n",
       "                      -3.9746e-01, -1.7357e-04, -3.2227e-01, -2.1851e-01,  1.3379e-01,\n",
       "                      -1.0269e-02, -8.3801e-02, -8.7207e-01, -3.6719e-01, -1.7188e-01,\n",
       "                      -1.0109e-02, -9.2346e-02, -4.8438e-01, -7.8354e-03,  3.5950e-02,\n",
       "                      -2.4036e-01, -6.9287e-01, -4.0161e-01, -2.0496e-01,  2.0885e-03,\n",
       "                      -2.7051e-01, -1.7676e-01, -2.2913e-01,  2.1774e-02, -3.6530e-02,\n",
       "                      -5.0354e-02, -5.8556e-03, -5.6549e-02, -1.9812e-01, -4.3335e-01,\n",
       "                      -1.2622e-01, -1.8896e-01, -3.4106e-01, -8.0627e-02,  6.5735e-02,\n",
       "                      -6.6711e-02, -4.4458e-01, -4.1699e-01, -7.6233e-02,  7.4158e-02,\n",
       "                      -1.6736e-01, -4.6158e-04, -4.8120e-01, -1.6248e-01, -4.7180e-02,\n",
       "                      -2.7173e-01, -3.8428e-01, -2.3633e-01, -2.3718e-01,  1.8567e-01,\n",
       "                      -5.1367e-01, -1.0187e-01,  1.0059e-01, -3.1433e-03, -3.4521e-01,\n",
       "                      -3.2812e-01, -4.6509e-01, -1.2878e-02,  6.1066e-02, -6.6137e-04,\n",
       "                      -3.9014e-01,  1.1304e-01, -1.5442e-01,  3.2074e-02, -2.8271e-01,\n",
       "                      -1.7188e-01, -3.6768e-01,  4.4830e-02, -2.7661e-01,  1.2372e-01,\n",
       "                       8.3740e-02, -1.9104e-01, -5.2148e-01, -1.6040e-01, -8.6212e-03,\n",
       "                      -4.5703e-01, -2.6196e-01, -1.3062e-01, -1.7175e-01, -2.0966e-02,\n",
       "                      -6.3184e-01, -6.0205e-01, -4.8193e-01, -3.0151e-01, -1.0205e-01,\n",
       "                      -1.2646e-01, -6.1760e-03,  6.0760e-02, -2.3083e-01, -6.6605e-03,\n",
       "                      -3.7134e-01, -8.0029e-01, -2.4854e-01, -1.3672e-01, -6.6406e-02,\n",
       "                      -1.4917e-01, -1.7102e-01, -3.1641e-01, -1.0028e-01, -4.8755e-01,\n",
       "                      -3.2959e-02,  7.4280e-02, -8.3740e-01, -4.3018e-01, -9.2590e-02,\n",
       "                      -7.9712e-02, -1.0535e-01, -2.7661e-01, -4.6356e-02, -2.4463e-01,\n",
       "                       3.3264e-02, -1.2964e-01, -2.6538e-01, -4.3140e-01, -7.6855e-01,\n",
       "                       3.4912e-02, -7.9224e-02, -5.7739e-02, -2.7148e-01, -2.7466e-01,\n",
       "                      -4.8492e-02, -1.1829e-01, -5.8936e-01, -2.6367e-01, -2.3889e-01,\n",
       "                      -7.6660e-01,  1.1371e-01, -5.8643e-01, -2.0667e-01, -6.7090e-01,\n",
       "                      -3.2324e-01, -5.5885e-03, -2.3108e-01, -1.6357e-01, -3.8232e-01,\n",
       "                      -7.9590e-02, -1.9897e-01, -7.0435e-02, -6.0693e-01,  1.0791e-01,\n",
       "                      -2.8735e-01, -4.6265e-02, -2.1558e-01, -2.6147e-01,  2.1229e-03,\n",
       "                      -1.2585e-01, -3.4473e-01, -7.9102e-02, -2.0332e-03, -4.7852e-01,\n",
       "                      -4.2627e-01, -3.3008e-01, -7.5195e-02, -1.4453e-01, -4.2554e-01,\n",
       "                      -1.8933e-01, -3.9526e-01, -2.1774e-02, -4.5337e-01, -1.8616e-01,\n",
       "                      -1.1948e-02, -1.2073e-01, -4.0967e-01, -3.1030e-01, -9.6375e-02,\n",
       "                      -1.3599e-01, -6.4148e-02, -6.5430e-02, -4.9487e-01, -2.9614e-01,\n",
       "                       1.7407e-01, -1.3623e-01, -3.6060e-01, -1.0034e-01, -5.5351e-03,\n",
       "                      -4.3872e-01,  2.0721e-02, -2.0081e-01, -9.3323e-02, -3.4985e-01,\n",
       "                      -1.5344e-01, -2.5586e-01, -4.3262e-01, -2.3823e-03, -2.1826e-01,\n",
       "                      -4.5801e-01, -6.0883e-02, -3.4961e-01, -3.1372e-02, -8.1665e-02,\n",
       "                      -1.7725e-01, -5.3271e-01, -1.5088e-01, -1.9861e-01, -8.2764e-02,\n",
       "                      -4.1016e-01, -4.3896e-01,  2.3926e-02, -4.0332e-01, -7.6562e-01,\n",
       "                      -9.2957e-02, -2.1228e-01, -4.0039e-01, -2.5620e-02, -2.2327e-01,\n",
       "                       2.6703e-03, -4.1290e-02, -9.2957e-02, -2.5732e-01, -4.5868e-02,\n",
       "                      -7.4158e-02, -2.9614e-01, -3.9380e-01, -2.4207e-01, -4.0039e-01,\n",
       "                      -3.6377e-02, -3.5474e-01, -3.5425e-01, -7.4707e-02,  1.2268e-02,\n",
       "                      -2.6953e-01, -1.7334e-01, -4.2969e-01, -3.6426e-01, -2.5299e-02,\n",
       "                      -2.8564e-01,  4.0192e-02, -5.0781e-01, -3.5767e-01, -1.0547e-01,\n",
       "                      -7.1838e-02, -1.3464e-01, -1.1871e-01, -1.6516e-01, -3.4521e-01,\n",
       "                      -1.2781e-01, -7.7026e-02, -1.2830e-01, -2.3547e-01, -1.1438e-01,\n",
       "                       2.7954e-01, -5.1697e-02, -1.9238e-01, -4.4800e-01, -1.6769e-02,\n",
       "                      -1.3550e-01,  2.8763e-02, -2.2620e-01, -3.9697e-01, -1.1731e-01,\n",
       "                      -6.2109e-01, -6.5381e-01,  7.7438e-03, -1.4111e-01, -2.0020e-01,\n",
       "                      -7.1143e-01, -1.0809e-01, -8.1055e-02, -4.2236e-01, -2.5952e-01,\n",
       "                      -4.8169e-01, -4.7949e-01, -1.9800e-01,  1.4221e-02, -2.5195e-01,\n",
       "                      -4.3433e-01, -2.8711e-01,  1.2444e-02, -4.3091e-01, -2.4670e-01,\n",
       "                      -5.1855e-01, -6.5857e-02,  1.8433e-02, -7.6355e-02, -2.4963e-01,\n",
       "                      -4.0625e-01, -4.4141e-01, -2.3486e-01, -3.5278e-01, -3.5718e-01,\n",
       "                      -3.4082e-01, -2.1729e-01, -1.6272e-01, -1.0028e-01, -2.5586e-01,\n",
       "                      -3.9062e-01, -4.3945e-01, -1.9873e-01, -7.4316e-01, -4.2456e-01,\n",
       "                      -3.4668e-01, -3.9941e-01, -1.6895e-01, -5.0684e-01, -3.2898e-02,\n",
       "                      -6.8848e-02,  4.3060e-02, -3.4351e-01,  4.7455e-02, -7.9956e-02,\n",
       "                      -5.1465e-01, -2.7808e-01, -4.8218e-01, -8.4375e-01, -4.0131e-02,\n",
       "                      -2.5574e-02, -6.5857e-02, -3.7085e-01, -3.0371e-01, -4.1357e-01,\n",
       "                      -4.2175e-02, -3.1396e-01, -1.0425e-01, -4.1113e-01,  2.4048e-02,\n",
       "                      -2.4487e-01,  1.0840e-01, -8.7402e-02, -3.0347e-01, -7.7581e-04,\n",
       "                      -2.0203e-01, -7.0410e-01, -8.5693e-02, -3.3569e-01, -5.7220e-02,\n",
       "                      -2.4658e-01, -1.1884e-01, -2.3828e-01, -1.8481e-01, -2.4792e-01,\n",
       "                       3.0838e-02, -4.4775e-04, -3.5059e-01, -4.3018e-01,  1.3626e-04,\n",
       "                      -1.5015e-01,  1.1145e-01, -2.3889e-01, -6.8054e-02, -4.1943e-01,\n",
       "                      -1.5625e-01,  2.0981e-02, -1.3623e-01, -2.0923e-01, -9.7046e-02,\n",
       "                      -3.5938e-01, -4.6973e-01, -3.6938e-01, -1.7419e-01, -2.2559e-01,\n",
       "                      -5.0735e-03, -4.3555e-01, -3.6230e-01, -1.6895e-01, -2.0752e-01,\n",
       "                      -3.8208e-01, -3.2690e-01, -4.3701e-01,  5.6648e-04,  8.8989e-02,\n",
       "                      -1.0229e-01, -4.9652e-02, -4.9927e-01, -1.0127e+00, -2.7417e-01,\n",
       "                      -3.4326e-01, -2.1045e-01,  1.0248e-01, -5.4639e-01, -5.9668e-01,\n",
       "                      -4.6826e-01, -3.1445e-01, -1.4050e-01, -3.0200e-01, -2.7515e-01,\n",
       "                      -2.2949e-01, -1.8054e-01, -1.8506e-01,  2.4780e-02, -6.1707e-02,\n",
       "                      -1.0098e+00,  7.6172e-02,  4.9896e-02, -5.4932e-01, -9.8206e-02,\n",
       "                      -3.9307e-01, -3.0957e-01, -9.9487e-02, -4.3018e-01, -1.7676e-01,\n",
       "                      -6.4404e-01, -3.9380e-01, -1.1755e-01, -4.8920e-02, -8.2715e-01,\n",
       "                      -1.8396e-01, -1.1237e-01, -2.4121e-01, -1.0925e-01, -2.1069e-01,\n",
       "                      -1.7188e-01, -1.2781e-01, -6.6101e-02, -1.0303e-01, -3.2471e-01,\n",
       "                       7.2327e-02, -2.9297e-01, -3.9331e-01, -7.7344e-01, -6.7383e-02,\n",
       "                      -3.7134e-01, -1.1676e-01, -2.0239e-01, -2.0532e-01, -2.9199e-01,\n",
       "                      -3.6572e-01, -1.3367e-01, -6.9519e-02,  1.5869e-02, -1.3110e-01,\n",
       "                      -5.1367e-01, -1.3818e-01, -4.5850e-01, -4.0894e-02,  5.8350e-02,\n",
       "                      -4.4238e-01, -1.2335e-01, -1.4941e-01, -5.9906e-02, -3.1250e-02,\n",
       "                       4.4495e-02,  1.0535e-01, -3.6450e-01, -2.1458e-03, -2.5220e-01,\n",
       "                      -2.8760e-01, -4.8608e-01,  2.4796e-02,  9.0515e-02, -4.1260e-01,\n",
       "                      -4.2651e-01,  5.9814e-02, -1.4746e-01, -1.2341e-01, -5.6592e-01,\n",
       "                      -4.7144e-01, -9.8083e-02, -3.1152e-01, -3.4619e-01, -2.3755e-01,\n",
       "                      -3.7476e-02, -3.7646e-01, -2.9175e-01, -6.7566e-02, -8.6212e-03,\n",
       "                       7.7271e-02, -2.1643e-01, -4.3579e-01, -3.7231e-01, -3.3838e-01,\n",
       "                       6.2561e-02, -1.7883e-01,  3.4149e-02, -2.4280e-01, -1.5149e-01,\n",
       "                      -6.6772e-02, -2.9907e-01, -3.2495e-01, -6.9214e-02, -3.0225e-01,\n",
       "                      -2.8857e-01, -7.2876e-02, -1.1243e-01, -3.5864e-01, -5.2002e-01,\n",
       "                      -5.0171e-02, -3.5327e-01, -2.2469e-03, -4.4043e-01, -5.5267e-02,\n",
       "                      -2.0325e-01, -4.3213e-01], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.2.conv.weight',\n",
       "              tensor([[[ 0.1364,  0.0595,  0.0558],\n",
       "                       [-0.0027,  0.0036,  0.0130],\n",
       "                       [ 0.0205, -0.1594, -0.1644],\n",
       "                       ...,\n",
       "                       [-0.0302, -0.1316, -0.3022],\n",
       "                       [-0.1302,  0.0339,  0.1055],\n",
       "                       [-0.0198,  0.0161, -0.2947]],\n",
       "              \n",
       "                      [[ 0.0486,  0.0623,  0.0605],\n",
       "                       [ 0.0252,  0.0129,  0.0387],\n",
       "                       [-0.0129,  0.0412,  0.1558],\n",
       "                       ...,\n",
       "                       [-0.0335,  0.0049,  0.1519],\n",
       "                       [ 0.0413,  0.0309, -0.1652],\n",
       "                       [ 0.2773,  0.1691, -0.1676]],\n",
       "              \n",
       "                      [[ 0.0562,  0.0528,  0.0679],\n",
       "                       [ 0.0068, -0.0200,  0.0214],\n",
       "                       [-0.2084,  0.0162, -0.0525],\n",
       "                       ...,\n",
       "                       [-0.0758,  0.0801,  0.1887],\n",
       "                       [ 0.3630,  0.2178,  0.0604],\n",
       "                       [ 0.0210,  0.2490,  0.1212]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.1433,  0.0085, -0.3591],\n",
       "                       [-0.0114,  0.0310,  0.0248],\n",
       "                       [ 0.0788,  0.0666,  0.0865],\n",
       "                       ...,\n",
       "                       [ 0.2230, -0.0247,  0.0292],\n",
       "                       [-0.0077, -0.0243,  0.0525],\n",
       "                       [ 0.0226, -0.0550,  0.0958]],\n",
       "              \n",
       "                      [[ 0.0714,  0.0373,  0.0659],\n",
       "                       [-0.0265,  0.0334,  0.0310],\n",
       "                       [ 0.1245, -0.2297, -0.0685],\n",
       "                       ...,\n",
       "                       [ 0.2686, -0.2405, -0.3040],\n",
       "                       [ 0.0626,  0.0735,  0.0289],\n",
       "                       [-0.0309, -0.0014,  0.0367]],\n",
       "              \n",
       "                      [[ 0.0657,  0.0394,  0.0715],\n",
       "                       [ 0.0172,  0.0166,  0.0124],\n",
       "                       [-0.0970, -0.1090, -0.0036],\n",
       "                       ...,\n",
       "                       [-0.1805, -0.0740,  0.1110],\n",
       "                       [ 0.2578,  0.2362,  0.1311],\n",
       "                       [-0.0439, -0.1266,  0.0463]]], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.2.conv.bias',\n",
       "              tensor([ 3.4570e-01,  8.5083e-02,  2.1570e-01,  2.8979e-01, -3.1464e-02,\n",
       "                       1.9470e-02,  1.6190e-02,  6.3574e-01,  2.1347e-02,  2.4231e-01,\n",
       "                       1.3232e-01,  1.1780e-01,  1.5381e-01, -2.2537e-02,  2.5952e-01,\n",
       "                       2.1106e-01,  3.3423e-01,  2.9102e-01,  4.6844e-02,  2.4402e-01,\n",
       "                       1.0406e-01,  6.7993e-02,  1.7041e-01,  2.4622e-01,  4.4525e-02,\n",
       "                       7.9346e-02,  1.5234e-01,  7.7637e-02, -2.0111e-02,  2.0923e-01,\n",
       "                       6.9885e-02,  1.5442e-01,  1.8054e-01, -3.5980e-02,  1.5991e-01,\n",
       "                      -4.7394e-02, -4.0161e-02,  2.9150e-01,  3.9948e-02,  2.5122e-01,\n",
       "                      -9.4666e-02,  2.1118e-01,  7.8979e-02, -1.0370e-01,  4.4830e-02,\n",
       "                       4.2676e-01,  8.6060e-02, -2.7985e-02, -7.1655e-02,  2.0105e-01,\n",
       "                      -6.3705e-03,  1.2622e-01,  1.1682e-01, -8.5938e-02,  4.7882e-02,\n",
       "                       7.7759e-02,  1.3741e-02,  3.6011e-02, -1.0687e-01, -8.4961e-02,\n",
       "                       2.5220e-01,  5.5084e-02,  1.4290e-02,  1.0590e-02,  9.3323e-02,\n",
       "                       3.9160e-01,  2.1472e-01,  9.9060e-02,  9.9731e-02,  3.6328e-01,\n",
       "                      -2.2202e-02,  7.3364e-02,  4.5093e-01,  7.7515e-02,  4.6875e-01,\n",
       "                       8.8623e-02,  8.8959e-03,  1.8970e-01,  1.7676e-01, -1.5601e-01,\n",
       "                      -3.0197e-02, -2.1103e-02,  8.2825e-02,  4.5502e-02, -1.6117e-03,\n",
       "                       1.2952e-01,  2.3389e-01,  1.3403e-01, -1.5820e-01,  7.3181e-02,\n",
       "                       9.1614e-02,  2.4988e-01,  4.5850e-01,  2.0349e-01, -3.1067e-02,\n",
       "                       6.4758e-02,  3.5181e-01, -8.8196e-02,  2.2571e-01,  1.0852e-01,\n",
       "                      -1.2354e-01,  5.8105e-02,  2.5098e-01,  4.8291e-01,  1.4905e-01,\n",
       "                       6.9153e-02, -1.8811e-01,  2.6465e-01,  1.1053e-01,  4.7455e-02,\n",
       "                      -3.6102e-02,  1.2549e-01,  3.8062e-01,  2.0203e-01,  3.1567e-01,\n",
       "                       2.5317e-01,  7.0740e-02,  2.0667e-01, -2.1768e-04, -1.7078e-01,\n",
       "                       2.7051e-01,  4.1333e-01,  1.6565e-01,  1.1823e-01,  2.5317e-01,\n",
       "                       8.8013e-02, -3.1769e-02,  2.0117e-01,  5.0244e-01,  1.8164e-01,\n",
       "                       5.3711e-02, -2.4445e-02, -1.0727e-02,  7.6355e-02,  7.6355e-02,\n",
       "                       1.8481e-01,  1.8604e-01,  3.9624e-01, -9.0256e-03,  2.2363e-01,\n",
       "                       2.3962e-01,  2.0471e-01,  1.0126e-01,  1.7993e-01, -6.5575e-03,\n",
       "                       1.0413e-01, -2.1858e-03,  5.9479e-02,  2.4365e-01,  3.6841e-01,\n",
       "                       3.7329e-01,  5.9204e-02,  2.5708e-01,  3.0859e-01,  2.4109e-01,\n",
       "                       1.9373e-01,  5.0244e-01,  4.4946e-01,  5.0262e-02, -9.9670e-02,\n",
       "                       1.3049e-01,  1.8628e-01, -4.4708e-02,  2.3315e-01,  6.8481e-02,\n",
       "                      -3.1250e-02,  8.9478e-02,  2.8955e-01, -9.2346e-02,  1.6144e-02,\n",
       "                       2.2705e-01, -2.9419e-02, -6.1615e-02,  1.3257e-01, -1.7075e-02,\n",
       "                      -1.9257e-02, -3.8055e-02,  2.6416e-01,  1.0767e-01,  6.7993e-02,\n",
       "                       5.1953e-01,  9.1492e-02,  5.0995e-02,  1.0199e-01,  6.0455e-02,\n",
       "                      -1.0297e-01,  1.4880e-01,  1.9409e-01,  1.8628e-01,  1.5479e-01,\n",
       "                       1.3649e-02,  2.8223e-01,  1.7114e-01,  2.3206e-01,  1.2976e-01,\n",
       "                       5.0995e-02, -1.3074e-01,  1.9641e-01,  9.8572e-02, -3.2990e-02,\n",
       "                       9.9426e-02,  6.1310e-02,  7.6180e-03,  3.4229e-01, -1.4679e-02,\n",
       "                       1.6565e-01,  1.0608e-01,  1.0663e-01,  2.9395e-01,  1.9006e-01,\n",
       "                       2.9395e-01,  3.2104e-01,  1.6565e-01,  1.6113e-01,  1.5894e-01,\n",
       "                      -1.9608e-02,  3.4937e-01,  3.2715e-02,  3.8159e-01,  4.4849e-01,\n",
       "                       2.6294e-01,  5.2414e-03,  6.9824e-02,  4.9591e-02,  4.8340e-02,\n",
       "                       4.9744e-02,  6.5063e-02,  2.9221e-02,  3.2056e-01,  2.1411e-01,\n",
       "                       2.6733e-01,  1.5918e-01,  1.9275e-01,  4.4312e-01,  4.5190e-01,\n",
       "                       2.1875e-01,  5.0537e-01,  1.7188e-01,  3.7573e-01,  2.2363e-01,\n",
       "                       1.0254e-02,  4.0955e-02, -4.6814e-02,  2.1387e-01, -2.0416e-02,\n",
       "                       1.8420e-01, -8.7158e-02,  2.9053e-02,  2.3987e-01,  1.4636e-01,\n",
       "                       2.1082e-01,  2.3438e-01,  1.3318e-01,  8.1482e-02,  1.0406e-01,\n",
       "                       3.0615e-01,  8.2214e-02,  2.5854e-01,  6.3538e-02,  5.1575e-03,\n",
       "                      -3.1494e-02,  2.2766e-01, -1.8555e-02,  2.2522e-01,  1.1316e-01,\n",
       "                       1.5039e-01,  1.5137e-01,  2.3145e-01, -1.9252e-04,  7.7881e-02,\n",
       "                       5.6824e-02,  2.6440e-01,  5.2881e-01,  3.3984e-01,  9.6436e-02,\n",
       "                      -3.7140e-02,  2.2998e-01,  3.1738e-01,  2.6343e-01,  2.1277e-01,\n",
       "                      -6.9824e-02,  1.5210e-01,  1.4941e-01,  2.0825e-01,  3.6133e-01,\n",
       "                       9.8267e-02,  3.6938e-01,  3.3691e-02,  2.8369e-01,  2.9028e-01,\n",
       "                      -9.8816e-02,  1.4160e-02,  1.8860e-02,  1.1279e-01,  1.3184e-01,\n",
       "                       1.3489e-02,  2.1448e-01,  1.0353e-02,  1.5308e-01,  3.7354e-01,\n",
       "                       1.8091e-01,  1.5295e-01,  1.9189e-01,  2.6050e-01, -1.1725e-01,\n",
       "                       1.6748e-01,  3.2178e-01,  1.9543e-01,  5.6885e-02,  1.8384e-01,\n",
       "                      -1.2012e-01,  1.3623e-01,  2.5195e-01,  1.4490e-01,  1.6504e-01,\n",
       "                       2.3816e-01,  2.3486e-01, -5.3375e-02,  2.3462e-01,  2.8101e-01,\n",
       "                       2.2141e-02,  1.9263e-01,  2.6840e-02,  6.0150e-02,  3.3447e-01,\n",
       "                       9.4360e-02,  6.7078e-02,  1.7480e-01,  2.4060e-01,  4.9622e-02,\n",
       "                       2.3560e-01,  5.5481e-02,  4.0527e-02, -3.9124e-02,  3.5919e-02,\n",
       "                      -6.3965e-02, -2.5928e-01, -1.4551e-01,  2.6831e-01, -4.4495e-02,\n",
       "                       1.9666e-01, -6.3416e-02, -4.7119e-02,  2.1509e-01,  1.1029e-01,\n",
       "                       2.1631e-01,  6.7139e-02,  2.0218e-02, -3.0945e-02,  4.4824e-01,\n",
       "                      -1.8921e-02,  5.7487e-03,  1.3696e-01,  2.3169e-01,  2.6465e-01,\n",
       "                       3.9185e-02,  4.0137e-01,  7.9163e-02, -2.7756e-02,  4.1687e-02,\n",
       "                       3.1519e-01,  5.7404e-02,  9.0698e-02,  1.0516e-01, -6.8115e-02,\n",
       "                       2.3792e-01, -3.3783e-02, -1.6384e-03,  1.1823e-01,  1.9287e-01,\n",
       "                       1.5881e-01, -3.7018e-02,  3.5286e-03, -1.4061e-02,  3.3276e-01,\n",
       "                       1.9250e-01, -2.5055e-02,  4.4312e-02,  4.7150e-02,  8.5510e-02,\n",
       "                       5.7953e-02, -7.5928e-02,  1.4099e-01,  1.1835e-01,  2.4536e-01,\n",
       "                       4.4263e-01,  2.4866e-01,  3.2935e-01, -3.0396e-02,  8.8867e-02,\n",
       "                      -1.0345e-01, -1.3596e-02,  1.2720e-01,  3.2837e-01,  3.1934e-01,\n",
       "                       1.5747e-01,  4.0649e-01,  1.0706e-01,  1.6760e-01,  4.6655e-01,\n",
       "                      -6.8970e-02, -2.5589e-02, -3.3661e-02,  3.9233e-01,  4.3213e-01,\n",
       "                       1.2854e-01,  3.3789e-01,  1.7847e-01, -1.6434e-02,  9.8999e-02,\n",
       "                       4.0710e-02,  1.4417e-01,  4.5227e-02,  2.7539e-01, -8.6853e-02,\n",
       "                       2.9932e-01,  1.9263e-01,  1.5900e-02,  4.9469e-02,  1.2329e-01,\n",
       "                       2.4158e-01, -1.7914e-02, -2.0703e-01,  9.7412e-02,  1.4844e-01,\n",
       "                       2.1875e-01, -1.9617e-01, -4.6936e-02,  1.2189e-01,  5.5029e-01,\n",
       "                       3.5181e-01,  9.1492e-02,  5.5115e-02,  1.2457e-01,  1.0602e-01,\n",
       "                       4.2908e-02,  2.0117e-01, -1.5762e-02,  2.0544e-01,  1.0785e-01,\n",
       "                       2.7026e-01,  1.4783e-01, -1.2413e-02,  1.9702e-01,  2.0337e-01,\n",
       "                      -8.4229e-02, -5.1422e-02,  4.5654e-01,  1.2891e-01, -5.6686e-03,\n",
       "                       2.0349e-01, -1.0658e-02,  4.0918e-01,  1.4246e-01,  7.5562e-02,\n",
       "                      -8.2275e-02,  1.6800e-02,  3.9697e-01, -2.6886e-02,  2.0654e-01,\n",
       "                       2.8345e-01,  1.5674e-01,  7.6355e-02,  1.6382e-01,  2.3328e-01,\n",
       "                       3.3417e-02, -3.6194e-02, -3.2990e-02,  1.9055e-01,  1.3542e-02,\n",
       "                       1.3281e-01,  3.8159e-01,  2.8979e-01,  2.0642e-01,  4.7668e-02,\n",
       "                       1.8262e-01,  1.6809e-01, -3.2623e-02,  1.0516e-01,  3.2013e-02,\n",
       "                       1.6101e-01, -7.5378e-02,  2.9395e-01,  2.9590e-01,  2.2742e-01,\n",
       "                       2.1338e-01,  1.3745e-01,  3.3301e-01, -3.7689e-02,  1.5637e-01,\n",
       "                      -2.6875e-03,  7.3792e-02,  2.5684e-01,  1.3757e-01,  3.6035e-01,\n",
       "                       8.7952e-02,  2.6392e-01,  5.4492e-01,  2.0349e-01, -2.8900e-02,\n",
       "                       2.2046e-01,  1.9516e-02, -1.0724e-01,  1.3025e-01,  1.0138e-01,\n",
       "                       3.3105e-01, -7.3303e-02,  1.9299e-01,  1.5259e-01,  1.3147e-01,\n",
       "                       4.9622e-02,  8.6670e-03], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight',\n",
       "              tensor([ 4.8193e-01,  8.4131e-01,  9.0967e-01,  6.5967e-01,  8.3789e-01,\n",
       "                       8.5986e-01,  8.3398e-01,  8.3923e-02,  1.1318e+00,  4.4165e-01,\n",
       "                       7.4316e-01,  1.1221e+00,  5.7031e-01,  1.2305e+00,  4.4214e-01,\n",
       "                       1.0059e+00,  4.0405e-01,  9.0039e-01,  8.5840e-01,  8.5205e-01,\n",
       "                       8.1250e-01,  1.1641e+00,  6.4551e-01,  1.0381e+00,  1.1748e+00,\n",
       "                       9.3750e-01,  8.9160e-01,  7.5000e-01,  1.2598e+00,  2.9297e-01,\n",
       "                       1.3486e+00,  1.3408e+00,  6.9385e-01,  6.0059e-01,  6.8506e-01,\n",
       "                       1.2109e+00,  1.0742e+00,  4.2871e-01,  6.6260e-01,  5.8887e-01,\n",
       "                       5.5811e-01,  2.5732e-01,  8.7207e-01,  6.0254e-01,  9.5166e-01,\n",
       "                       5.8350e-01,  9.5068e-01,  1.1914e+00,  9.4678e-01,  5.4590e-01,\n",
       "                       2.0176e+00,  1.2812e+00,  7.3584e-01,  7.9639e-01,  6.2207e-01,\n",
       "                       8.4277e-01,  4.4482e-01,  7.0996e-01,  5.1611e-01,  7.2266e-01,\n",
       "                       6.1621e-01,  4.3750e-01,  7.9248e-01,  1.1982e+00,  1.0830e+00,\n",
       "                       4.2480e-01,  5.4395e-01,  7.6904e-01,  7.1387e-01,  4.0723e-01,\n",
       "                       9.5898e-01,  1.1104e+00,  3.6670e-01,  6.5088e-01,  2.8320e-01,\n",
       "                       8.2422e-01,  9.6387e-01,  7.9297e-01,  4.6558e-01,  1.4102e+00,\n",
       "                       1.6416e+00,  9.0918e-01,  7.5635e-01,  1.3359e+00,  5.6250e-01,\n",
       "                       4.6582e-01,  1.3110e-01,  1.0879e+00,  1.2666e+00,  6.5186e-01,\n",
       "                       4.7363e-01,  1.1484e+00,  2.5488e-01,  1.0928e+00,  1.2344e+00,\n",
       "                       1.2119e+00,  3.3154e-01,  1.1504e+00,  8.0176e-01,  2.2839e-01,\n",
       "                       1.4580e+00,  9.0479e-01,  2.9785e-01,  2.6587e-01,  8.1689e-01,\n",
       "                       9.1016e-01,  5.2979e-01,  6.5625e-01,  6.4941e-01,  5.2539e-01,\n",
       "                       6.8604e-01,  1.2236e+00,  3.0493e-01,  7.3584e-01,  3.2935e-01,\n",
       "                       7.6123e-01,  6.4062e-01,  6.2012e-01,  1.1836e+00,  4.6655e-01,\n",
       "                       5.3076e-01,  3.3887e-01,  8.7744e-01,  7.7783e-01,  7.6611e-01,\n",
       "                       1.4277e+00,  8.4570e-01,  1.1191e+00,  3.2080e-01,  9.3408e-01,\n",
       "                       1.5635e+00,  8.4863e-01,  4.6875e-01,  8.7061e-01,  9.0088e-01,\n",
       "                       7.4072e-01,  5.9131e-01,  3.7671e-01,  8.5596e-01,  7.5049e-01,\n",
       "                       4.5654e-01,  6.9727e-01,  1.2148e+00,  3.4033e-01,  5.8936e-01,\n",
       "                       1.0244e+00,  5.7080e-01,  8.8135e-01,  8.2617e-01,  3.1201e-01,\n",
       "                       4.4360e-01,  6.1670e-01,  4.1528e-01,  1.6089e-01,  1.2666e+00,\n",
       "                       7.3975e-01,  1.0217e-01,  2.5879e-01,  1.0967e+00,  1.2070e+00,\n",
       "                       8.3447e-01,  7.3877e-01,  1.0420e+00,  5.4785e-01,  6.0889e-01,\n",
       "                       8.4229e-01,  1.0322e+00,  5.1758e-01,  1.3418e+00,  1.6797e+00,\n",
       "                       5.2588e-01,  8.5596e-01,  9.3750e-01,  8.2959e-01,  3.4985e-01,\n",
       "                       8.6816e-01,  8.5352e-01,  5.6055e-01,  7.8369e-01,  6.6260e-01,\n",
       "                       2.2607e-01,  6.0010e-01,  7.6416e-01,  5.4102e-01,  1.4004e+00,\n",
       "                       1.9258e+00,  1.1201e+00,  8.2861e-01,  1.5000e+00,  8.2617e-01,\n",
       "                       1.1221e+00,  7.7002e-01,  3.9648e-01,  5.9912e-01,  6.7773e-01,\n",
       "                       9.7217e-01,  1.0283e+00,  9.9609e-01,  4.4336e-01,  1.4639e+00,\n",
       "                       9.0869e-01,  8.6328e-01,  8.5645e-01,  3.1104e-01,  8.8037e-01,\n",
       "                       7.0605e-01,  1.2393e+00,  8.6621e-01,  3.3521e-01,  1.1182e+00,\n",
       "                       4.1455e-01,  3.2715e-01,  6.9727e-01,  7.8320e-01,  3.5425e-01,\n",
       "                       8.1055e-01,  8.6279e-01,  8.5449e-01,  5.8252e-01,  2.0581e-01,\n",
       "                       4.8804e-01,  1.2207e+00,  1.0010e+00,  1.4121e+00,  1.0312e+00,\n",
       "                       1.0361e+00,  1.6250e+00,  7.6270e-01,  5.4297e-01,  4.5728e-01,\n",
       "                       2.5024e-01,  8.8721e-01,  6.5479e-01,  3.2056e-01,  1.4673e-01,\n",
       "                       6.4307e-01,  3.3911e-01,  7.7148e-01,  3.1421e-01,  4.5923e-01,\n",
       "                       8.5303e-01,  8.8574e-01,  1.4668e+00,  8.3887e-01,  1.1719e+00,\n",
       "                       6.1621e-01,  1.1641e+00,  8.4961e-01,  7.1094e-01,  1.2715e+00,\n",
       "                       9.7363e-01,  9.7119e-01,  1.0215e+00,  1.2588e+00,  6.0938e-01,\n",
       "                       8.0615e-01,  9.1064e-01,  3.6035e-01,  7.0850e-01,  1.3770e+00,\n",
       "                       9.6094e-01,  5.1025e-01,  8.5840e-01,  6.9727e-01,  5.7471e-01,\n",
       "                       7.0410e-01,  2.7832e-01,  5.5371e-01,  8.5107e-01,  5.0098e-01,\n",
       "                       6.1035e-01,  5.9326e-01,  2.6050e-01,  6.6406e-01,  8.9404e-01,\n",
       "                       9.2969e-01,  3.7402e-01,  3.4985e-01,  5.3711e-01,  5.2588e-01,\n",
       "                       8.7646e-01,  9.0430e-01,  7.5146e-01,  6.6211e-01,  1.1641e+00,\n",
       "                       9.8682e-01,  3.3301e-01,  9.6338e-01,  4.4336e-01,  2.8052e-01,\n",
       "                       8.7061e-01,  1.3916e+00,  8.8330e-01,  9.4775e-01,  1.4316e+00,\n",
       "                       8.0176e-01,  8.6963e-01,  7.5732e-01,  1.3291e+00,  3.6743e-01,\n",
       "                       6.9629e-01,  4.0698e-01,  4.5459e-01,  6.7578e-01,  4.8779e-01,\n",
       "                       1.4248e+00,  3.0640e-01,  5.7910e-01,  7.9785e-01,  1.2803e+00,\n",
       "                       6.8945e-01,  7.1826e-01,  9.4629e-01,  6.8799e-01,  7.5342e-01,\n",
       "                       1.1162e+00,  4.2188e-01,  9.3359e-01,  6.3232e-01,  7.7637e-01,\n",
       "                       1.6162e+00,  7.6611e-01,  8.3350e-01,  1.0234e+00,  6.5771e-01,\n",
       "                       8.4814e-01,  5.9717e-01,  8.8037e-01,  8.4326e-01,  8.3301e-01,\n",
       "                       6.8164e-01,  1.3555e+00,  9.1162e-01,  1.3389e+00,  9.0283e-01,\n",
       "                       1.1162e+00,  1.4688e+00,  1.0078e+00,  5.5127e-01,  1.0176e+00,\n",
       "                       8.7842e-01,  4.2456e-01,  1.2578e+00,  7.1484e-01,  5.6592e-01,\n",
       "                       6.0547e-01,  1.0400e+00,  1.1338e+00,  6.7871e-01,  1.5564e-01,\n",
       "                       7.5928e-01,  8.1982e-01,  8.3594e-01,  7.1924e-01,  3.3447e-01,\n",
       "                       6.0449e-01,  2.0105e-01,  6.4111e-01,  9.0820e-01,  9.8340e-01,\n",
       "                       3.8208e-01,  1.0420e+00,  8.1787e-01,  8.1006e-01,  9.6826e-01,\n",
       "                       4.0234e-01,  9.5312e-01,  8.3594e-01,  6.3428e-01,  6.8994e-01,\n",
       "                       4.7144e-01,  9.4141e-01,  1.3018e+00,  7.9004e-01,  1.0430e+00,\n",
       "                       3.8721e-01,  8.6035e-01,  1.2666e+00,  9.2920e-01,  8.9893e-01,\n",
       "                       1.3896e+00,  1.0898e+00,  4.9731e-01,  6.5137e-01,  5.2783e-01,\n",
       "                       2.4854e-01,  3.1592e-01,  4.7266e-01,  8.1348e-01,  9.7852e-01,\n",
       "                       7.0166e-01,  8.6572e-01,  1.0156e+00,  3.5840e-01,  4.2578e-01,\n",
       "                       9.3652e-01,  3.3936e-01,  7.7295e-01,  6.5723e-01, -6.8426e-05,\n",
       "                       6.9531e-01,  7.7930e-01,  6.5088e-01,  3.7939e-01,  2.6343e-01,\n",
       "                       5.6592e-01,  2.8516e-01,  6.6846e-01,  9.6094e-01,  1.5049e+00,\n",
       "                       1.0449e+00,  3.7036e-01,  7.4951e-01,  4.4849e-01,  1.3135e+00,\n",
       "                       8.9990e-01,  7.6855e-01,  9.0088e-01,  4.0918e-01,  7.0801e-01,\n",
       "                       9.1895e-01,  1.0781e+00,  5.0586e-01,  6.5576e-01,  9.3262e-01,\n",
       "                       6.2549e-01,  4.8755e-01,  1.1885e+00,  5.5176e-01,  1.9128e-01,\n",
       "                       3.8672e-01,  8.3105e-01,  3.9380e-01,  4.7900e-01,  6.9141e-01,\n",
       "                       5.0537e-01,  4.3750e-01,  8.6768e-01,  5.6299e-01,  3.9893e-01,\n",
       "                       3.7085e-01,  8.9697e-01,  1.1094e+00,  4.0454e-01,  5.7129e-01,\n",
       "                       1.1992e+00,  1.6953e+00,  2.8613e-01,  8.9160e-01,  8.3789e-01,\n",
       "                       8.2764e-01,  7.9590e-01,  4.7461e-01,  7.8760e-01,  1.4961e+00,\n",
       "                       1.0957e+00,  9.0576e-01,  4.2627e-01,  8.9697e-01,  9.0137e-01,\n",
       "                       4.7705e-01,  5.9424e-01,  7.7344e-01,  6.5771e-01,  7.6416e-01,\n",
       "                       1.1484e+00,  9.3604e-01,  1.3340e+00,  5.5811e-01,  7.3291e-01,\n",
       "                       7.7783e-01,  3.5107e-01,  8.7891e-01,  6.2354e-01,  9.7900e-01,\n",
       "                       8.4180e-01,  5.1855e-01,  8.4277e-01,  8.1396e-01,  4.2847e-01,\n",
       "                       8.9258e-01,  5.9863e-01,  4.9756e-01,  5.9961e-01,  3.8477e-01,\n",
       "                       2.9614e-01,  8.2520e-01,  3.0078e-01,  8.3545e-01,  1.2715e+00,\n",
       "                       1.0010e+00,  1.0684e+00,  7.6025e-01,  6.6211e-01,  3.7061e-01,\n",
       "                       8.8525e-01,  6.4795e-01,  1.6296e-01,  4.7681e-01,  7.7686e-01,\n",
       "                       2.8760e-01,  6.6797e-01,  9.5117e-01,  7.8320e-01,  8.4619e-01,\n",
       "                       1.1328e+00,  4.2749e-01,  4.2603e-01,  3.5010e-01,  4.1089e-01,\n",
       "                       1.1836e+00,  9.5166e-01], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias',\n",
       "              tensor([-2.6562e-01, -2.7124e-01, -3.3350e-01, -3.3838e-01, -3.3618e-01,\n",
       "                      -1.5894e-01, -2.5928e-01, -1.1865e-01, -2.5732e-01, -1.1298e-01,\n",
       "                      -3.1641e-01, -2.8442e-01, -1.7395e-01, -4.5264e-01, -2.5391e-01,\n",
       "                      -3.1592e-01, -2.7319e-01, -4.4385e-01, -3.8916e-01, -4.2529e-01,\n",
       "                      -1.4856e-01, -8.1299e-01, -4.0039e-01, -3.2544e-01, -9.8535e-01,\n",
       "                      -3.2349e-01, -3.6108e-01, -4.6338e-01, -1.8542e-01, -1.6980e-01,\n",
       "                      -2.9688e-01, -3.0054e-01, -2.7954e-01, -1.6040e-01, -3.6255e-01,\n",
       "                      -1.8604e-01, -4.8248e-02, -2.7441e-01, -2.0544e-01, -3.9111e-01,\n",
       "                      -1.2756e-01,  5.5634e-02, -2.9150e-01, -1.0706e-01, -4.4458e-01,\n",
       "                      -3.9087e-01, -7.9150e-01, -3.7720e-01, -3.3008e-01, -1.7578e-01,\n",
       "                      -6.5479e-01, -2.0044e-01, -4.0088e-01,  1.9547e-02, -2.4902e-01,\n",
       "                      -2.5879e-01,  1.8518e-01, -1.1340e-01, -1.2170e-01, -1.2817e-01,\n",
       "                      -4.5801e-01, -2.0801e-01, -2.0654e-01, -4.4116e-01, -4.1992e-01,\n",
       "                      -2.1118e-01, -2.2717e-01, -3.8330e-01, -3.9038e-01, -1.8518e-01,\n",
       "                      -5.0146e-01, -3.6108e-01, -2.4915e-01, -2.5732e-01, -2.2803e-01,\n",
       "                      -4.0015e-01, -3.3691e-02, -3.4302e-01, -1.2793e-01, -8.4619e-01,\n",
       "                      -6.6260e-01, -2.9077e-01, -3.3789e-01, -8.4033e-01, -1.6309e-01,\n",
       "                      -1.3794e-01, -1.3298e-02, -3.9233e-01, -3.4204e-01, -2.3474e-01,\n",
       "                      -3.1067e-02, -7.8418e-01, -2.2327e-01, -6.0498e-01, -6.3184e-01,\n",
       "                      -7.6074e-01, -1.7615e-01, -3.5156e-01, -2.7148e-01,  9.3994e-03,\n",
       "                      -4.2267e-02, -3.0249e-01, -2.3608e-01, -2.4841e-01, -3.4766e-01,\n",
       "                      -1.6882e-01,  5.1178e-02, -2.6001e-01, -1.5002e-01, -2.4536e-02,\n",
       "                      -2.4475e-01, -1.8591e-01, -2.2070e-01, -2.4890e-01, -2.0593e-01,\n",
       "                      -3.7915e-01, -1.2805e-01, -2.8101e-01, -3.7305e-01, -1.5515e-01,\n",
       "                      -3.5498e-01, -2.3743e-01, -3.8892e-01, -1.8726e-01, -3.2422e-01,\n",
       "                      -1.4941e-01, -1.5796e-01, -3.2861e-01, -2.5757e-01, -4.9756e-01,\n",
       "                      -2.8125e-01, -2.6367e-01, -2.0544e-01, -3.6523e-01, -4.1187e-01,\n",
       "                      -3.0542e-01, -2.0459e-01, -2.1875e-01, -2.4463e-01, -3.4497e-01,\n",
       "                      -1.3062e-01, -4.0430e-01, -2.3877e-01, -1.1792e-01, -2.9248e-01,\n",
       "                      -7.5781e-01, -6.8481e-02, -2.0093e-01, -3.0493e-01, -2.2412e-01,\n",
       "                      -1.2695e-01, -2.3828e-01, -2.5244e-01, -1.3184e-01, -3.1421e-01,\n",
       "                      -3.0835e-01, -2.1777e-01, -2.2400e-01, -2.8223e-01, -2.4683e-01,\n",
       "                       3.4058e-02, -6.8994e-01, -2.1106e-01, -2.1863e-01, -2.0129e-01,\n",
       "                      -3.1885e-01, -2.1851e-01, -1.6968e-01, -5.5713e-01, -3.0542e-01,\n",
       "                      -2.8979e-01, -2.3999e-01, -1.1316e-01, -3.6743e-01, -1.2036e-01,\n",
       "                      -2.9858e-01, -2.5146e-01, -2.0776e-01, -3.1836e-01, -1.8384e-01,\n",
       "                      -2.2498e-01, -2.5854e-01, -2.7979e-01, -1.6895e-01, -6.7188e-01,\n",
       "                       3.4424e-02, -8.7793e-01, -3.0518e-01, -2.7222e-01, -2.5586e-01,\n",
       "                      -5.5078e-01, -2.1277e-01, -1.4880e-01, -2.1667e-01, -1.8457e-01,\n",
       "                      -4.4214e-01, -1.6199e-01, -3.5889e-01, -2.3267e-01, -5.5469e-01,\n",
       "                      -1.6272e-01, -3.2715e-01, -2.2278e-01, -2.2229e-01, -3.4399e-01,\n",
       "                      -3.5718e-01, -3.3252e-01, -4.4629e-01, -2.3010e-01, -3.4814e-01,\n",
       "                      -2.6538e-01, -1.0559e-01, -2.7222e-01, -3.3447e-01, -5.4047e-02,\n",
       "                      -2.0581e-01, -3.5327e-01, -3.3008e-01, -5.7129e-01, -1.0590e-01,\n",
       "                      -2.5903e-01, -2.1655e-01, -3.2812e-01, -1.1270e+00, -3.8159e-01,\n",
       "                      -5.2441e-01, -3.2056e-01, -3.5669e-01, -4.8431e-02, -1.3318e-01,\n",
       "                      -2.6709e-01, -5.7324e-01, -4.7290e-01, -1.7651e-01, -1.3757e-01,\n",
       "                      -4.0674e-01, -2.3181e-01, -4.1772e-01, -2.0984e-01, -2.0447e-01,\n",
       "                      -1.0687e-01, -4.1089e-01,  9.2840e-04, -3.3936e-01, -6.3232e-01,\n",
       "                      -1.8250e-01, -5.3271e-01, -1.4465e-01, -1.6492e-01, -9.7363e-01,\n",
       "                      -5.6836e-01, -3.7427e-01, -3.8916e-01, -8.9697e-01, -2.3145e-01,\n",
       "                      -2.7197e-01, -5.7178e-01, -1.7871e-01, -1.3342e-01, -8.0322e-01,\n",
       "                      -1.7273e-01, -1.8958e-01, -1.4233e-01, -4.4678e-01, -1.1951e-01,\n",
       "                      -3.5229e-01, -1.3452e-01, -2.5171e-01, -2.8784e-01, -2.1643e-01,\n",
       "                      -1.1334e-01, -2.8149e-01, -2.1082e-01, -3.1958e-01, -3.0835e-01,\n",
       "                      -3.7720e-01, -1.9458e-01, -2.6660e-01, -1.6687e-01, -3.3911e-01,\n",
       "                      -1.5540e-01, -5.9912e-01, -2.8076e-01, -3.1030e-01, -4.4116e-01,\n",
       "                      -2.5391e-01, -1.8042e-01, -3.5620e-01, -1.8359e-01, -1.7493e-01,\n",
       "                      -2.7710e-01, -6.4014e-01, -1.8311e-01, -3.1250e-01, -2.6074e-01,\n",
       "                      -8.0505e-02, -3.1152e-01, -2.2852e-01, -8.9990e-01, -2.3938e-01,\n",
       "                      -2.1436e-01, -1.4539e-01, -1.4429e-01, -3.1055e-01,  6.2164e-02,\n",
       "                      -8.4570e-01, -2.2717e-01, -3.6841e-01, -4.4556e-02, -9.0869e-01,\n",
       "                      -1.4661e-01, -2.8540e-01, -7.0020e-01, -3.1763e-01, -3.0151e-01,\n",
       "                      -5.1318e-01, -2.5659e-01, -2.3743e-01, -2.9810e-01, -3.7012e-01,\n",
       "                      -2.9126e-01, -3.5986e-01, -1.7261e-01, -3.0078e-01, -4.3140e-01,\n",
       "                      -3.8525e-01, -2.4377e-01, -4.5044e-01, -4.5947e-01, -2.9663e-01,\n",
       "                      -4.3750e-01, -4.5337e-01, -2.8491e-01, -2.9199e-01, -1.5295e-01,\n",
       "                      -2.7734e-01, -9.5276e-02, -5.3925e-02, -2.4731e-01, -3.5254e-01,\n",
       "                      -2.2754e-01, -9.0149e-02, -8.4180e-01, -3.9038e-01, -2.0447e-01,\n",
       "                      -2.3718e-01, -4.5142e-01, -7.3389e-01, -2.2534e-01, -1.3220e-01,\n",
       "                      -2.4463e-01, -2.0129e-01, -4.7021e-01, -3.1323e-01, -1.5955e-01,\n",
       "                      -1.7505e-01, -1.0895e-01, -2.7246e-01, -2.6611e-01, -6.7480e-01,\n",
       "                      -4.8004e-02, -6.3477e-01, -3.7476e-01, -3.6719e-01, -3.5034e-01,\n",
       "                      -1.9592e-01, -3.2275e-01, -2.5610e-01, -3.1079e-01, -4.1504e-01,\n",
       "                      -1.5454e-01, -4.4995e-01, -6.5576e-01, -1.5442e-01, -1.6357e-01,\n",
       "                      -1.7664e-01, -1.7554e-01, -7.9785e-01, -6.6455e-01, -3.8062e-01,\n",
       "                      -3.4888e-01, -3.2178e-01, -1.8677e-01, -4.5068e-01, -4.2383e-01,\n",
       "                      -2.3596e-01, -2.4368e-02, -3.4961e-01, -1.5808e-01, -4.2529e-01,\n",
       "                      -2.5665e-02, -2.5024e-01, -4.2700e-01, -2.3853e-01, -1.8689e-01,\n",
       "                      -4.8511e-01, -2.2864e-01, -4.3042e-01, -3.5400e-01, -2.1458e-03,\n",
       "                      -1.3647e-01, -3.4717e-01, -8.1970e-02, -2.4585e-01, -2.2070e-01,\n",
       "                      -2.6050e-01, -4.6326e-02, -4.6289e-01, -2.5439e-01, -2.5171e-01,\n",
       "                       9.3750e-02, -4.1321e-02, -2.7881e-01, -2.8979e-01, -9.9414e-01,\n",
       "                      -4.8584e-01, -3.1665e-01, -3.2812e-01, -1.8677e-01, -3.8794e-01,\n",
       "                      -3.2568e-01, -1.2000e-01,  7.8918e-02, -2.4170e-01, -3.9771e-01,\n",
       "                      -2.5049e-01,  3.2440e-02, -1.3879e-01, -2.6538e-01, -1.8542e-01,\n",
       "                      -2.4268e-01, -4.2285e-01, -1.0834e-01, -9.9487e-02, -3.0762e-01,\n",
       "                      -1.7041e-01, -4.9072e-02, -1.4075e-01, -3.0103e-01, -8.0627e-02,\n",
       "                      -2.1326e-01, -4.2603e-01, -8.1482e-02, -6.6711e-02, -2.2424e-01,\n",
       "                      -4.6094e-01, -7.6904e-01, -2.6074e-01, -1.9397e-01, -2.6050e-01,\n",
       "                      -4.7803e-01, -2.2375e-01, -2.2400e-01, -3.8892e-01, -5.7666e-01,\n",
       "                      -3.1079e-01, -1.3843e-01, -6.9580e-02, -1.8713e-01, -4.0527e-01,\n",
       "                      -1.6821e-01, -2.7075e-01, -3.1567e-01, -1.8286e-01, -2.8418e-01,\n",
       "                      -7.0020e-01, -3.1738e-01, -1.1650e+00, -8.8196e-02, -9.2041e-02,\n",
       "                      -3.8916e-01,  1.2469e-01, -1.7615e-01, -2.3157e-01, -2.8101e-01,\n",
       "                      -4.4287e-01, -2.0227e-01, -1.9080e-01, -3.4497e-01, -1.4514e-01,\n",
       "                      -3.4473e-01, -8.5938e-02, -2.2229e-01, -3.1470e-01, -1.6260e-01,\n",
       "                      -1.7725e-01, -3.5474e-01, -2.1545e-01, -1.5918e-01, -3.0176e-01,\n",
       "                      -3.2788e-01, -7.0215e-01, -4.5142e-01, -1.1298e-01, -1.8091e-01,\n",
       "                      -1.8225e-01, -3.1738e-01, -1.8396e-01, -1.1273e-01, -2.8442e-01,\n",
       "                      -4.0100e-02, -2.4512e-01, -1.9629e-01, -3.5742e-01, -1.9080e-01,\n",
       "                      -5.1709e-01,  5.2704e-02, -3.1567e-01, -2.4597e-01, -1.5210e-01,\n",
       "                      -4.0747e-01, -3.5107e-01], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.3.conv.weight',\n",
       "              tensor([[[ 1.3147e-01,  8.8440e-02,  9.1858e-02],\n",
       "                       [ 1.1438e-01,  1.2207e-01,  8.4351e-02],\n",
       "                       [-2.8516e-01, -3.0151e-01, -1.5527e-01],\n",
       "                       ...,\n",
       "                       [-2.2986e-01, -2.8491e-01, -2.9541e-01],\n",
       "                       [-3.6835e-02, -3.0914e-02, -6.7627e-02],\n",
       "                       [-1.4771e-01, -4.5746e-02, -2.8198e-01]],\n",
       "              \n",
       "                      [[-2.1765e-01, -4.1595e-02, -2.7847e-02],\n",
       "                       [-3.6285e-02, -4.1504e-02,  7.3486e-02],\n",
       "                       [-1.1700e-01, -1.4313e-02,  2.1271e-02],\n",
       "                       ...,\n",
       "                       [ 1.9730e-02, -3.6835e-02,  9.3994e-02],\n",
       "                       [ 1.4587e-01,  1.5381e-01,  5.5756e-02],\n",
       "                       [-6.3843e-02, -1.6296e-02, -3.1860e-02]],\n",
       "              \n",
       "                      [[-1.2329e-01, -1.3171e-01, -1.1395e-01],\n",
       "                       [-5.0110e-02, -8.8257e-02,  1.0889e-01],\n",
       "                       [-5.3833e-02, -1.0233e-03,  5.3345e-02],\n",
       "                       ...,\n",
       "                       [ 1.1711e-02,  1.0193e-01,  2.4829e-01],\n",
       "                       [ 4.0955e-02,  1.1017e-02, -3.4580e-03],\n",
       "                       [-1.3757e-01, -5.0079e-02,  4.1901e-02]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-1.1102e-01, -1.1682e-01, -1.5369e-01],\n",
       "                       [ 1.3684e-01,  7.1716e-02,  1.2537e-01],\n",
       "                       [ 5.0110e-02,  1.6077e-01,  1.1353e-01],\n",
       "                       ...,\n",
       "                       [ 3.9111e-01,  3.4692e-01,  4.0405e-01],\n",
       "                       [ 3.3402e-04, -5.2948e-02, -2.8458e-02],\n",
       "                       [ 1.8896e-01,  1.8604e-01,  9.4116e-02]],\n",
       "              \n",
       "                      [[ 2.6196e-01,  3.1464e-02, -3.8910e-02],\n",
       "                       [ 1.1267e-01,  2.8540e-01,  2.9126e-01],\n",
       "                       [-1.4038e-01, -1.8811e-01, -2.4158e-01],\n",
       "                       ...,\n",
       "                       [-1.0223e-02, -8.7891e-03, -3.6106e-03],\n",
       "                       [ 3.1616e-02, -4.0283e-03,  4.8767e-02],\n",
       "                       [-3.3936e-01, -2.8491e-01, -2.8711e-01]],\n",
       "              \n",
       "                      [[-1.9385e-01, -2.0142e-01, -2.0520e-01],\n",
       "                       [-1.2891e-01, -5.8289e-02,  5.2856e-02],\n",
       "                       [ 4.9286e-02,  3.2837e-02, -2.8778e-02],\n",
       "                       ...,\n",
       "                       [-1.2189e-01,  2.1094e-01,  5.6641e-01],\n",
       "                       [ 8.0994e-02,  6.3599e-02,  6.2683e-02],\n",
       "                       [ 1.5457e-02, -5.0262e-02, -7.3242e-02]]], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.3.conv.bias',\n",
       "              tensor([ 7.0374e-02,  2.4078e-02,  1.2292e-01, -1.2794e-02,  2.8564e-01,\n",
       "                       9.5154e-02,  8.1055e-02, -3.9062e-02,  2.5040e-02,  1.3110e-01,\n",
       "                      -2.0251e-01, -5.4321e-02,  1.4514e-01,  3.6426e-01, -1.1975e-01,\n",
       "                       5.0110e-02, -7.2784e-03, -5.4504e-02,  1.5576e-01, -1.6937e-02,\n",
       "                       1.3293e-01,  6.9458e-02,  4.4342e-02,  6.0913e-02,  1.3757e-01,\n",
       "                      -5.9662e-02,  1.2415e-01,  2.5696e-02, -7.2021e-03,  7.1838e-02,\n",
       "                       2.5284e-02,  2.2485e-01,  1.3989e-01,  7.4341e-02, -1.6882e-01,\n",
       "                      -9.7900e-02,  4.3396e-02,  1.0315e-01,  3.2501e-02,  9.5581e-02,\n",
       "                       9.1492e-02, -1.6541e-01,  1.5100e-01,  1.2024e-01,  2.6245e-01,\n",
       "                       7.1106e-02,  4.3018e-01, -9.2468e-02,  1.2646e-01,  2.4182e-01,\n",
       "                       1.4026e-01,  8.7097e-02,  1.0089e-01,  4.4342e-02,  2.0557e-01,\n",
       "                       1.8237e-01, -5.6610e-02,  1.8066e-01,  1.9562e-02, -9.6069e-02,\n",
       "                       4.4604e-01,  3.1201e-01,  3.0981e-01, -2.8442e-02,  2.8854e-02,\n",
       "                      -2.9434e-02,  7.7881e-02, -5.5939e-02, -1.2671e-01,  4.4220e-02,\n",
       "                      -1.5137e-01,  2.7832e-01,  2.4231e-01,  1.9885e-01, -5.2765e-02,\n",
       "                      -9.3750e-02,  1.5698e-01,  1.7053e-01,  7.9956e-02,  7.5439e-02,\n",
       "                      -4.6234e-02,  3.4058e-01,  3.3057e-01,  9.5154e-02,  1.1078e-01,\n",
       "                      -1.8753e-02,  5.8685e-02,  5.1788e-02,  7.2937e-02,  1.0046e-01,\n",
       "                       9.0515e-02, -3.5187e-02,  2.1619e-01, -2.1667e-02,  9.5825e-02,\n",
       "                      -3.2444e-03,  2.0752e-01,  6.3477e-03,  1.4664e-02,  2.9434e-02,\n",
       "                       8.2947e-02,  5.6854e-02,  2.2919e-02, -7.1777e-02,  1.2469e-01,\n",
       "                       1.0077e-01,  7.0992e-03, -2.3071e-02,  2.4988e-01,  6.7787e-03,\n",
       "                       6.9336e-02,  1.7249e-01,  2.7661e-01,  2.1851e-01,  1.8738e-01,\n",
       "                       9.8999e-02, -1.1548e-01,  1.6895e-01, -1.5186e-01, -6.3721e-02,\n",
       "                       8.1238e-02,  1.5808e-01,  8.7585e-02,  1.3489e-01,  5.4504e-02,\n",
       "                      -3.0746e-02, -6.1249e-02,  2.3364e-01, -3.3951e-03,  3.6865e-01,\n",
       "                      -1.3596e-02,  4.2053e-02,  3.7506e-02, -3.2768e-03,  7.1289e-02,\n",
       "                       2.6782e-01,  6.4697e-02, -1.3696e-01,  1.7493e-01, -1.6125e-01,\n",
       "                       1.9019e-01, -6.4331e-02, -3.5065e-02,  3.2397e-01,  1.9312e-01,\n",
       "                       1.1978e-02,  1.0217e-01,  5.4779e-03, -1.9119e-02,  7.4463e-02,\n",
       "                       1.7603e-01,  4.0436e-02, -5.5389e-02,  8.9645e-04,  1.0858e-01,\n",
       "                       2.6947e-02,  1.4114e-02,  9.9548e-02,  1.9531e-02, -3.9215e-03,\n",
       "                       9.7168e-02,  1.7456e-01,  1.7261e-01,  3.4637e-02, -3.1174e-02,\n",
       "                       1.3977e-01,  3.1177e-01,  3.4302e-02,  4.2206e-02,  7.4158e-02,\n",
       "                       4.5319e-02,  1.4771e-01, -1.3123e-02,  1.8884e-01,  1.1053e-01,\n",
       "                       2.5610e-01,  3.8116e-02, -6.1432e-02,  3.0835e-01,  8.9233e-02,\n",
       "                       6.3232e-02,  1.5466e-01,  2.3193e-01, -3.9124e-02, -4.3945e-02,\n",
       "                       1.5625e-02,  1.1951e-01,  2.0142e-02,  9.8572e-02,  6.3538e-02,\n",
       "                       1.0913e-01,  4.8218e-02,  2.5708e-01,  2.9395e-01,  2.6465e-01,\n",
       "                      -4.9561e-02,  1.3745e-01,  1.3794e-01,  2.0325e-01, -9.6130e-03,\n",
       "                       1.0376e-01,  1.0595e-03,  1.9519e-01,  1.1157e-01,  2.8442e-01,\n",
       "                       5.8563e-02,  1.8115e-01,  5.4474e-02, -2.0020e-02,  8.6487e-02,\n",
       "                       9.8083e-02,  2.1729e-01,  1.7529e-01,  3.0563e-02, -4.0070e-02,\n",
       "                       6.0699e-02,  1.4641e-02,  1.1456e-01,  1.1646e-01,  7.6355e-02,\n",
       "                      -6.4575e-02,  9.5520e-02,  2.8149e-01,  1.6748e-01,  3.0078e-01,\n",
       "                       1.0931e-01,  4.7089e-02,  6.4758e-02, -1.4502e-01,  4.8999e-01,\n",
       "                      -1.1133e-01, -3.9864e-03,  1.2268e-02,  3.9612e-02,  1.7773e-01,\n",
       "                       2.4048e-02,  1.9861e-01, -5.8136e-02, -1.8640e-01,  1.2680e-02,\n",
       "                       3.3569e-01, -1.5430e-01,  2.6260e-02, -1.0449e-01,  1.8298e-01,\n",
       "                      -5.3497e-02,  2.0300e-01,  1.6516e-01,  1.5186e-01,  9.0393e-02,\n",
       "                       2.5781e-01,  1.8066e-02, -8.3069e-02, -7.9880e-03, -5.2429e-02,\n",
       "                       2.5659e-01,  4.8248e-02,  2.5732e-01,  4.8431e-02,  8.1482e-02,\n",
       "                      -3.8025e-02,  3.6450e-01, -1.5967e-01, -6.0120e-02,  2.3376e-01,\n",
       "                       1.0553e-01,  1.5976e-02,  1.9531e-02,  1.2018e-01,  2.6465e-01,\n",
       "                       6.4880e-02,  1.0382e-01,  2.5909e-02,  3.5919e-02,  2.5537e-01,\n",
       "                       1.3220e-01,  2.4597e-01,  1.8567e-01,  2.1985e-01, -5.3009e-02,\n",
       "                       3.7109e-02,  2.9602e-02,  7.6355e-02,  1.7883e-01,  3.8513e-02,\n",
       "                      -4.3701e-02,  1.7542e-01,  2.8247e-01,  1.0370e-01,  3.1396e-01,\n",
       "                       6.1890e-02,  8.9661e-02, -2.4548e-01,  9.1553e-03,  2.1558e-01,\n",
       "                       1.9385e-01,  2.1790e-02,  2.6392e-01, -1.4136e-01, -1.5373e-02,\n",
       "                       1.4124e-01, -3.2776e-02,  7.0572e-03,  3.6774e-02, -4.8065e-02,\n",
       "                       2.0862e-01,  1.2927e-01,  4.8218e-02, -2.9968e-02,  3.1372e-01,\n",
       "                       1.2177e-01,  8.5449e-02, -1.1749e-03,  1.0852e-01,  4.2480e-02,\n",
       "                      -1.6882e-01,  7.0524e-04,  1.5045e-02,  4.0308e-01,  1.9788e-01,\n",
       "                       1.3208e-01,  1.0483e-02,  1.0333e-01,  1.5686e-01,  1.9751e-01,\n",
       "                       9.1614e-02,  2.7542e-02,  2.1591e-03, -8.5999e-02,  5.6702e-02,\n",
       "                       9.1553e-02,  2.0740e-01,  1.1322e-01,  1.2646e-01,  1.7407e-01,\n",
       "                       6.3049e-02,  2.1130e-01,  3.0420e-01,  3.5010e-01,  1.8396e-01,\n",
       "                       8.5876e-02, -1.3184e-01,  1.8445e-01,  2.3083e-01,  7.3608e-02,\n",
       "                       1.9531e-01,  8.0933e-02, -1.4282e-02,  2.0850e-01,  1.8958e-01,\n",
       "                       4.6387e-02,  1.5100e-01,  1.1487e-01,  6.8245e-03,  2.2308e-02,\n",
       "                      -1.4941e-01,  8.1970e-02,  2.9694e-02,  2.6343e-01,  4.2847e-01,\n",
       "                      -3.7262e-02, -1.5793e-02,  2.0801e-01,  2.5864e-02,  4.2572e-02,\n",
       "                      -4.8462e-02, -3.0853e-02,  1.4954e-01, -3.8055e-02,  2.8271e-01,\n",
       "                       4.4495e-02, -7.2021e-02, -2.3392e-02,  3.2406e-03,  2.4207e-01,\n",
       "                      -4.7638e-02,  1.7285e-01, -3.4912e-02, -8.9741e-04, -5.2094e-02,\n",
       "                       3.0737e-01,  2.0093e-01,  3.1681e-03,  1.8787e-01,  2.5665e-02,\n",
       "                       1.2016e-02,  1.6138e-01,  5.5145e-02,  6.9153e-02, -1.1377e-01,\n",
       "                       4.0253e-02,  8.4076e-03,  6.3184e-01,  2.0544e-01,  1.0345e-01,\n",
       "                       2.7441e-01, -4.5258e-02, -4.7943e-02, -4.9805e-02, -6.8115e-02,\n",
       "                       9.4299e-02,  2.3376e-01,  2.0096e-02,  9.6069e-02, -8.9478e-02,\n",
       "                       1.4978e-01, -2.0416e-02, -3.2007e-01, -1.3269e-01,  3.1738e-02,\n",
       "                       7.7698e-02, -1.5259e-01,  1.7688e-01, -1.7383e-01, -1.1383e-01,\n",
       "                      -8.6243e-02, -3.7537e-02,  3.0347e-01,  1.7065e-01,  6.6284e-02,\n",
       "                       1.4258e-01,  5.5603e-02,  1.3684e-01, -6.4148e-02,  1.7444e-01,\n",
       "                       1.9934e-01, -3.3630e-02, -1.1505e-02,  1.3257e-01,  1.4453e-01,\n",
       "                       3.2318e-02,  2.9541e-02, -9.1675e-02,  1.2329e-01, -5.6305e-02,\n",
       "                       3.0566e-01,  3.0884e-01,  3.4888e-01,  3.5076e-03,  3.2593e-02,\n",
       "                       8.4656e-02, -2.4933e-02,  5.1880e-02,  3.7842e-02, -3.7155e-03,\n",
       "                       1.1743e-01,  7.2205e-02,  3.5498e-01,  2.5366e-01,  3.9795e-02,\n",
       "                       2.0435e-01,  2.3689e-03,  3.8971e-02, -5.2399e-02,  4.2389e-02,\n",
       "                       2.0898e-01, -4.4891e-02,  2.4304e-01,  1.0333e-01,  2.4902e-01,\n",
       "                       1.8396e-01,  3.6670e-01,  1.3831e-01, -2.1576e-02, -3.6678e-03,\n",
       "                      -3.1219e-02,  8.3740e-02,  1.8250e-01,  6.9031e-02,  4.7638e-02,\n",
       "                      -1.3879e-01,  8.0627e-02,  1.9006e-01,  2.4438e-01,  1.0413e-01,\n",
       "                      -1.0124e-02,  1.1230e-01, -9.2590e-02, -3.9001e-02,  2.1204e-01,\n",
       "                       2.6270e-01,  1.3208e-01,  2.7930e-01, -1.1124e-02,  2.4841e-01,\n",
       "                       2.7496e-02,  9.5947e-02,  3.5980e-02,  3.8940e-02, -2.7664e-02,\n",
       "                       7.7026e-02,  1.6309e-01,  7.0850e-01, -1.6272e-01,  8.3679e-02,\n",
       "                       2.0801e-01,  1.9169e-03,  1.5100e-01, -9.3140e-02,  4.0710e-02,\n",
       "                      -2.8595e-02,  9.2285e-02,  1.6022e-02, -2.3041e-02,  1.8933e-01,\n",
       "                      -1.2262e-01,  3.1555e-02,  6.8779e-03, -6.9641e-02,  1.5271e-01,\n",
       "                       1.0175e-01, -5.4382e-02], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.3.layer_norm.weight',\n",
       "              tensor([0.5840, 0.6870, 0.3955, 0.4963, 0.3679, 0.4226, 0.7583, 0.7490, 0.7183,\n",
       "                      0.3723, 0.8921, 0.5752, 0.6587, 0.1932, 1.2275, 1.0859, 1.0449, 0.5601,\n",
       "                      0.6436, 0.9844, 0.7983, 0.7290, 0.7715, 0.7388, 0.3953, 0.6973, 0.5786,\n",
       "                      0.9551, 0.6045, 0.5928, 0.9507, 0.7388, 0.3503, 1.0381, 1.3926, 0.6323,\n",
       "                      1.2432, 0.4658, 0.4822, 0.9897, 0.6592, 0.8774, 0.5073, 0.5312, 0.3376,\n",
       "                      0.5308, 0.3208, 0.9653, 0.6187, 0.4172, 0.7388, 0.7554, 0.7925, 0.8755,\n",
       "                      0.3831, 0.4829, 0.8657, 0.4675, 0.5317, 0.7510, 0.1938, 0.2720, 0.3096,\n",
       "                      1.4473, 0.7910, 0.7261, 0.3965, 0.9956, 0.7490, 0.4490, 1.2666, 0.3162,\n",
       "                      0.2937, 0.6689, 0.8574, 0.7627, 0.3423, 0.4961, 0.7529, 0.4216, 1.0156,\n",
       "                      0.2455, 0.3521, 0.7407, 0.8154, 0.7520, 0.5625, 0.6084, 0.7432, 0.3457,\n",
       "                      0.7544, 1.3389, 0.4458, 0.9238, 0.8696, 0.5425, 0.3398, 0.6919, 0.7988,\n",
       "                      0.9663, 1.4072, 0.7544, 0.7314, 0.6104, 0.6924, 1.2334, 0.9565, 0.8774,\n",
       "                      0.4172, 0.8945, 0.4180, 0.4861, 0.4375, 0.5049, 0.4128, 0.8462, 1.3906,\n",
       "                      0.4014, 0.9551, 0.6694, 0.9370, 0.5889, 0.6519, 0.4246, 0.8203, 0.8677,\n",
       "                      0.7295, 0.6641, 0.8633, 0.2727, 0.9741, 0.6914, 0.6792, 0.5112, 0.4546,\n",
       "                      0.3345, 1.0693, 1.2822, 0.4456, 0.7905, 0.4023, 0.6504, 0.6772, 0.2473,\n",
       "                      0.3938, 0.6885, 0.4846, 0.6099, 0.4912, 0.9883, 0.7065, 0.8887, 1.1152,\n",
       "                      0.9424, 0.9526, 0.5425, 0.8262, 0.6284, 0.7681, 0.5674, 0.6338, 0.8418,\n",
       "                      0.3945, 1.0615, 0.9355, 0.7139, 0.2534, 0.5649, 0.4573, 0.5020, 0.7114,\n",
       "                      0.6831, 0.6138, 0.8325, 0.7144, 0.8081, 0.7178, 0.4866, 0.4653, 0.5229,\n",
       "                      0.5884, 0.6816, 1.0303, 0.5127, 0.5903, 0.9336, 0.4866, 0.7993, 0.5479,\n",
       "                      0.5859, 0.6738, 0.9985, 0.4119, 0.3657, 0.2937, 1.0684, 0.6167, 0.5664,\n",
       "                      0.4734, 0.6362, 0.7671, 0.4993, 0.3286, 0.4341, 0.5962, 0.5752, 0.4429,\n",
       "                      0.6436, 0.9458, 0.5024, 0.7905, 0.4563, 0.4583, 0.5918, 1.0342, 0.8901,\n",
       "                      0.8818, 0.7993, 0.6899, 0.4421, 0.7852, 0.6548, 0.2656, 0.7271, 0.2893,\n",
       "                      0.8442, 0.8960, 0.5752, 1.5439, 0.1890, 1.1201, 0.7178, 0.7637, 0.6123,\n",
       "                      0.7393, 0.4790, 0.5488, 0.4412, 0.9443, 0.5269, 0.3342, 1.0908, 0.4841,\n",
       "                      0.8589, 0.4465, 1.3564, 0.6030, 0.3486, 0.4307, 0.4910, 0.3679, 0.6758,\n",
       "                      0.7061, 0.5767, 0.9463, 0.3701, 0.9429, 0.3979, 0.8105, 0.7192, 0.8052,\n",
       "                      0.2362, 1.1855, 0.6104, 0.4902, 0.3752, 0.4790, 0.8296, 0.6353, 0.3928,\n",
       "                      0.7603, 0.3503, 0.6284, 0.7808, 0.3350, 0.3843, 0.3213, 0.7837, 0.4241,\n",
       "                      0.8633, 0.7837, 0.8096, 1.0127, 0.4165, 0.9902, 1.0312, 0.5581, 0.3262,\n",
       "                      0.4851, 0.2554, 0.7451, 0.5801, 0.9907, 0.5322, 0.3394, 0.3435, 1.0342,\n",
       "                      0.3601, 0.7622, 0.9336, 0.4468, 0.7422, 0.6963, 0.5459, 0.7324, 0.6064,\n",
       "                      0.3804, 0.8213, 0.9785, 0.2920, 0.7661, 0.7173, 0.5850, 0.7202, 0.5713,\n",
       "                      1.3623, 0.8047, 0.8496, 0.2908, 0.7407, 0.4111, 0.7900, 0.7324, 0.5264,\n",
       "                      0.3254, 0.4744, 0.7051, 0.6045, 1.0127, 0.4502, 0.7329, 0.4312, 0.5972,\n",
       "                      0.4226, 0.4275, 0.5337, 0.5923, 0.2722, 0.3420, 0.2732, 0.8237, 0.6714,\n",
       "                      0.3784, 0.3621, 0.8013, 1.0225, 0.9517, 0.6333, 0.3242, 0.5435, 0.5786,\n",
       "                      0.3455, 0.6367, 0.7783, 0.7412, 1.2988, 0.6187, 1.0254, 0.3481, 0.2260,\n",
       "                      1.0762, 0.6719, 0.7378, 0.5298, 0.4521, 0.8560, 0.6797, 0.5771, 1.0625,\n",
       "                      0.3542, 0.6064, 0.8608, 0.8901, 0.6743, 0.8047, 0.6724, 0.5957, 0.8076,\n",
       "                      0.5815, 0.8091, 0.2534, 0.4712, 0.7656, 0.4570, 0.9043, 0.8110, 0.7510,\n",
       "                      0.4922, 0.4636, 0.7256, 0.4365, 0.5078, 0.2039, 0.3101, 0.8242, 0.2561,\n",
       "                      1.2236, 1.0332, 0.8423, 0.5835, 0.5532, 0.3516, 0.8267, 0.5698, 0.7686,\n",
       "                      0.4663, 0.9751, 1.7324, 1.2041, 0.9116, 0.5317, 1.3447, 0.5249, 1.4199,\n",
       "                      1.0273, 1.0605, 0.8257, 0.2871, 0.7861, 0.6812, 0.3977, 0.8750, 0.7979,\n",
       "                      1.0068, 0.4067, 0.3691, 0.7969, 0.4585, 0.6626, 0.4724, 1.0947, 1.0918,\n",
       "                      1.4062, 0.3447, 0.8438, 0.2368, 0.3899, 0.3379, 0.9028, 0.7881, 0.7769,\n",
       "                      0.8823, 0.6938, 0.6611, 0.9395, 0.7290, 0.9985, 0.3142, 0.4224, 0.6304,\n",
       "                      0.4031, 0.8428, 0.4539, 0.6050, 0.4917, 0.3699, 1.0195, 0.3438, 0.4158,\n",
       "                      0.3391, 0.3701, 0.2688, 0.3613, 0.7617, 0.5991, 0.8975, 0.4270, 0.7212,\n",
       "                      0.5239, 0.4612, 0.8037, 0.4436, 0.8491, 0.3892, 0.5015, 0.5391, 0.7935,\n",
       "                      1.0117, 0.8545, 0.3940, 0.3875, 0.4001, 0.2571, 0.5996, 0.3228, 0.5854,\n",
       "                      0.3970, 0.4050, 0.6660, 1.1172, 0.5962, 0.6729, 0.0446, 1.4639, 0.7539,\n",
       "                      0.4844, 0.7520, 0.6982, 0.5469, 0.7925, 0.7349, 0.7207, 0.9365, 1.0645,\n",
       "                      0.6558, 0.9409, 0.8325, 0.8530, 1.0732, 0.5205, 0.6558, 0.5859],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.3.layer_norm.bias',\n",
       "              tensor([-0.2830, -0.1367, -0.0483, -0.0236, -0.3149,  0.1836, -0.2632, -0.2539,\n",
       "                      -0.2031, -0.1543, -0.2126,  0.1937, -0.3237, -0.2001, -0.4934, -0.4631,\n",
       "                      -0.1652, -0.2450, -0.3770, -0.7144, -0.4551, -0.3337, -0.2155, -0.3418,\n",
       "                      -0.1466, -0.3235, -0.3159, -0.6274, -0.1809, -0.3962, -0.1449, -0.2178,\n",
       "                      -0.1631, -0.3489, -0.3140, -0.0219, -0.6919, -0.0220, -0.2103, -0.3418,\n",
       "                      -0.2852, -0.2242, -0.2460, -0.1305, -0.1729, -0.0943, -0.2551, -0.3113,\n",
       "                      -0.2915, -0.1915, -0.5366, -0.3223, -0.1495, -0.3569,  0.0863, -0.1982,\n",
       "                      -0.2666, -0.0959,  0.0778, -0.1433, -0.3232, -0.2402, -0.2213,  0.0129,\n",
       "                      -0.0948, -0.0791, -0.1544, -0.6592, -0.1284,  0.1179, -0.8896,  0.1153,\n",
       "                      -0.2590, -0.2271, -0.1670, -0.1709, -0.1782, -0.2269, -0.1428, -0.2153,\n",
       "                      -0.4475, -0.2419, -0.2646, -0.1964, -0.2749, -0.3218, -0.2771, -0.2832,\n",
       "                      -0.1372, -0.1133, -0.2289, -0.6035, -0.2023, -0.4045, -0.3923, -0.1986,\n",
       "                      -0.2373, -0.1071, -0.2327, -0.3865, -0.8389, -0.3547, -0.3091, -0.3450,\n",
       "                      -0.1150, -0.4792, -0.4822, -0.4744, -0.1641, -0.5483, -0.2498, -0.2281,\n",
       "                      -0.2305, -0.2539, -0.0940, -0.3425, -0.2339, -0.2629, -0.2355, -0.1509,\n",
       "                      -0.4272, -0.2375, -0.3354, -0.1744, -0.1691, -0.3787, -0.2012, -0.3701,\n",
       "                      -0.4988, -0.2778, -0.3784, -0.3049, -0.2489, -0.6807, -0.0584, -0.2749,\n",
       "                      -0.4155, -0.0983, -0.2510, -0.1242, -0.0951, -0.2236, -0.2102, -0.1746,\n",
       "                      -0.2812, -0.1429, -0.2168, -0.2128, -0.1044, -0.3999, -0.1886, -0.7603,\n",
       "                      -0.4741, -0.4976, -0.4417, -0.1299, -0.4778, -0.3379, -0.3391, -0.2715,\n",
       "                      -0.5015, -0.3037, -0.1891, -0.3186, -0.7070, -0.4041, -0.2510, -0.2460,\n",
       "                       0.0453, -0.1152, -0.1749, -0.4036, -0.1213, -0.1026, -0.1841, -0.0757,\n",
       "                      -0.1580,  0.0852, -0.3616, -0.2510, -0.3628, -0.3718, -0.4260, -0.2617,\n",
       "                      -0.0709, -0.5205, -0.1891, -0.3276, -0.0827, -0.2014, -0.1588, -0.4443,\n",
       "                      -0.2499, -0.0774, -0.0745, -0.7573, -0.1129, -0.4702, -0.3025, -0.2861,\n",
       "                      -0.1898, -0.1290, -0.2346, -0.0850, -0.4004, -0.1064, -0.3330, -0.1340,\n",
       "                      -0.5361, -0.2094, -0.3323, -0.2849, -0.2571, -0.4878, -0.2566, -0.3579,\n",
       "                      -0.3413, -0.1921, -0.3384, -0.1284, -0.2585, -0.1755, -0.2527, -0.2028,\n",
       "                       0.0087, -0.4102, -0.2421, -0.1902, -0.0521, -0.3274, -0.1223, -0.2832,\n",
       "                      -0.2125, -0.1415, -0.5137, -0.1562, -0.3088, -0.1368, -0.1377, -0.0909,\n",
       "                      -0.2201, -0.5278, -0.0267, -0.3083, -0.2191, -0.9136, -0.2275, -0.0911,\n",
       "                       0.0248, -0.1973, -0.0926, -0.1261, -0.1573, -0.4509, -0.4412, -0.2148,\n",
       "                      -0.7319, -0.2532, -0.4150, -0.2017, -0.3640, -0.3291, -0.4131, -0.2338,\n",
       "                       0.2317, -0.1813, -0.4414, -0.2805, -0.3000, -0.1929, -0.1912, -0.1665,\n",
       "                      -0.2622, -0.2561, -0.2253, -0.2695, -0.1096, -0.1049, -0.2046, -0.4756,\n",
       "                      -0.4229, -0.5767, -0.3225, -0.1660, -0.6372, -0.4875, -0.0718, -0.1713,\n",
       "                      -0.1952, -0.2361, -0.0850, -0.4714, -0.2130, -0.3728, -0.1304, -0.1112,\n",
       "                      -0.6362, -0.0205, -0.3516, -0.5391, -0.2291, -0.1121, -0.4424, -0.1157,\n",
       "                      -0.1058, -0.1168, -0.1733, -0.3374, -0.4148, -0.2766, -0.3501, -0.3110,\n",
       "                      -0.0858, -0.2952, -0.0602, -0.3201, -0.1510, -0.3491, -0.2312, -0.3164,\n",
       "                       0.1650, -0.6455, -0.3567,  0.2299, -0.2654, -0.2632, -0.1803, -0.2032,\n",
       "                      -0.6084, -0.3098, -0.5039, -0.0310, -0.2751, -0.1851,  0.0099, -0.4971,\n",
       "                      -0.1819, -0.1376, -0.1926, -0.2106, -0.3652, -0.0184, -0.1681, -0.0328,\n",
       "                      -0.3044, -0.4014, -0.7339, -0.5688, -0.2006, -0.1105, -0.1815, -0.0402,\n",
       "                      -0.2456, -0.4438, -0.2496, -0.2573, -0.2445, -0.4492, -0.1947, -0.2372,\n",
       "                      -0.3367, -0.0971, -0.3279,  0.0053, -0.0217, -0.3096, -0.3818, -0.1328,\n",
       "                      -0.3782, -0.3044, -0.2197, -0.4707, -0.2732, -0.2710, -0.2367, -0.4346,\n",
       "                      -0.3044, -0.2466, -0.1925, -0.4971, -0.2092, -0.2568, -0.3403, -0.0589,\n",
       "                      -0.5264, -0.4175, -0.3003, -0.1819, -0.2568, -0.2150, -0.2236, -0.1260,\n",
       "                      -0.3271, -0.0204, -0.3171, -0.0597, -0.6421, -0.5449, -0.2966,  0.2539,\n",
       "                       0.0626, -0.2637, -0.4578, -0.2654, -0.2678, -0.1703, -0.4722, -0.2438,\n",
       "                      -0.6826, -0.3464, -0.1317, -0.1405, -0.2964, -0.2256, -0.4060, -0.4277,\n",
       "                      -0.3005, -0.2274, -0.1608, -0.2015, -0.0103, -0.1221, -0.3232, -0.5234,\n",
       "                      -0.0767, -0.1442, -0.2954,  0.1067, -0.3254,  0.0494, -0.3762, -0.7402,\n",
       "                      -0.0235, -0.2397, -0.4170, -0.0543, -0.2651, -0.1820, -0.1312, -0.4065,\n",
       "                      -0.2637, -0.2844, -0.3748, -0.3779, -0.6304, -0.3572, -0.4448, -0.1595,\n",
       "                      -0.2883, -0.0759, -0.2717, -0.5938, -0.2119, -0.2273,  0.1831, -0.2522,\n",
       "                      -0.6138, -0.1958, -0.1083, -0.2412, -0.2177, -0.1777, -0.2223, -0.1696,\n",
       "                      -0.1498, -0.3730, -0.0793, -0.0722, -0.1377, -0.1760, -0.1819,  0.0608,\n",
       "                      -0.4285, -0.1188, -0.2419, -0.1220, -0.2856, -0.3203, -0.3228, -0.2233,\n",
       "                      -0.2966, -0.1573, -0.2566, -0.1096, -0.1902, -0.3706, -0.2190, -0.1172,\n",
       "                      -0.1930, -0.4214, -0.2261, -0.5674, -0.0739, -0.2117, -0.4124, -0.1531,\n",
       "                      -0.0011, -0.3083, -0.2087,  0.0653, -0.2800, -0.1637, -0.5234, -0.5371,\n",
       "                      -0.3384, -0.5483, -0.5068, -0.2734, -0.5947, -0.3701, -0.3901, -0.1571],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.4.conv.weight',\n",
       "              tensor([[[-3.0613e-04,  6.1188e-02,  1.1597e-02],\n",
       "                       [ 2.2668e-01,  3.2471e-01,  2.1521e-01],\n",
       "                       [ 8.7402e-02,  5.1147e-02,  1.7227e-02],\n",
       "                       ...,\n",
       "                       [-3.4882e-02, -3.6469e-02, -5.4054e-03],\n",
       "                       [ 8.2886e-02,  8.3374e-02,  2.1042e-02],\n",
       "                       [-1.6492e-01, -1.1841e-01, -7.7087e-02]],\n",
       "              \n",
       "                      [[-6.2469e-02, -1.2939e-02,  1.7609e-02],\n",
       "                       [ 1.1298e-01,  4.7363e-02,  1.1414e-01],\n",
       "                       [-9.1003e-02, -4.6906e-02, -1.0553e-01],\n",
       "                       ...,\n",
       "                       [-1.0870e-01, -5.9723e-02, -8.8806e-02],\n",
       "                       [-8.6182e-02, -1.3623e-01, -1.4197e-01],\n",
       "                       [ 7.2823e-03, -2.2461e-02, -8.0750e-02]],\n",
       "              \n",
       "                      [[ 4.5776e-02,  1.7444e-01,  5.6152e-02],\n",
       "                       [ 2.3758e-02,  7.3547e-02,  3.4332e-02],\n",
       "                       [-1.0339e-01, -1.3318e-01, -1.3574e-01],\n",
       "                       ...,\n",
       "                       [-1.9272e-02, -1.0193e-01, -4.6112e-02],\n",
       "                       [-6.7383e-02, -1.0236e-01, -1.8616e-01],\n",
       "                       [ 3.3142e-02,  3.1372e-02,  4.6356e-02]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-1.8604e-01, -2.5986e-02,  7.5684e-02],\n",
       "                       [ 2.7466e-02,  3.0945e-02, -4.3457e-02],\n",
       "                       [ 2.1460e-01, -1.1438e-01,  1.2878e-01],\n",
       "                       ...,\n",
       "                       [ 6.1157e-02, -8.6487e-02, -2.8442e-02],\n",
       "                       [-1.3403e-01, -3.5004e-02,  4.4800e-02],\n",
       "                       [-3.7140e-02,  5.4718e-02, -1.4519e-02]],\n",
       "              \n",
       "                      [[-1.0319e-03, -5.5023e-02, -7.8003e-02],\n",
       "                       [-9.1629e-03,  2.6352e-02,  6.7505e-02],\n",
       "                       [-5.9113e-02, -5.6190e-03,  2.3590e-02],\n",
       "                       ...,\n",
       "                       [ 8.5938e-02, -8.0444e-02, -3.3142e-02],\n",
       "                       [ 1.0236e-01,  5.4108e-02, -2.2998e-01],\n",
       "                       [ 6.9824e-02,  3.6192e-04,  1.2561e-01]],\n",
       "              \n",
       "                      [[ 3.3894e-03, -1.0284e-01,  4.7760e-02],\n",
       "                       [ 6.1859e-02,  6.0616e-03,  8.2520e-02],\n",
       "                       [-3.6896e-02, -5.9601e-02, -2.6215e-02],\n",
       "                       ...,\n",
       "                       [ 2.2491e-02, -2.1194e-02,  1.3098e-01],\n",
       "                       [-1.6907e-01,  6.0547e-02,  3.1948e-03],\n",
       "                       [-1.3379e-01, -4.1718e-02, -1.7358e-01]]], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.4.conv.bias',\n",
       "              tensor([-0.1340, -0.0091,  0.4941,  0.3403, -0.0213,  0.2808, -0.0520,  0.0105,\n",
       "                       0.0013,  0.1985,  0.2081, -0.1735,  0.1625,  0.1764,  0.0205,  0.0959,\n",
       "                       0.0999,  0.0406,  0.1581, -0.1064,  0.0087,  0.0104,  0.1299,  0.0046,\n",
       "                       0.1393,  0.0028, -0.0375,  0.0262,  0.3215, -0.0703,  0.0671,  0.1037,\n",
       "                       0.1516,  0.0424,  0.0439, -0.0105, -0.0830,  0.1127,  0.0130,  0.1243,\n",
       "                       0.1111,  0.2434,  0.1140, -0.1803, -0.0831, -0.0830, -0.0360, -0.0396,\n",
       "                       0.0846,  0.1687,  0.0927,  0.0661,  0.1088,  0.2996, -0.0057, -0.1132,\n",
       "                      -0.0398,  0.2739,  0.0487,  0.0546,  0.2776,  0.0444,  0.0584, -0.0899,\n",
       "                       0.0801,  0.1183,  0.1138, -0.0115, -0.1282,  0.0684,  0.2107, -0.0696,\n",
       "                      -0.0294, -0.0583, -0.0159,  0.0458,  0.1799, -0.0568,  0.1470,  0.0377,\n",
       "                      -0.1366,  0.1357, -0.0342,  0.3293,  0.1047,  0.0452,  0.0155,  0.0288,\n",
       "                       0.1246,  0.2517,  0.1150,  0.1039, -0.0458,  0.0890, -0.0867,  0.1471,\n",
       "                       0.0428,  0.3857,  0.1301,  0.0300,  0.0675,  0.1626,  0.0380,  0.0221,\n",
       "                       0.0597,  0.1888,  0.1233,  0.0401,  0.1355, -0.0052,  0.0764,  0.3323,\n",
       "                      -0.0041,  0.2788, -0.1586, -0.0373, -0.0956,  0.0418, -0.0819, -0.1134,\n",
       "                       0.0110,  0.2556, -0.0184, -0.0637,  0.0498, -0.0999,  0.0337,  0.0920,\n",
       "                       0.0196,  0.0389,  0.1609, -0.0154, -0.1780,  0.1703, -0.0433, -0.0812,\n",
       "                       0.0143,  0.0784, -0.1089,  0.1697, -0.1288, -0.0385,  0.0870,  0.0372,\n",
       "                      -0.0059,  0.1144, -0.2026, -0.0983,  0.1274,  0.0016,  0.1710, -0.0409,\n",
       "                       0.0483,  0.0021, -0.0255,  0.0193, -0.0443,  0.2208, -0.0389,  0.2861,\n",
       "                       0.0007,  0.1530, -0.0817,  0.0578, -0.0788,  0.1804,  0.0059,  0.1758,\n",
       "                       0.0900,  0.1024, -0.0388,  0.2395,  0.0215,  0.1237,  0.2402, -0.0172,\n",
       "                       0.1765,  0.1295,  0.1793, -0.0095,  0.1271,  0.0178,  0.3113, -0.0053,\n",
       "                      -0.0865,  0.0704, -0.1077, -0.1575,  0.0016,  0.0825, -0.0111, -0.0429,\n",
       "                      -0.0379,  0.3049,  0.1242,  0.1714,  0.1654,  0.0396, -0.0273,  0.0162,\n",
       "                       0.0139,  0.0544, -0.0186,  0.0020,  0.2021,  0.0023, -0.0223, -0.0582,\n",
       "                      -0.0416, -0.1682,  0.1299,  0.0920,  0.2578, -0.1062, -0.0925,  0.1749,\n",
       "                       0.0687,  0.0222,  0.1355,  0.2148, -0.1302,  0.0793,  0.0577,  0.1146,\n",
       "                      -0.0020,  0.0903, -0.2561,  0.0469, -0.0391,  0.1058,  0.2510,  0.0410,\n",
       "                       0.0024,  0.1793, -0.0132,  0.0380, -0.0428,  0.1561, -0.0581,  0.1144,\n",
       "                      -0.0318,  0.0471, -0.0790,  0.0452, -0.1099, -0.0138,  0.1477, -0.0103,\n",
       "                       0.0196, -0.1057,  0.4651, -0.0372,  0.1660,  0.1433, -0.0673, -0.2610,\n",
       "                      -0.0655,  0.0576,  0.1921,  0.0197,  0.1459,  0.0173,  0.0198, -0.0134,\n",
       "                       0.0919, -0.1043, -0.0187,  0.1277,  0.0643,  0.0151,  0.0303,  0.0524,\n",
       "                       0.0800,  0.3035,  0.0436,  0.0216, -0.0827,  0.2861,  0.1311, -0.0511,\n",
       "                      -0.0345,  0.1962,  0.1700,  0.3354,  0.1575,  0.2617,  0.0878,  0.0970,\n",
       "                       0.0853, -0.0515,  0.0265,  0.2318,  0.0423,  0.0287,  0.1014,  0.2891,\n",
       "                      -0.0536, -0.0220,  0.1556,  0.0148,  0.0738,  0.0192,  0.0172, -0.0131,\n",
       "                       0.2305,  0.1231,  0.0249, -0.1043,  0.3569, -0.0454,  0.1432,  0.1013,\n",
       "                       0.1514,  0.2639,  0.3816,  0.0107, -0.0449, -0.0434,  0.1162, -0.1438,\n",
       "                      -0.0365, -0.1399,  0.0045, -0.0757,  0.0556,  0.2983,  0.1731, -0.2345,\n",
       "                      -0.0582,  0.2321,  0.1459,  0.0512,  0.2141,  0.1082,  0.3704,  0.0331,\n",
       "                       0.0287,  0.1975,  0.1214,  0.0704,  0.0524, -0.1486, -0.0884,  0.0116,\n",
       "                       0.1519,  0.0510, -0.0069,  0.0359, -0.2059, -0.2051,  0.0995, -0.0900,\n",
       "                       0.0692,  0.0682,  0.1033,  0.2148, -0.0374,  0.3396, -0.0833,  0.1149,\n",
       "                      -0.0200,  0.0543,  0.1797,  0.0439,  0.1564, -0.0782,  0.1943,  0.0785,\n",
       "                       0.2130, -0.0509,  0.3752,  0.3076, -0.0498,  0.1351,  0.0483,  0.0269,\n",
       "                      -0.0916, -0.0534,  0.2178,  0.0466, -0.1583,  0.1489, -0.1312,  0.1749,\n",
       "                      -0.1023,  0.1125, -0.0226,  0.1809,  0.2430,  0.1042, -0.0354,  0.3101,\n",
       "                      -0.0094,  0.0799,  0.0929, -0.0863, -0.0313,  0.0033,  0.0597,  0.2294,\n",
       "                       0.1401,  0.2664, -0.0568, -0.0588, -0.0941, -0.1537,  0.1066,  0.1675,\n",
       "                       0.0679, -0.0728, -0.0846,  0.1260, -0.0140, -0.0079,  0.2238, -0.0035,\n",
       "                       0.1310,  0.1681, -0.0011,  0.2539,  0.0127, -0.0932,  0.0591,  0.1172,\n",
       "                      -0.0223, -0.0289,  0.3618,  0.1760, -0.0375,  0.0093, -0.0989, -0.0285,\n",
       "                       0.0820,  0.0176, -0.0611,  0.3530,  0.0391,  0.1946, -0.0855,  0.0845,\n",
       "                       0.0661, -0.0729, -0.0526,  0.1195,  0.0565,  0.0380, -0.0925, -0.2393,\n",
       "                       0.0736,  0.2174, -0.0656,  0.1709,  0.0443,  0.2754,  0.1981,  0.2188,\n",
       "                      -0.0488, -0.0566,  0.1292, -0.1487,  0.1959, -0.0485,  0.1271,  0.1461,\n",
       "                       0.2358,  0.1565, -0.0595,  0.0253,  0.2583, -0.0058,  0.0583,  0.1067,\n",
       "                       0.2441,  0.3999, -0.0985,  0.3110, -0.0759,  0.0475,  0.0098,  0.3147,\n",
       "                      -0.2908, -0.0667,  0.0322,  0.0744,  0.1272, -0.0215, -0.0033, -0.0477,\n",
       "                       0.0426,  0.2344,  0.1219,  0.0851,  0.0271, -0.0881, -0.0158, -0.0751,\n",
       "                       0.0784,  0.1636, -0.0450, -0.1937, -0.1608,  0.0969,  0.1631, -0.1230,\n",
       "                       0.2922,  0.0601,  0.1444,  0.0681,  0.0331,  0.1203,  0.0252,  0.1920],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.4.layer_norm.weight',\n",
       "              tensor([0.6206, 0.4829, 0.3208, 0.3118, 0.7598, 0.3921, 0.7993, 0.9736, 0.8447,\n",
       "                      0.4077, 0.3198, 0.8950, 0.4556, 0.3291, 0.5913, 0.4299, 0.3787, 0.5552,\n",
       "                      0.4858, 1.1934, 0.6729, 0.5151, 0.4490, 0.6035, 0.3186, 0.3179, 0.6606,\n",
       "                      0.6299, 0.2961, 0.3374, 0.5215, 0.6318, 0.3748, 0.5728, 0.3972, 0.6816,\n",
       "                      0.6362, 0.3699, 0.7134, 0.5366, 0.4705, 0.3816, 0.5469, 0.6504, 0.6406,\n",
       "                      0.7749, 0.7793, 0.7588, 0.4568, 0.4407, 0.3821, 0.6694, 0.4316, 0.2976,\n",
       "                      0.7188, 0.7222, 0.9209, 0.2776, 0.6064, 0.5132, 0.4082, 0.4888, 0.4260,\n",
       "                      0.5889, 0.6016, 0.5137, 0.3477, 0.9312, 1.0293, 0.4873, 0.3538, 0.4819,\n",
       "                      0.5562, 0.9741, 0.5444, 0.3877, 0.3291, 0.7114, 0.3245, 0.8276, 0.6504,\n",
       "                      0.5220, 0.4070, 0.3162, 0.5366, 1.1445, 0.5103, 0.5654, 0.6240, 0.3704,\n",
       "                      0.7144, 0.4087, 0.8110, 0.5967, 0.5718, 0.4333, 0.4775, 0.2568, 0.5205,\n",
       "                      0.4944, 0.7319, 0.4377, 0.4226, 0.8481, 0.7910, 0.4192, 0.4658, 0.7979,\n",
       "                      0.3645, 0.5708, 0.4099, 0.2925, 0.6245, 0.2993, 1.1631, 0.5605, 0.9536,\n",
       "                      0.7339, 0.7417, 0.6714, 0.7207, 0.3489, 1.2939, 0.9697, 1.0771, 0.6890,\n",
       "                      0.4214, 0.5137, 0.6489, 0.5176, 0.5049, 0.8281, 0.9678, 0.4556, 0.6416,\n",
       "                      0.7373, 0.7471, 0.5967, 0.6680, 0.4446, 0.6602, 0.9819, 0.4609, 0.3508,\n",
       "                      0.4934, 0.6670, 0.6865, 0.6763, 0.4458, 0.5571, 0.4526, 0.6943, 0.6890,\n",
       "                      0.8428, 0.8301, 0.6753, 0.9883, 0.3745, 0.6567, 0.2988, 0.7642, 0.4187,\n",
       "                      0.6201, 0.7271, 0.9844, 0.4097, 0.7124, 0.3091, 0.5645, 0.4915, 0.7144,\n",
       "                      0.2842, 0.5244, 0.6929, 0.2976, 0.6421, 0.4343, 0.4475, 0.4539, 0.7202,\n",
       "                      0.4749, 0.6929, 0.6187, 0.5718, 0.9297, 0.5225, 0.7163, 1.3984, 0.5762,\n",
       "                      0.4253, 0.5703, 0.7261, 0.5596, 0.3113, 0.5640, 0.3647, 0.3213, 0.7710,\n",
       "                      0.4790, 0.7344, 0.5757, 0.3455, 0.7197, 0.4568, 0.3032, 0.5601, 0.7988,\n",
       "                      0.5659, 0.6792, 0.5576, 0.3379, 0.3955, 0.3472, 0.9868, 0.8091, 0.3335,\n",
       "                      0.5669, 0.5952, 0.3376, 0.5264, 0.8306, 0.4988, 0.6753, 0.4280, 0.6802,\n",
       "                      0.7881, 1.7217, 0.5254, 0.7930, 0.3958, 0.3105, 0.5146, 0.8198, 0.3613,\n",
       "                      0.5972, 0.4866, 0.7002, 0.4231, 0.7744, 0.3096, 0.5352, 0.6885, 0.7129,\n",
       "                      0.7075, 0.5010, 0.5957, 0.3638, 1.1973, 0.6670, 0.6919, 0.2188, 0.7925,\n",
       "                      0.5249, 0.3276, 0.9058, 1.0938, 0.7598, 0.6094, 0.2905, 0.5488, 0.5122,\n",
       "                      0.4214, 0.4485, 0.6670, 0.4045, 0.8838, 0.4358, 0.6558, 0.4805, 0.6758,\n",
       "                      0.6582, 0.6450, 0.5181, 0.3345, 0.6104, 0.6343, 0.7729, 0.2769, 0.3887,\n",
       "                      0.9893, 0.5947, 0.3447, 0.3303, 0.3250, 0.4019, 0.3142, 0.5493, 0.5767,\n",
       "                      0.4536, 0.5195, 0.4580, 0.3655, 0.5015, 0.3613, 0.6958, 0.3853, 0.6719,\n",
       "                      0.5806, 0.3115, 0.6533, 0.4329, 0.5425, 0.7212, 0.7700, 0.3462, 0.3333,\n",
       "                      0.5034, 0.9341, 0.3274, 0.6118, 0.5283, 0.6079, 0.4878, 0.4338, 0.2615,\n",
       "                      0.5044, 0.6680, 1.1865, 0.5903, 0.8252, 0.6470, 0.8984, 0.4385, 0.5078,\n",
       "                      0.3391, 0.3757, 0.4275, 0.8208, 0.8110, 0.3591, 0.5396, 0.9248, 0.3333,\n",
       "                      0.5474, 0.2524, 0.7632, 0.6445, 0.3267, 0.6377, 0.5889, 0.4141, 0.6392,\n",
       "                      0.6245, 0.6880, 0.3958, 0.5103, 0.7407, 0.6182, 0.7188, 0.7847, 0.7432,\n",
       "                      0.6924, 0.7588, 0.6680, 0.6494, 0.3228, 0.5229, 0.2617, 1.0029, 0.3589,\n",
       "                      0.4851, 0.5259, 0.4187, 0.5483, 0.4387, 0.4919, 0.3569, 0.7910, 0.3528,\n",
       "                      0.7700, 0.2676, 0.3987, 0.6284, 0.4587, 0.5439, 0.4460, 0.7568, 0.5991,\n",
       "                      0.3892, 0.5615, 1.1582, 0.5283, 1.0088, 0.3438, 0.8257, 0.5640, 0.5708,\n",
       "                      0.5391, 0.4636, 0.5449, 0.8208, 0.3154, 0.4817, 0.5449, 0.8193, 0.5093,\n",
       "                      0.5347, 0.7812, 0.7769, 0.3945, 0.3118, 0.4106, 0.7549, 0.7612, 0.8154,\n",
       "                      0.6064, 0.4604, 0.3667, 0.6094, 0.9048, 0.9399, 0.3711, 0.9067, 0.4917,\n",
       "                      0.4438, 0.5688, 0.3308, 0.3184, 0.5708, 0.2668, 0.6182, 1.0059, 0.9209,\n",
       "                      0.4468, 0.6089, 1.1836, 0.2830, 0.3167, 0.5010, 0.6274, 0.8896, 0.6880,\n",
       "                      0.4272, 0.6543, 0.7729, 0.2332, 0.5537, 0.5400, 0.8799, 0.4875, 0.3547,\n",
       "                      0.5298, 0.5439, 0.3901, 0.5161, 0.5859, 0.7876, 0.8188, 0.5625, 0.3516,\n",
       "                      1.7842, 0.3611, 0.4153, 0.2798, 0.3958, 0.5674, 0.5430, 0.5718, 0.5537,\n",
       "                      1.1104, 0.4155, 0.6646, 0.4248, 0.6475, 0.4141, 0.3867, 0.3845, 0.5312,\n",
       "                      0.3215, 0.6245, 0.7139, 0.3853, 0.4392, 0.2957, 0.6924, 0.3845, 0.7632,\n",
       "                      0.4858, 0.7456, 0.2920, 0.7998, 0.6523, 0.7466, 0.6895, 0.4934, 0.5142,\n",
       "                      0.5728, 0.7295, 1.0381, 0.4077, 0.5620, 0.6177, 0.6182, 0.7178, 0.8042,\n",
       "                      0.9346, 0.5737, 0.4561, 0.5254, 1.3438, 1.0605, 0.4319, 0.2849, 0.6074,\n",
       "                      0.2932, 0.7827, 0.3472, 0.4939, 1.1357, 0.5547, 0.4827, 0.3274],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.4.layer_norm.bias',\n",
       "              tensor([-3.3032e-01,  4.9866e-02,  4.3701e-02, -9.6497e-02, -6.7529e-01,\n",
       "                      -4.0222e-02, -3.9624e-01, -2.2400e-01, -1.2720e-01, -2.2546e-01,\n",
       "                      -1.9006e-01, -1.2415e-01, -2.2717e-01, -1.6663e-01, -3.1519e-01,\n",
       "                      -2.2095e-01, -1.5149e-01, -2.6855e-01, -2.2375e-01, -2.0483e-01,\n",
       "                      -3.4692e-01, -4.4946e-01, -3.8745e-01, -3.2300e-01, -8.1970e-02,\n",
       "                       3.5980e-02, -3.7061e-01, -2.4402e-01, -1.1267e-01, -3.0273e-01,\n",
       "                      -3.0542e-01, -1.3037e-01, -1.0876e-01, -3.6865e-01, -1.6577e-01,\n",
       "                      -5.6104e-01, -3.7451e-01, -1.4282e-01, -3.9746e-01, -2.3254e-01,\n",
       "                      -3.2837e-01, -1.8433e-01, -1.3367e-01, -5.4541e-01, -3.5254e-01,\n",
       "                      -4.1162e-01, -5.8838e-01, -8.4863e-01, -4.9146e-01, -1.6052e-01,\n",
       "                      -2.3755e-01, -2.7905e-01, -1.2494e-01, -9.0088e-02, -5.2588e-01,\n",
       "                      -4.3335e-01, -6.7871e-01,  1.1829e-01, -1.0162e-01, -2.0569e-01,\n",
       "                      -3.1464e-02, -2.2998e-01, -2.5415e-01, -6.0352e-01,  1.6916e-04,\n",
       "                      -3.6572e-01, -2.0959e-01, -4.5801e-01, -8.4961e-01, -1.9678e-01,\n",
       "                      -7.3059e-02, -3.4473e-01, -2.9004e-01, -6.6748e-01, -1.4600e-01,\n",
       "                      -1.4746e-01, -1.7249e-01, -2.4487e-01, -1.5405e-01,  6.9458e-02,\n",
       "                      -6.8799e-01, -1.0388e-01,  7.0648e-03, -6.6956e-02, -2.3254e-01,\n",
       "                       5.9021e-02, -1.7664e-01, -1.1462e-01, -2.8369e-01, -1.3904e-01,\n",
       "                      -1.8555e-01, -4.3732e-02, -4.2798e-01, -1.9250e-01, -1.0309e-01,\n",
       "                      -1.2830e-01, -1.7932e-01,  5.2490e-02, -2.0325e-01, -8.2397e-02,\n",
       "                      -1.3196e-01, -1.6919e-01, -2.1973e-01, -3.8696e-01, -4.3530e-01,\n",
       "                      -1.0907e-01, -9.8694e-02, -3.4497e-01,  4.8492e-02, -2.6416e-01,\n",
       "                      -1.0773e-01, -9.2102e-02, -3.9526e-01,  3.5797e-02, -6.5625e-01,\n",
       "                      -1.8359e-01, -7.2900e-01, -3.6285e-02, -6.5088e-01, -4.2798e-01,\n",
       "                      -4.3774e-01, -3.9856e-02, -7.3389e-01, -9.1602e-01, -3.4692e-01,\n",
       "                      -6.9629e-01, -6.5674e-02, -1.6077e-01, -1.3318e-01, -2.7197e-01,\n",
       "                      -1.8982e-01, -1.4551e-01, -7.6855e-01, -3.9819e-01, -5.5078e-01,\n",
       "                      -3.9429e-01, -2.2949e-01,  1.1841e-02, -2.4670e-01, -1.5613e-01,\n",
       "                      -4.7412e-01, -4.3506e-01, -1.6943e-01,  2.6566e-02, -4.8804e-01,\n",
       "                      -2.9858e-01, -1.5955e-01, -2.7148e-01, -1.6821e-01, -2.2852e-01,\n",
       "                      -1.4319e-01, -3.6743e-01, -4.2969e-01, -6.5967e-01, -3.1006e-01,\n",
       "                      -2.8125e-01, -6.8066e-01, -2.6245e-01, -4.7461e-01, -2.0471e-01,\n",
       "                      -4.7241e-01, -1.9788e-01, -4.2236e-01, -2.6172e-01, -2.9077e-01,\n",
       "                      -1.3940e-01, -3.4521e-01, -6.8298e-02, -4.5239e-01, -1.9836e-01,\n",
       "                      -6.0059e-01, -1.2457e-01, -2.9517e-01, -3.3545e-01, -5.1971e-02,\n",
       "                      -5.0684e-01, -6.2805e-02, -3.2983e-01, -9.8267e-02, -3.4399e-01,\n",
       "                      -4.6265e-02, -6.8909e-02, -1.7822e-01, -1.6760e-01, -7.3975e-01,\n",
       "                      -4.0466e-02, -7.6270e-01, -9.1846e-01, -3.6108e-01, -3.8354e-01,\n",
       "                      -2.6074e-01, -5.8789e-01, -3.0957e-01, -7.6233e-02, -3.4131e-01,\n",
       "                      -1.4380e-01, -1.2793e-01, -4.0479e-01, -2.8809e-01, -3.6108e-01,\n",
       "                      -3.7988e-01, -4.9347e-02, -4.6973e-01, -2.8467e-01, -1.0138e-01,\n",
       "                      -1.1383e-01, -3.0835e-01, -6.2207e-01, -3.3740e-01, -4.5923e-01,\n",
       "                      -1.6455e-01, -1.2646e-01, -1.8176e-01, -4.4043e-01, -7.4219e-01,\n",
       "                       1.6772e-01, -5.2441e-01, -4.1357e-01, -1.3379e-01, -1.8213e-01,\n",
       "                      -6.5918e-01, -3.1616e-01, -1.4575e-01, -4.7803e-01, -5.2344e-01,\n",
       "                      -2.7417e-01, -8.3691e-01, -1.4038e-01, -2.4695e-01, -2.2705e-01,\n",
       "                      -1.0132e-01, -1.1066e-01, -6.8945e-01, -1.3123e-01, -2.9419e-01,\n",
       "                      -2.7295e-01, -2.5146e-01, -2.1594e-01, -6.0352e-01,  2.2831e-03,\n",
       "                      -1.2915e-01, -2.4158e-01, -6.1670e-01, -3.6011e-01, -3.2422e-01,\n",
       "                      -4.2920e-01, -1.6809e-01, -5.8643e-01, -4.7510e-01, -3.2251e-01,\n",
       "                      -7.1350e-02, -3.9893e-01, -2.8296e-01, -1.0004e-01, -7.9199e-01,\n",
       "                      -7.9297e-01, -6.6504e-01, -3.7500e-01,  5.2643e-02, -2.1948e-01,\n",
       "                      -3.4692e-01, -2.4750e-02, -1.6370e-01, -5.1465e-01, -1.0199e-01,\n",
       "                      -2.3401e-01, -1.6602e-01, -2.0044e-01, -1.3013e-01,  1.1345e-02,\n",
       "                      -3.8867e-01, -3.0298e-01, -3.4814e-01, -2.0044e-01, -1.9385e-01,\n",
       "                      -3.6206e-01, -1.9214e-01, -9.1431e-02, -1.7468e-01, -4.6021e-01,\n",
       "                      -4.3628e-01, -8.0994e-02, -1.1224e-01, -1.3000e-01, -1.1322e-01,\n",
       "                      -1.4160e-01, -3.5376e-01, -4.0308e-01, -1.3354e-01, -1.6150e-01,\n",
       "                      -1.6089e-01, -1.2915e-01, -1.6028e-01,  2.9480e-02, -3.8452e-01,\n",
       "                      -4.1443e-02, -5.6299e-01, -6.1719e-01, -1.9946e-01, -3.2635e-03,\n",
       "                      -1.4392e-01, -2.8442e-01, -2.2476e-02, -3.5864e-01, -8.8013e-02,\n",
       "                      -1.5674e-01, -1.1292e-01, -7.4121e-01, -1.1090e-01, -4.3018e-01,\n",
       "                      -1.5491e-01, -5.1697e-02, -3.6841e-01, -1.5540e-01,  1.0577e-01,\n",
       "                      -2.8833e-01, -5.1172e-01, -9.6973e-01, -2.2668e-01, -2.9907e-01,\n",
       "                      -4.6167e-01, -4.1528e-01, -1.2073e-01, -1.2781e-01, -1.9275e-01,\n",
       "                      -1.0559e-01, -2.1094e-01, -4.0015e-01, -6.3574e-01, -1.5125e-01,\n",
       "                      -1.9971e-01, -7.1240e-01, -1.7065e-01, -2.7222e-01, -2.3682e-01,\n",
       "                      -1.4946e-02, -3.7354e-01, -1.1584e-01, -6.2683e-02, -1.5100e-01,\n",
       "                      -1.7151e-01, -4.5947e-01, -4.6411e-01, -6.6846e-01, -1.2457e-01,\n",
       "                      -1.5479e-01, -1.1359e-01, -3.5571e-01, -2.8345e-01, -2.4036e-01,\n",
       "                      -2.3755e-01, -4.3237e-01, -1.6907e-01, -3.9966e-01, -5.6787e-01,\n",
       "                      -1.3879e-01, -5.4785e-01,  2.6230e-02, -6.1865e-01, -6.5002e-02,\n",
       "                      -3.1641e-01, -3.3740e-01, -1.8384e-01, -2.8931e-01, -8.8806e-02,\n",
       "                      -2.5635e-01, -5.9357e-02, -3.3154e-01, -5.4901e-02, -4.0063e-01,\n",
       "                      -1.5601e-01, -9.5276e-02, -3.3960e-01, -3.5913e-01, -4.4434e-01,\n",
       "                      -6.8115e-02, -6.0107e-01, -2.0996e-01, -5.7922e-02, -2.5171e-01,\n",
       "                      -9.2480e-01, -1.1334e-01, -5.9521e-01, -3.7598e-02, -5.8057e-01,\n",
       "                      -2.4109e-01, -3.5352e-01, -7.5439e-02, -2.9248e-01, -3.9215e-02,\n",
       "                      -6.6650e-01, -1.5845e-01, -3.2788e-01, -3.0151e-01, -2.7246e-01,\n",
       "                      -3.7256e-01, -3.2324e-01, -4.2310e-01, -4.3140e-01, -6.6650e-02,\n",
       "                      -1.1383e-01, -2.2476e-02, -3.6377e-01, -5.2930e-01, -6.8115e-01,\n",
       "                      -1.4771e-01, -6.9153e-02, -1.0870e-01, -4.2212e-01, -4.8950e-01,\n",
       "                      -6.1670e-01, -1.6467e-01, -3.4814e-01, -1.0327e-01, -1.6699e-01,\n",
       "                      -2.4121e-01,  7.8552e-02, -8.9417e-02, -3.0811e-01,  6.8787e-02,\n",
       "                      -4.6533e-01, -5.6738e-01, -6.6650e-01, -2.5195e-01, -5.5469e-01,\n",
       "                      -6.7041e-01,  1.0986e-01, -1.3342e-01, -1.1938e-01, -2.4390e-01,\n",
       "                      -7.4219e-01, -7.0752e-01,  1.8707e-02, -2.0642e-01, -4.0430e-01,\n",
       "                      -1.2793e-01, -3.9624e-01, -4.8706e-02, -5.2100e-01, -3.0176e-01,\n",
       "                      -2.4551e-02, -2.2021e-01, -3.6743e-01, -1.1304e-01, -9.7778e-02,\n",
       "                      -4.1382e-01, -5.3711e-01, -4.2676e-01, -2.4939e-01,  2.9083e-02,\n",
       "                      -1.1104e+00, -1.6675e-01, -1.3159e-01, -6.6284e-02, -1.7004e-01,\n",
       "                      -2.3743e-01, -2.8979e-01, -1.3916e-01, -1.7578e-01, -8.4131e-01,\n",
       "                      -2.3462e-01, -2.3206e-01, -5.0018e-02, -4.0625e-01, -1.2939e-01,\n",
       "                      -1.0864e-01, -1.4490e-01, -2.0935e-02, -8.8135e-02, -6.1963e-01,\n",
       "                      -2.0654e-01, -1.3513e-01, -6.2408e-02, -1.7908e-01, -2.8076e-01,\n",
       "                      -4.6509e-02, -4.8071e-01, -2.5610e-01, -2.8735e-01, -1.6251e-02,\n",
       "                      -3.0469e-01, -1.7651e-01, -8.2092e-02, -1.8738e-01, -2.2070e-01,\n",
       "                      -4.6826e-01, -3.4473e-01, -4.3652e-01, -1.6724e-01, -1.9128e-01,\n",
       "                      -4.8242e-01, -2.8345e-01, -1.8542e-01, -7.1729e-01, -3.9795e-01,\n",
       "                      -3.6572e-01, -2.6440e-01, -1.3916e-01, -1.3464e-01, -1.1768e+00,\n",
       "                      -9.2383e-01, -1.2988e-01,  1.2830e-01, -2.4451e-01,  2.0248e-02,\n",
       "                      -1.4343e-01, -3.9032e-02, -3.0078e-01,  6.6162e-02, -3.8208e-01,\n",
       "                      -3.7695e-01, -7.5195e-02], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.5.conv.weight',\n",
       "              tensor([[[ 0.0214,  0.0554],\n",
       "                       [ 0.1965,  0.1343],\n",
       "                       [ 0.0563,  0.1332],\n",
       "                       ...,\n",
       "                       [-0.0089, -0.0493],\n",
       "                       [-0.0609, -0.0565],\n",
       "                       [ 0.1775,  0.1448]],\n",
       "              \n",
       "                      [[ 0.0201, -0.0822],\n",
       "                       [ 0.1063,  0.0501],\n",
       "                       [ 0.1725,  0.1716],\n",
       "                       ...,\n",
       "                       [ 0.1331,  0.1711],\n",
       "                       [-0.0099,  0.0205],\n",
       "                       [ 0.1428, -0.0825]],\n",
       "              \n",
       "                      [[ 0.0548,  0.0277],\n",
       "                       [-0.0942,  0.0267],\n",
       "                       [ 0.2312,  0.0282],\n",
       "                       ...,\n",
       "                       [ 0.0071,  0.1177],\n",
       "                       [-0.0260,  0.0163],\n",
       "                       [-0.1019, -0.1031]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 0.2267, -0.0589],\n",
       "                       [-0.0163,  0.2529],\n",
       "                       [-0.0173,  0.0948],\n",
       "                       ...,\n",
       "                       [-0.0413, -0.0154],\n",
       "                       [-0.1207,  0.0447],\n",
       "                       [ 0.0706,  0.0398]],\n",
       "              \n",
       "                      [[-0.0027, -0.0516],\n",
       "                       [ 0.0590, -0.0947],\n",
       "                       [ 0.0360,  0.1412],\n",
       "                       ...,\n",
       "                       [-0.0269, -0.0478],\n",
       "                       [-0.0864, -0.0166],\n",
       "                       [-0.1039,  0.0642]],\n",
       "              \n",
       "                      [[ 0.0134, -0.0243],\n",
       "                       [ 0.0797, -0.1887],\n",
       "                       [ 0.0174,  0.2292],\n",
       "                       ...,\n",
       "                       [ 0.3982, -0.2153],\n",
       "                       [-0.0564, -0.1382],\n",
       "                       [-0.0852,  0.0184]]], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.5.conv.bias',\n",
       "              tensor([-0.0463,  0.2317,  0.3276,  0.3315,  0.1085,  0.1359,  0.0092,  0.1390,\n",
       "                       0.0305, -0.0459,  0.1276, -0.0480,  0.0301, -0.1785, -0.2917, -0.0530,\n",
       "                       0.0449, -0.0964,  0.2455,  0.1161, -0.0113,  0.0034,  0.3586, -0.0685,\n",
       "                       0.0423,  0.0656,  0.4834,  0.0346,  0.0958,  0.0228,  0.3274,  0.2766,\n",
       "                      -0.0681,  0.2195,  0.3420, -0.0139,  0.2539, -0.0284, -0.0601,  0.3069,\n",
       "                       0.0034,  0.2296,  0.1632,  0.2177,  0.3960, -0.0168,  0.3088,  0.3909,\n",
       "                       0.0303,  0.0630,  0.0321,  0.2216,  0.2402,  0.0077,  0.2939, -0.0091,\n",
       "                       0.0104,  0.0453, -0.0153, -0.0074, -0.0663, -0.1022, -0.0954,  0.0804,\n",
       "                       0.0917, -0.0056,  0.1919,  0.3674,  0.1591,  0.2903,  0.1028,  0.1888,\n",
       "                       0.0908,  0.1566,  0.3118,  0.0102,  0.1718, -0.0093, -0.0757,  0.0489,\n",
       "                      -0.2052,  0.0258,  0.0626, -0.0380,  0.2905,  0.2354,  0.0346,  0.1613,\n",
       "                       0.0635,  0.3438,  0.0019,  0.1335,  0.2881,  0.0892,  0.0620, -0.1088,\n",
       "                      -0.0247,  0.0120,  0.0742,  0.1569,  0.2935,  0.0394,  0.0532,  0.1982,\n",
       "                       0.0169,  0.1616,  0.1076,  0.3005, -0.1537,  0.0519,  0.0991,  0.2299,\n",
       "                      -0.1305,  0.1204,  0.1863, -0.0112,  0.0884,  0.0428,  0.0640,  0.0547,\n",
       "                      -0.0820, -0.1406,  0.0633,  0.1603, -0.0527, -0.2308,  0.2260,  0.0030,\n",
       "                      -0.1140, -0.0203, -0.0368, -0.0330, -0.1443, -0.1677,  0.2451, -0.0396,\n",
       "                       0.1147,  0.2832,  0.3064,  0.2399,  0.0463, -0.0188,  0.0030,  0.0141,\n",
       "                      -0.0273,  0.0389,  0.0083,  0.3503, -0.0279,  0.2095, -0.0092,  0.3206,\n",
       "                       0.3083,  0.4348,  0.1094,  0.0338, -0.0037,  0.2578,  0.1410,  0.1692,\n",
       "                       0.2937,  0.3430,  0.2323, -0.0544,  0.1177,  0.1517,  0.2563, -0.1198,\n",
       "                       0.2656,  0.1218,  0.1449, -0.1186, -0.0105,  0.0845,  0.3591,  0.0528,\n",
       "                       0.0546, -0.1141,  0.1068,  0.2268, -0.0108, -0.0858, -0.0820, -0.0682,\n",
       "                       0.2715,  0.0282,  0.1054,  0.2744,  0.4167,  0.0427,  0.1523, -0.1877,\n",
       "                       0.0904,  0.2291, -0.0891,  0.2822,  0.1758,  0.1503,  0.2705, -0.0728,\n",
       "                      -0.0764, -0.1147,  0.1089,  0.0100,  0.1306,  0.2327,  0.1152,  0.0005,\n",
       "                      -0.0201,  0.1539,  0.1116,  0.2803,  0.2607,  0.1161, -0.0122,  0.1981,\n",
       "                       0.0761,  0.0476,  0.1490,  0.2878,  0.0101, -0.0675, -0.0277, -0.1262,\n",
       "                       0.0592,  0.1523,  0.1829,  0.0655,  0.0524,  0.0168, -0.0299,  0.1250,\n",
       "                       0.0850,  0.0815,  0.0029,  0.0908, -0.0411,  0.2312, -0.0663,  0.3523,\n",
       "                      -0.2490,  0.1427, -0.0872,  0.2092,  0.1833,  0.2073,  0.0872,  0.0074,\n",
       "                      -0.0701,  0.2939, -0.0596,  0.0457, -0.0308,  0.2898,  0.0371,  0.2771,\n",
       "                      -0.0580, -0.0166,  0.2908,  0.1461, -0.0420,  0.0850,  0.1976, -0.0925,\n",
       "                       0.3718,  0.0940, -0.1312,  0.0043,  0.1785,  0.2283,  0.2610, -0.0547,\n",
       "                       0.0033,  0.3335,  0.1029,  0.0525, -0.1196,  0.0020,  0.3123,  0.1114,\n",
       "                      -0.0428, -0.1333,  0.2360,  0.1143, -0.0055,  0.0038,  0.1720,  0.3499,\n",
       "                       0.1008,  0.1533,  0.4951, -0.0349, -0.0535,  0.0314, -0.0342, -0.1913,\n",
       "                      -0.1134,  0.1093,  0.0188,  0.3303,  0.1528, -0.1426,  0.0431, -0.0307,\n",
       "                       0.1936,  0.0154,  0.2375,  0.3726, -0.0809, -0.0466,  0.2429,  0.3896,\n",
       "                       0.1379,  0.1098, -0.0040, -0.0517,  0.2510,  0.0887,  0.0572,  0.1971,\n",
       "                      -0.0201,  0.2642,  0.1282,  0.1274,  0.1848,  0.0657, -0.0180,  0.1866,\n",
       "                       0.0054,  0.4219, -0.1666,  0.0453,  0.1919,  0.1350,  0.0156, -0.0892,\n",
       "                      -0.0098,  0.2810,  0.2803, -0.3796, -0.0063, -0.0296, -0.1387,  0.2930,\n",
       "                      -0.0202, -0.2208,  0.0308,  0.2783,  0.0006,  0.0562, -0.0167,  0.1182,\n",
       "                      -0.0165,  0.0202,  0.3225, -0.0096,  0.3679,  0.0218,  0.0380, -0.0702,\n",
       "                      -0.0065, -0.0170,  0.0277,  0.0922,  0.0623, -0.0671,  0.2351,  0.0375,\n",
       "                       0.2761,  0.1002,  0.3782,  0.1344,  0.0778, -0.1226,  0.0801,  0.0488,\n",
       "                      -0.0930,  0.0292,  0.2462,  0.0248,  0.1174,  0.2068,  0.0087, -0.0245,\n",
       "                       0.2056,  0.1761,  0.1295,  0.1346,  0.3171,  0.0202,  0.1218,  0.0050,\n",
       "                       0.2356,  0.2001,  0.1437,  0.0227, -0.0190, -0.0847, -0.1138, -0.0908,\n",
       "                       0.1553,  0.4426,  0.4609,  0.2668,  0.2145,  0.1382,  0.0082, -0.0788,\n",
       "                       0.0016, -0.0248,  0.2927,  0.0064,  0.0117,  0.1208,  0.0429,  0.1210,\n",
       "                       0.1836,  0.2285,  0.0599, -0.0555,  0.1360,  0.1963,  0.0204, -0.0583,\n",
       "                      -0.0033,  0.2842,  0.2700,  0.1781,  0.0576,  0.3569,  0.3694,  0.1234,\n",
       "                       0.3345,  0.3889,  0.2637, -0.1033,  0.0118,  0.0450, -0.0401,  0.0852,\n",
       "                       0.1769,  0.1577,  0.1716, -0.0453, -0.0142,  0.0550,  0.0748,  0.0317,\n",
       "                       0.1420, -0.0067,  0.0546,  0.0408,  0.1276,  0.1243,  0.0145,  0.0396,\n",
       "                       0.4451,  0.1860, -0.1045,  0.0856, -0.0919,  0.2302,  0.0768,  0.2264,\n",
       "                       0.1631, -0.2292,  0.1261,  0.0213,  0.0662, -0.0198, -0.0414,  0.3091,\n",
       "                      -0.0714, -0.0573,  0.0870,  0.3442,  0.1808,  0.1131, -0.1219,  0.0113,\n",
       "                       0.0776,  0.2021,  0.0818, -0.0686,  0.0237,  0.1198,  0.1387,  0.0058,\n",
       "                       0.0657,  0.0562, -0.0344, -0.0106,  0.1411,  0.1333,  0.0217, -0.0679,\n",
       "                       0.2568, -0.4351, -0.0083, -0.1151,  0.3206, -0.0183,  0.4060, -0.0569,\n",
       "                       0.0432, -0.0091,  0.1969,  0.0927, -0.1661,  0.0093,  0.1433,  0.0052],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.5.layer_norm.weight',\n",
       "              tensor([0.5889, 0.2610, 0.3306, 0.3723, 0.3132, 0.3481, 0.5278, 0.4551, 0.4729,\n",
       "                      0.6313, 0.5068, 0.6802, 0.5127, 0.9526, 0.7178, 0.5112, 0.5137, 1.0283,\n",
       "                      0.3311, 0.5703, 0.5552, 0.4597, 0.2866, 1.0127, 0.7271, 0.6250, 0.2703,\n",
       "                      0.4399, 0.4690, 0.8232, 0.2998, 0.2905, 0.5850, 0.3496, 0.2507, 0.5039,\n",
       "                      0.3232, 0.5674, 0.7549, 0.2515, 0.7471, 0.3423, 0.3420, 0.2874, 0.3054,\n",
       "                      0.5684, 0.3289, 0.2458, 0.6011, 0.5298, 0.4866, 0.2661, 0.3235, 0.6099,\n",
       "                      0.3489, 0.5522, 0.5552, 0.6035, 0.7812, 0.6743, 0.6152, 0.6523, 0.8467,\n",
       "                      0.6040, 0.3862, 0.4829, 0.2817, 0.2759, 0.5747, 0.2715, 0.4663, 0.3169,\n",
       "                      0.4304, 0.2661, 0.2830, 0.6436, 0.4094, 0.5933, 0.8638, 0.3916, 0.8838,\n",
       "                      0.4995, 0.6050, 0.8540, 0.2507, 0.2891, 0.6899, 0.4304, 0.4282, 0.2832,\n",
       "                      0.6816, 0.4182, 0.2952, 0.3987, 0.5532, 0.8428, 0.6812, 0.4363, 0.6885,\n",
       "                      0.4399, 0.3313, 0.4941, 0.6401, 0.3132, 0.4771, 0.4880, 0.4458, 0.2891,\n",
       "                      0.6875, 0.4587, 0.2644, 0.3442, 0.6396, 0.8457, 0.3003, 0.5239, 0.4006,\n",
       "                      0.6338, 0.7441, 0.5342, 0.7881, 0.8721, 0.7617, 0.3337, 0.5620, 1.0117,\n",
       "                      0.4128, 0.5479, 0.6138, 0.4648, 0.6680, 0.5425, 0.7222, 0.7490, 0.3430,\n",
       "                      0.7559, 0.6777, 0.3599, 0.2979, 0.5181, 0.6899, 0.9341, 0.5908, 0.5889,\n",
       "                      0.6318, 0.6836, 0.6201, 0.2408, 0.8032, 0.3970, 0.7520, 0.3406, 0.2754,\n",
       "                      0.2617, 0.3247, 0.3452, 0.6094, 0.3044, 0.3247, 0.2964, 0.2988, 0.2900,\n",
       "                      0.2971, 0.8433, 0.4326, 0.3599, 0.3677, 0.6846, 0.2622, 0.3479, 0.3486,\n",
       "                      0.6172, 0.6753, 0.3982, 0.2228, 0.4102, 0.5020, 0.9775, 0.3481, 0.5938,\n",
       "                      0.5815, 0.7549, 0.8037, 0.6494, 0.3005, 0.5703, 0.3452, 0.2808, 0.2644,\n",
       "                      0.5854, 0.3218, 0.9883, 0.5132, 0.3047, 0.7891, 0.2598, 0.4292, 0.6152,\n",
       "                      0.2217, 0.7354, 0.6587, 0.8579, 0.2788, 0.7910, 0.3254, 0.2786, 0.9058,\n",
       "                      0.4575, 0.5928, 0.4185, 0.4543, 0.2769, 0.3794, 0.4221, 0.7095, 0.2913,\n",
       "                      0.4829, 0.4500, 0.2832, 0.3247, 0.6230, 0.5649, 0.7002, 0.7554, 0.4697,\n",
       "                      0.9199, 0.2788, 0.4199, 0.5620, 0.7900, 0.9053, 0.3997, 0.4033, 0.3665,\n",
       "                      0.6392, 0.6338, 0.5781, 0.2830, 0.5737, 0.3088, 0.7803, 0.4270, 0.5928,\n",
       "                      0.2900, 0.3284, 0.3066, 0.7627, 0.5229, 0.5366, 0.2751, 0.9976, 0.4883,\n",
       "                      0.7305, 0.4395, 0.6812, 0.2460, 0.5776, 0.6592, 0.3049, 0.5659, 0.7017,\n",
       "                      0.3950, 0.2274, 0.7676, 0.2563, 0.5962, 0.7896, 0.5625, 0.2306, 0.3386,\n",
       "                      0.2942, 0.6133, 0.7339, 0.2776, 0.2998, 0.3662, 0.6465, 0.9067, 0.2979,\n",
       "                      0.3684, 0.5327, 0.9570, 0.2866, 1.0225, 0.5537, 0.9512, 0.3271, 0.3411,\n",
       "                      0.4580, 0.6880, 0.3052, 0.6045, 0.8906, 0.4519, 0.6216, 0.9253, 0.6421,\n",
       "                      0.3594, 0.5073, 0.2957, 0.2632, 0.9741, 0.4504, 0.8047, 0.3892, 0.8364,\n",
       "                      0.3127, 0.2649, 0.7510, 0.6235, 0.2961, 0.2881, 0.4121, 0.3560, 0.4492,\n",
       "                      0.5405, 0.2937, 0.4888, 0.5200, 0.3103, 0.7759, 0.3530, 0.4797, 0.3606,\n",
       "                      0.3508, 0.5605, 0.4668, 0.3213, 0.3884, 0.2416, 0.7598, 0.4900, 0.3477,\n",
       "                      0.4761, 0.6997, 0.7793, 0.5728, 0.2443, 0.2976, 0.8164, 0.5942, 0.8794,\n",
       "                      0.9590, 0.2639, 0.6431, 0.8418, 0.4985, 0.3687, 0.6543, 0.3884, 0.7075,\n",
       "                      0.4517, 0.9224, 0.7031, 0.2954, 0.7090, 0.2467, 0.4756, 0.3071, 0.5791,\n",
       "                      1.0254, 0.6309, 0.3982, 0.3274, 1.0596, 1.0010, 0.3413, 0.4399, 0.2922,\n",
       "                      0.7026, 0.2881, 0.4766, 0.3975, 0.7437, 0.4561, 0.4915, 0.8735, 0.4463,\n",
       "                      0.2776, 0.4963, 0.3235, 0.3750, 0.4695, 0.5132, 0.3779, 0.3289, 0.3435,\n",
       "                      0.4380, 0.1956, 0.4695, 0.4224, 0.5962, 0.2549, 0.3228, 0.4338, 0.4480,\n",
       "                      0.7866, 0.9092, 0.6519, 1.0742, 0.3416, 0.3093, 0.2561, 0.3105, 0.2834,\n",
       "                      0.3391, 0.5854, 0.7871, 0.6724, 0.6333, 0.3103, 0.6606, 0.8623, 0.5669,\n",
       "                      0.5024, 0.2656, 0.2588, 0.2974, 0.7661, 0.6909, 0.4382, 0.2625, 0.6738,\n",
       "                      0.6519, 0.7603, 0.2742, 0.3542, 0.2996, 0.4019, 0.2559, 0.2900, 0.3611,\n",
       "                      0.2517, 0.2664, 0.2810, 0.8701, 0.5317, 0.4021, 0.4641, 0.5034, 0.3347,\n",
       "                      0.5010, 0.3181, 0.5542, 0.7861, 0.5317, 0.4565, 0.5293, 0.4561, 0.5762,\n",
       "                      0.5117, 1.0518, 0.2927, 0.2683, 0.4917, 0.6812, 0.2722, 0.3213, 0.7622,\n",
       "                      0.5581, 0.9658, 0.2883, 0.4084, 0.3135, 0.4253, 0.8315, 0.4976, 0.6636,\n",
       "                      0.4973, 0.4478, 0.6489, 0.2573, 0.7476, 1.0039, 0.3789, 0.2573, 0.3411,\n",
       "                      0.3376, 0.8486, 0.6274, 0.8208, 0.3442, 0.4707, 0.8018, 0.5425, 0.2983,\n",
       "                      0.5771, 0.5449, 0.4739, 0.5024, 0.5054, 0.5894, 0.3130, 0.4675, 0.4211,\n",
       "                      0.7271, 0.3618, 0.8433, 0.5586, 1.0420, 0.3118, 0.5825, 0.2319, 0.5947,\n",
       "                      0.6699, 0.6953, 0.3188, 0.4485, 1.5283, 0.8496, 0.4807, 0.7529],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.5.layer_norm.bias',\n",
       "              tensor([-4.5898e-01, -1.0840e-01, -1.1945e-01, -7.2815e-02, -2.7734e-01,\n",
       "                      -1.1395e-01, -5.4150e-01, -2.9883e-01, -4.5947e-01, -5.2490e-01,\n",
       "                      -3.4424e-02, -5.5225e-01, -3.2642e-01, -8.1738e-01, -5.6787e-01,\n",
       "                      -4.0649e-01, -1.9519e-01, -7.4023e-01, -8.7463e-02, -2.2266e-01,\n",
       "                      -4.2847e-01, -3.9673e-01, -8.3252e-02, -7.3584e-01, -5.3516e-01,\n",
       "                      -3.4595e-01, -5.9967e-02, -3.2983e-01, -2.3706e-01, -2.9126e-01,\n",
       "                       3.5187e-02, -8.3923e-02, -1.9519e-01, -7.2632e-02, -1.1157e-01,\n",
       "                      -1.5576e-01,  8.5526e-03, -5.4785e-01, -6.7432e-01, -1.6296e-02,\n",
       "                      -5.1904e-01,  3.5334e-04, -6.0974e-02, -4.2786e-02, -1.0307e-02,\n",
       "                      -5.6494e-01,  8.3008e-03,  1.7197e-02, -4.0503e-01, -2.3438e-01,\n",
       "                      -3.3130e-01, -1.6663e-01,  2.0813e-02, -5.1758e-01, -1.5930e-01,\n",
       "                      -2.5928e-01, -2.9785e-01, -4.3677e-01, -4.3311e-01, -4.8901e-01,\n",
       "                      -5.3369e-01, -5.6543e-01, -8.8232e-01, -2.8003e-01,  7.1533e-02,\n",
       "                      -3.0688e-01, -1.7615e-01, -7.8430e-02, -2.0996e-01, -1.7197e-02,\n",
       "                      -2.5146e-01, -1.3538e-01, -2.1777e-01, -1.6345e-01, -8.3923e-02,\n",
       "                      -3.0469e-01, -2.2595e-01, -4.2773e-01, -6.3916e-01, -3.8965e-01,\n",
       "                      -8.4424e-01, -3.4497e-01, -3.3154e-01, -3.5498e-01, -1.6516e-01,\n",
       "                      -1.4612e-01, -3.0884e-01, -3.7451e-01, -2.8247e-01, -4.6783e-02,\n",
       "                      -5.0879e-01, -6.0089e-02,  1.4595e-02, -1.2073e-01, -3.4326e-01,\n",
       "                      -8.6328e-01, -6.8359e-01, -3.0737e-01, -2.4829e-01, -2.7197e-01,\n",
       "                      -7.0312e-02, -2.5366e-01, -2.8735e-01, -1.5450e-02, -3.1641e-01,\n",
       "                      -1.1499e-01, -6.5552e-02, -2.7634e-02, -7.9248e-01, -3.3496e-01,\n",
       "                      -1.3965e-01, -7.5623e-02, -2.6440e-01, -2.2766e-01, -8.7280e-02,\n",
       "                      -2.8418e-01, -3.4448e-01, -4.2896e-01, -2.5928e-01, -1.8201e-01,\n",
       "                      -6.4600e-01, -5.5908e-01, -3.4790e-01, -1.0010e-01, -6.6064e-01,\n",
       "                      -9.4873e-01, -8.4351e-02, -3.1567e-01, -6.3135e-01, -6.1914e-01,\n",
       "                      -6.7773e-01, -5.5615e-01, -2.7246e-01, -3.4326e-01, -9.1614e-02,\n",
       "                      -5.5762e-01, -4.8364e-01, -7.3425e-02,  1.6312e-02, -2.0862e-01,\n",
       "                      -4.0112e-01, -6.1475e-01, -5.4346e-01, -3.2617e-01, -7.7002e-01,\n",
       "                      -3.7427e-01, -4.2871e-01, -1.4111e-01, -3.2568e-01, -1.3953e-01,\n",
       "                      -7.3438e-01,  1.6907e-01, -2.9556e-02,  2.7435e-02, -1.8323e-01,\n",
       "                      -2.1820e-02, -3.9624e-01,  5.7526e-02,  2.2980e-02, -1.2146e-01,\n",
       "                      -6.3416e-02, -7.2510e-02, -6.8970e-02, -2.9419e-01, -1.4624e-01,\n",
       "                       7.8735e-03, -1.2329e-01, -6.4355e-01, -1.5991e-02, -4.9866e-02,\n",
       "                       2.2156e-02, -6.0693e-01, -5.8105e-01, -5.8014e-02, -1.3367e-02,\n",
       "                      -1.5845e-01, -6.9824e-02, -8.4766e-01,  2.7115e-02, -2.0544e-01,\n",
       "                      -4.4653e-01, -6.6406e-01, -6.7383e-01, -5.4590e-01,  1.2047e-02,\n",
       "                      -8.4863e-01, -2.5543e-02, -5.2246e-02, -1.1975e-01, -4.3896e-01,\n",
       "                      -3.0441e-02, -1.0156e+00, -2.7612e-01, -8.2275e-02, -5.8984e-01,\n",
       "                      -1.2732e-01, -9.9487e-02, -2.0190e-01,  6.2141e-03, -7.0947e-01,\n",
       "                      -3.7573e-01, -8.5986e-01, -8.4167e-02, -5.0537e-01, -4.6204e-02,\n",
       "                      -5.2094e-02, -3.8354e-01, -4.7192e-01, -4.8291e-01, -3.5010e-01,\n",
       "                      -3.1421e-01, -1.1060e-01, -2.9877e-02, -1.2158e-01, -1.8225e-01,\n",
       "                       7.8812e-03, -2.5558e-02, -3.0054e-01, -6.6895e-02,  8.0872e-03,\n",
       "                      -5.2197e-01, -5.5762e-01, -8.1201e-01, -7.7393e-01, -3.1226e-01,\n",
       "                      -2.4512e-01, -1.1066e-01, -2.6807e-01, -3.0762e-01, -2.6343e-01,\n",
       "                      -4.2554e-01, -3.0151e-01, -1.7737e-01, -8.7280e-02, -4.4238e-01,\n",
       "                      -2.8735e-01, -4.8364e-01,  9.0714e-03, -8.8086e-01, -4.2877e-02,\n",
       "                      -4.2188e-01, -2.4036e-01, -5.4736e-01, -5.5145e-02, -1.1304e-01,\n",
       "                      -1.2415e-01, -1.3184e-01, -4.9780e-01, -4.1870e-01, -3.4210e-02,\n",
       "                      -6.9580e-01, -4.8413e-01, -4.1309e-01, -5.4138e-02, -4.8657e-01,\n",
       "                      -1.4075e-01, -1.9531e-01, -3.9746e-01, -9.3384e-02,  2.2568e-02,\n",
       "                      -6.5625e-01, -3.6523e-01, -8.8959e-03, -8.3545e-01, -1.1328e-01,\n",
       "                      -1.1926e-01, -4.4604e-01, -4.8730e-01, -5.9601e-02, -1.0980e-01,\n",
       "                      -1.1725e-01, -6.1768e-01, -4.8926e-01, -9.7122e-03, -1.5771e-01,\n",
       "                      -1.3574e-01, -5.0537e-01, -3.7500e-01, -3.7140e-02, -1.8600e-02,\n",
       "                      -5.1367e-01, -4.6411e-01, -1.2798e-03, -2.6831e-01, -4.2383e-01,\n",
       "                      -3.8452e-01, -2.0837e-01,  7.6477e-02, -2.5220e-01, -2.3572e-01,\n",
       "                      -2.3117e-02, -5.4248e-01, -6.9922e-01, -5.1074e-01, -5.6982e-01,\n",
       "                      -5.0244e-01, -2.5269e-01, -5.1941e-02, -2.2058e-01, -1.2323e-01,\n",
       "                      -1.4832e-01, -8.2324e-01, -2.7539e-01, -5.0586e-01, -1.0950e-01,\n",
       "                      -6.8457e-01, -3.7079e-02, -1.2622e-01, -3.3740e-01, -6.4502e-01,\n",
       "                      -2.1683e-02, -8.9340e-03, -2.0129e-01, -1.1230e-01, -3.6597e-01,\n",
       "                      -3.1177e-01, -1.2952e-01, -4.1553e-01, -3.7769e-01, -1.0400e-01,\n",
       "                      -2.2339e-01, -8.3496e-02, -9.7229e-02, -1.3220e-01, -1.3318e-01,\n",
       "                      -1.6089e-01, -5.9180e-01, -1.8030e-01, -1.2939e-01,  4.5990e-02,\n",
       "                      -6.7139e-01, -2.3230e-01, -1.0968e-01, -2.8979e-01, -2.1802e-01,\n",
       "                      -6.7627e-01, -4.6045e-01, -3.8452e-02,  1.8127e-02, -5.0000e-01,\n",
       "                      -4.0161e-01, -6.2549e-01, -4.7119e-01, -1.5479e-01, -6.7432e-01,\n",
       "                      -5.1758e-01, -2.2742e-01, -3.1952e-02, -4.7656e-01, -3.1860e-02,\n",
       "                      -3.6475e-01, -2.4878e-01, -5.7422e-01, -5.1611e-01, -4.2000e-03,\n",
       "                      -3.7012e-01, -1.7297e-01, -3.0688e-01,  7.7881e-02, -4.4824e-01,\n",
       "                      -4.4287e-01, -6.2793e-01,  2.6657e-02, -7.7454e-02, -3.4644e-01,\n",
       "                      -7.4023e-01, -1.0217e-01, -3.2349e-01, -9.3567e-02, -5.2246e-01,\n",
       "                       5.8685e-02, -1.6479e-01, -2.5299e-02, -5.1318e-01, -3.9868e-01,\n",
       "                       3.8971e-02, -8.1592e-01, -4.3311e-01, -8.5571e-02, -5.2832e-01,\n",
       "                      -6.5186e-02, -8.8623e-02, -3.2910e-01, -2.5562e-01, -1.0419e-01,\n",
       "                      -2.3285e-02, -6.7688e-02, -2.6074e-01, -1.2793e-01, -1.1401e-01,\n",
       "                      -1.6553e-01, -4.4849e-01, -4.4098e-02,  8.4925e-04, -1.5234e-01,\n",
       "                      -4.4006e-02, -6.6309e-01, -8.7402e-01, -7.6123e-01, -7.8809e-01,\n",
       "                       7.5867e-02,  1.0300e-02,  1.0010e-01, -1.3330e-01, -1.3293e-01,\n",
       "                       2.0630e-01, -3.9307e-01, -7.7344e-01, -7.5830e-01, -6.8018e-01,\n",
       "                      -4.7119e-02, -6.6260e-01, -2.2986e-01,  2.5742e-02, -2.1545e-01,\n",
       "                      -1.6565e-01, -1.6589e-01, -1.7712e-01, -1.6760e-01, -5.1416e-01,\n",
       "                      -1.4893e-01, -9.0088e-02, -4.0405e-01, -5.0732e-01, -4.3701e-01,\n",
       "                      -4.8126e-02,  3.2495e-01, -1.7822e-01, -3.3228e-01, -2.3651e-02,\n",
       "                      -7.2937e-02, -1.7151e-01, -2.4048e-02, -1.3229e-02, -7.4768e-02,\n",
       "                      -7.2949e-01, -5.7275e-01, -8.8257e-02, -4.8145e-01, -1.4709e-01,\n",
       "                      -1.0828e-01, -2.4927e-01, -7.9041e-02, -4.9805e-01, -3.0127e-01,\n",
       "                      -2.5562e-01, -1.7969e-01, -5.0586e-01, -1.9617e-01, -5.0684e-01,\n",
       "                      -3.4473e-01, -3.2520e-01, -1.3062e-01, -1.6510e-02, -2.7222e-01,\n",
       "                      -4.4263e-01,  3.7079e-02, -3.7537e-02, -8.9600e-01, -1.1017e-01,\n",
       "                      -8.8330e-01, -1.1017e-01, -1.2042e-01, -1.5808e-01, -1.2421e-01,\n",
       "                      -5.2393e-01, -4.4653e-01, -3.1079e-01, -3.7012e-01, -5.0000e-01,\n",
       "                      -4.3848e-01, -1.0522e-01, -7.9395e-01, -5.7422e-01,  1.8616e-03,\n",
       "                      -5.5634e-02,  1.1780e-01, -4.3182e-02, -7.1533e-01, -5.4248e-01,\n",
       "                      -3.5791e-01, -9.9487e-02, -3.1494e-01, -7.5439e-01, -4.4141e-01,\n",
       "                      -2.2363e-01,  2.6989e-03, -6.0400e-01, -4.1211e-01, -3.0908e-01,\n",
       "                      -5.6445e-01, -4.8828e-01, -1.7749e-01, -1.6260e-01, -3.6206e-01,\n",
       "                      -4.9829e-01,  1.4966e-01, -5.2881e-01, -6.7480e-01, -7.9492e-01,\n",
       "                      -5.0018e-02, -3.6914e-01, -3.3169e-03, -1.7224e-01, -2.8394e-01,\n",
       "                      -3.3911e-01, -5.8777e-02, -2.6270e-01, -9.2969e-01, -6.4502e-01,\n",
       "                      -1.9421e-01, -4.2871e-01], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.6.conv.weight',\n",
       "              tensor([[[-6.1279e-02,  2.9617e-02],\n",
       "                       [ 1.1658e-01, -2.5562e-01],\n",
       "                       [ 1.1493e-01,  8.0933e-02],\n",
       "                       ...,\n",
       "                       [-1.0645e-01, -8.4167e-02],\n",
       "                       [-2.6587e-01,  2.5830e-01],\n",
       "                       [ 5.9128e-03,  1.7593e-02]],\n",
       "              \n",
       "                      [[-1.2109e-01, -2.0447e-01],\n",
       "                       [ 1.3977e-01,  8.6975e-02],\n",
       "                       [ 3.4451e-05,  6.4575e-02],\n",
       "                       ...,\n",
       "                       [ 1.0754e-01,  1.1145e-01],\n",
       "                       [-3.6804e-02, -4.9866e-02],\n",
       "                       [ 4.6326e-02, -3.6240e-03]],\n",
       "              \n",
       "                      [[-2.9358e-02,  5.3436e-02],\n",
       "                       [ 3.1030e-01, -1.5552e-01],\n",
       "                       [ 9.9976e-02,  6.8665e-02],\n",
       "                       ...,\n",
       "                       [-2.1255e-02,  2.1378e-02],\n",
       "                       [ 5.3528e-02,  5.6534e-03],\n",
       "                       [-1.2932e-03,  1.3867e-01]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[-4.2236e-02, -7.3608e-02],\n",
       "                       [-2.7539e-01,  6.4148e-02],\n",
       "                       [ 1.4832e-01, -1.1761e-01],\n",
       "                       ...,\n",
       "                       [-6.2286e-02, -7.6599e-02],\n",
       "                       [ 9.2407e-02,  1.3440e-01],\n",
       "                       [ 8.9874e-03,  2.8641e-02]],\n",
       "              \n",
       "                      [[ 7.5867e-02, -5.1384e-03],\n",
       "                       [-1.3452e-01,  2.3083e-01],\n",
       "                       [ 1.4380e-01,  1.9341e-03],\n",
       "                       ...,\n",
       "                       [ 2.8152e-03, -3.5034e-02],\n",
       "                       [ 3.7720e-02,  1.6296e-01],\n",
       "                       [-2.4646e-01, -1.3306e-01]],\n",
       "              \n",
       "                      [[-6.3965e-02, -6.6589e-02],\n",
       "                       [ 7.1045e-02,  1.9360e-01],\n",
       "                       [ 7.5378e-03, -4.7339e-01],\n",
       "                       ...,\n",
       "                       [ 7.8369e-02,  1.4900e-02],\n",
       "                       [ 2.1631e-01, -2.8809e-01],\n",
       "                       [-1.1284e-02,  1.0939e-03]]], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.6.conv.bias',\n",
       "              tensor([ 3.3325e-01,  3.8513e-02,  2.3926e-02,  2.5342e-01,  2.4731e-01,\n",
       "                       3.7671e-01, -3.0766e-03, -3.6102e-02,  5.1178e-02,  3.7079e-02,\n",
       "                      -2.1652e-02,  2.9774e-03,  3.9429e-01,  5.9601e-02,  6.6406e-02,\n",
       "                       4.9591e-02,  7.4043e-03,  2.5122e-01,  1.0205e-01,  3.4973e-02,\n",
       "                       2.8076e-01,  3.0304e-02,  3.6182e-01,  3.5425e-01, -3.3379e-03,\n",
       "                       6.7810e-02,  1.3647e-01, -2.8137e-02,  9.8511e-02, -7.4585e-02,\n",
       "                       2.8271e-01,  4.2383e-01,  3.6438e-02,  6.3965e-02, -1.3885e-02,\n",
       "                       2.1301e-01,  3.4448e-01,  9.2041e-02, -1.0735e-02,  1.9516e-02,\n",
       "                       1.1993e-02,  9.7656e-02,  1.2573e-01, -2.8915e-02, -1.7643e-03,\n",
       "                      -1.8341e-02,  4.5715e-02,  1.2917e-02,  2.1973e-01,  3.3032e-01,\n",
       "                       9.2773e-02,  1.1787e-02,  8.6853e-02, -5.2948e-02,  4.1733e-03,\n",
       "                       4.6509e-02,  5.7678e-02,  4.2603e-02,  9.6558e-02, -2.0638e-03,\n",
       "                       6.7444e-02, -1.0156e-01,  2.3206e-01, -1.8219e-02,  5.8136e-02,\n",
       "                       6.0394e-02, -4.1321e-02, -7.0763e-03,  3.1348e-01, -2.9480e-02,\n",
       "                      -1.2311e-01,  8.7891e-02, -2.6306e-02,  1.0117e-02,  1.6357e-02,\n",
       "                       2.5562e-01,  8.5815e-02,  3.4790e-01,  2.4805e-01,  6.6650e-02,\n",
       "                       1.1005e-01, -3.4210e-02,  1.8701e-01, -2.5452e-02,  6.0463e-03,\n",
       "                       2.9785e-01,  2.4573e-01,  6.7261e-02,  7.3792e-02,  3.5303e-01,\n",
       "                       4.9133e-02,  2.9688e-01,  1.8481e-01, -4.2053e-02,  2.7808e-01,\n",
       "                       5.3760e-01,  2.0093e-01,  2.6929e-01,  2.6074e-01,  1.7444e-01,\n",
       "                      -3.0914e-02,  2.8955e-01,  3.7262e-02, -2.3605e-02,  5.5695e-02,\n",
       "                       1.4307e-01,  5.8929e-02,  1.3330e-01,  2.2021e-01,  2.1106e-01,\n",
       "                       2.5537e-01,  3.1952e-02,  3.6206e-01,  3.0884e-02,  1.2646e-01,\n",
       "                       3.1030e-01,  4.7192e-01,  1.5955e-01, -5.0842e-02,  3.0098e-03,\n",
       "                      -9.8267e-03,  2.5732e-01, -9.8114e-03, -9.6436e-02,  1.6772e-01,\n",
       "                       1.6174e-01,  4.7388e-01,  4.2871e-01,  7.2021e-02, -2.2141e-02,\n",
       "                       1.2338e-04,  6.6956e-02,  4.6936e-02,  6.6040e-02,  5.4512e-03,\n",
       "                       7.9041e-02,  1.5022e-02,  6.1676e-02,  3.4253e-01,  8.5938e-02,\n",
       "                       4.3732e-02,  3.1641e-01,  5.1147e-02, -1.2619e-02,  9.3002e-03,\n",
       "                       4.2896e-01,  3.2910e-01,  5.2917e-02,  3.7524e-01,  5.0385e-02,\n",
       "                      -3.1113e-02,  1.7685e-02,  3.1641e-01, -6.7017e-02,  7.1777e-02,\n",
       "                       3.5736e-02,  6.7566e-02,  2.7368e-01,  1.4001e-01,  2.6276e-02,\n",
       "                      -8.8989e-02,  8.3923e-02,  1.2964e-01,  7.1960e-02,  1.6040e-01,\n",
       "                       1.9730e-02,  1.1401e-01,  2.4765e-02,  1.2659e-01, -3.8727e-02,\n",
       "                       9.7229e-02,  2.1118e-01,  3.0136e-02,  4.2090e-01,  2.9846e-02,\n",
       "                       2.1997e-01,  1.3391e-01,  1.0895e-02, -3.5736e-02,  1.3831e-01,\n",
       "                       3.6865e-01,  1.3039e-02,  1.9913e-02,  7.3181e-02,  7.1526e-03,\n",
       "                       1.3306e-01,  4.3243e-02, -6.5727e-03,  6.2195e-02,  3.4839e-01,\n",
       "                       3.7402e-01,  4.7021e-01,  2.7441e-01,  1.1481e-01,  4.4403e-02,\n",
       "                       2.6929e-01,  3.5059e-01, -8.3069e-02,  3.5000e-03,  3.4180e-02,\n",
       "                       3.3765e-01,  1.3391e-01, -1.8097e-02,  6.7444e-03,  1.1829e-01,\n",
       "                       2.3865e-01,  2.9321e-01,  8.1177e-02,  3.1006e-01, -5.6946e-02,\n",
       "                       1.3098e-01,  2.7319e-01,  2.6880e-01,  2.6489e-01,  1.0907e-01,\n",
       "                       2.8516e-01,  4.7722e-03, -8.9783e-02,  4.1382e-02,  2.9877e-02,\n",
       "                       3.7918e-03, -8.1604e-02,  2.1411e-01, -1.1169e-02,  9.3018e-02,\n",
       "                       1.7166e-02, -2.7740e-02,  1.4343e-03,  3.3051e-02,  3.4839e-01,\n",
       "                       9.9304e-02,  8.9111e-02,  2.3499e-02,  2.3474e-01,  1.3013e-01,\n",
       "                       1.2184e-02, -5.5695e-04,  2.8467e-01,  2.4780e-01,  1.9189e-01,\n",
       "                       4.4281e-02,  3.1396e-01,  1.5369e-01,  2.3926e-01,  3.6182e-01,\n",
       "                       2.7783e-01,  2.7905e-01,  1.0571e-01,  1.9946e-01,  3.2031e-01,\n",
       "                       8.1909e-02,  2.6904e-01,  1.1102e-01,  1.6495e-02,  1.5511e-02,\n",
       "                       2.9199e-01,  4.3384e-01,  9.7473e-02,  4.0649e-02,  1.8665e-01,\n",
       "                       2.0813e-02,  3.3643e-01,  1.7346e-01,  7.4097e-02,  5.3131e-02,\n",
       "                       5.4810e-02,  2.8467e-01,  5.9021e-02,  2.9327e-02,  6.0822e-02,\n",
       "                       2.3621e-01, -3.5431e-02,  6.8237e-02, -2.0554e-02,  1.4380e-01,\n",
       "                       2.9434e-02,  5.9174e-02,  2.3834e-02,  6.1523e-02,  3.0591e-01,\n",
       "                       2.5928e-01,  5.1575e-02, -2.5574e-02,  7.1777e-02, -4.4861e-02,\n",
       "                       2.7344e-01,  5.0934e-02,  3.7109e-01,  1.0272e-01, -1.0925e-01,\n",
       "                       3.2422e-01,  1.2000e-01,  2.4902e-01,  2.5562e-01, -3.0121e-02,\n",
       "                       8.6060e-02,  4.7211e-02,  9.8694e-02,  1.0626e-01,  2.5903e-01,\n",
       "                       2.3486e-01,  2.2913e-01,  7.9727e-03,  5.4169e-02,  7.5012e-02,\n",
       "                       3.6621e-01,  3.1421e-01,  5.1758e-02,  9.2468e-03,  3.0347e-01,\n",
       "                       1.1978e-02,  7.3059e-02,  2.9541e-01,  5.1819e-02,  2.9956e-01,\n",
       "                      -4.3526e-03, -9.5703e-02,  5.0842e-02,  3.6499e-02,  1.6370e-01,\n",
       "                       8.6060e-02,  3.0640e-01,  1.7944e-01,  9.4055e-02,  3.0298e-01,\n",
       "                      -7.1350e-02,  3.5010e-01,  2.3608e-01,  2.9126e-01,  3.1250e-01,\n",
       "                      -6.0059e-02,  2.9639e-01,  1.7090e-01,  3.2007e-01,  3.6865e-01,\n",
       "                       2.9678e-02, -3.0212e-02,  1.5649e-01,  1.7407e-01, -7.6904e-03,\n",
       "                       6.5247e-02,  2.9346e-01,  6.8237e-02,  2.0959e-01,  1.1208e-02,\n",
       "                       2.6535e-02,  2.1305e-03,  4.0479e-01,  6.2561e-02,  9.0942e-02,\n",
       "                      -2.8717e-02, -9.2834e-02,  2.5562e-01,  1.6565e-01,  2.0166e-01,\n",
       "                      -7.1678e-03,  2.2534e-01,  1.1200e-02,  9.6359e-03, -4.6921e-03,\n",
       "                       4.2114e-02,  8.7738e-03,  1.9812e-01, -3.4332e-03,  3.0542e-01,\n",
       "                       3.0591e-01,  1.0028e-01,  8.8562e-02, -4.4739e-02,  8.3557e-02,\n",
       "                       7.4524e-02,  1.0724e-01,  2.4512e-01,  3.2007e-01,  3.5645e-02,\n",
       "                       1.3573e-02, -1.0429e-02, -4.2450e-02,  2.1887e-01,  3.6401e-01,\n",
       "                       1.8787e-01, -6.4659e-03,  3.1299e-01,  8.6487e-02,  1.9116e-01,\n",
       "                       1.5613e-01,  3.1665e-01,  2.8351e-02,  3.3472e-01,  2.3376e-01,\n",
       "                       3.1128e-01,  7.0007e-02,  2.5000e-01,  1.7838e-02,  1.9897e-02,\n",
       "                       1.9788e-01, -1.3519e-02,  2.4597e-01, -2.7222e-02,  2.4811e-02,\n",
       "                       1.3525e-01,  1.3779e-02,  2.7222e-01,  5.4504e-02,  7.6141e-03,\n",
       "                       3.4790e-01,  1.0870e-01,  2.1948e-01, -1.1047e-02,  3.0298e-01,\n",
       "                       5.5084e-02,  1.1877e-01, -3.7811e-02,  1.5173e-01,  9.2896e-02,\n",
       "                       4.1797e-01,  2.2534e-01,  2.2107e-01, -1.9180e-02,  2.2266e-01,\n",
       "                       2.6489e-02, -1.7578e-02,  2.7051e-01, -1.7583e-05,  7.1838e-02,\n",
       "                       1.1499e-01,  7.8918e-02,  1.9910e-01,  3.3051e-02,  3.0380e-02,\n",
       "                       3.3154e-01,  8.6670e-03, -8.2855e-03,  2.4994e-02,  1.4392e-01,\n",
       "                      -2.9037e-02,  2.1400e-03,  8.8623e-02,  1.8115e-01,  2.2400e-01,\n",
       "                       7.5867e-02,  9.4543e-02,  3.0457e-02,  3.4473e-01,  2.7026e-01,\n",
       "                       8.3847e-03,  6.2927e-02, -4.9927e-02,  2.9846e-02,  8.3923e-02,\n",
       "                       8.4778e-02,  6.2469e-02,  1.2659e-01,  1.8951e-02,  7.6256e-03,\n",
       "                       2.7539e-01,  7.5562e-02,  2.9761e-01,  6.5125e-02,  1.8604e-01,\n",
       "                       2.5537e-01, -3.2074e-02,  3.0029e-01, -2.2156e-02,  1.3649e-02,\n",
       "                      -2.6382e-02,  8.2458e-02,  7.7942e-02,  2.6807e-01,  5.8319e-02,\n",
       "                      -8.8867e-02,  4.8706e-02, -2.0599e-02,  2.5879e-01,  4.1284e-01,\n",
       "                      -1.3695e-02,  3.3032e-01,  2.7173e-01,  1.6953e-02, -4.8561e-03,\n",
       "                       8.5022e-02,  2.7026e-01, -5.0293e-02,  2.0859e-02,  2.9224e-01,\n",
       "                       1.2146e-01,  1.0199e-01,  2.6660e-01,  3.9429e-02,  9.1553e-02,\n",
       "                       4.6069e-01,  1.2909e-02,  2.8516e-01,  9.6558e-02,  4.6295e-02,\n",
       "                       1.1798e-01,  1.2543e-02,  5.3131e-02,  1.9873e-01, -6.4697e-02,\n",
       "                       1.2964e-01,  3.4521e-01,  3.2446e-01,  2.6047e-02, -1.5732e-02,\n",
       "                       1.7346e-01,  2.9004e-01, -3.8330e-02,  1.7249e-01,  1.9153e-01,\n",
       "                       6.2103e-02,  1.6211e-01], device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.6.layer_norm.weight',\n",
       "              tensor([0.1343, 0.5264, 0.6792, 0.1519, 0.1465, 0.1437, 0.6479, 0.6177, 0.4595,\n",
       "                      0.5498, 0.6484, 0.5244, 0.1498, 0.6460, 0.6118, 0.6450, 0.4805, 0.1310,\n",
       "                      0.4934, 0.4399, 0.1362, 0.5898, 0.1447, 0.1404, 0.4446, 0.4526, 0.2035,\n",
       "                      0.7568, 0.4407, 0.5400, 0.1392, 0.1512, 0.4302, 0.4302, 0.5142, 0.1482,\n",
       "                      0.1680, 0.4617, 0.4529, 0.6323, 0.7632, 0.5410, 0.2069, 0.5986, 0.6890,\n",
       "                      0.6470, 0.6768, 0.5869, 0.1754, 0.1331, 0.5718, 0.7197, 0.3784, 0.7461,\n",
       "                      0.5142, 0.3989, 0.4314, 0.4673, 0.1824, 0.6309, 0.1696, 0.7627, 0.1755,\n",
       "                      0.5933, 0.7261, 0.5830, 0.5776, 0.6216, 0.1531, 0.6440, 0.7017, 0.5229,\n",
       "                      0.7778, 0.6240, 0.6279, 0.1475, 0.5771, 0.1539, 0.1355, 0.4250, 0.2705,\n",
       "                      0.6323, 0.1209, 0.5420, 0.5454, 0.1505, 0.1476, 0.7729, 0.4561, 0.1421,\n",
       "                      0.5537, 0.1428, 0.1587, 0.7354, 0.1478, 0.1293, 0.1544, 0.1528, 0.1620,\n",
       "                      0.1678, 0.7397, 0.1313, 0.5200, 0.6011, 0.1785, 0.3521, 0.4802, 0.3376,\n",
       "                      0.1270, 0.1874, 0.1365, 0.7280, 0.1482, 0.7583, 0.2108, 0.1522, 0.1226,\n",
       "                      0.1621, 0.4724, 0.4478, 0.6846, 0.1433, 0.4661, 0.7974, 0.1870, 0.1510,\n",
       "                      0.1226, 0.1266, 0.4529, 0.5156, 0.6929, 0.3833, 0.7085, 0.5781, 0.5874,\n",
       "                      0.8813, 0.5737, 0.4739, 0.1842, 0.4788, 0.5693, 0.1531, 0.6675, 0.6274,\n",
       "                      0.4436, 0.1475, 0.1567, 0.7227, 0.1445, 0.4158, 0.5679, 0.4326, 0.1309,\n",
       "                      0.6528, 0.4382, 0.3970, 0.4338, 0.1553, 0.3083, 0.6650, 0.9932, 0.4626,\n",
       "                      0.1610, 0.5156, 0.1605, 0.6567, 0.4148, 0.6060, 0.3425, 0.1245, 0.5107,\n",
       "                      0.1406, 0.5298, 0.1316, 0.5078, 0.1520, 0.5669, 0.4016, 0.5254, 0.3992,\n",
       "                      0.1311, 0.7329, 0.7183, 0.4995, 0.5571, 0.1672, 0.6255, 0.6553, 0.6318,\n",
       "                      0.1287, 0.1312, 0.1464, 0.1329, 0.5576, 0.6938, 0.1206, 0.1360, 0.5742,\n",
       "                      0.4150, 0.7295, 0.1406, 0.1459, 0.7178, 0.5132, 0.2451, 0.1555, 0.2025,\n",
       "                      0.4558, 0.1400, 0.7280, 0.4919, 0.1429, 0.1489, 0.1399, 0.1772, 0.1501,\n",
       "                      0.4912, 0.7173, 0.5454, 0.3528, 0.5605, 0.6533, 0.2404, 0.5078, 0.3191,\n",
       "                      0.5820, 0.5674, 0.5020, 0.4326, 0.1281, 0.4326, 0.4702, 0.4648, 0.1689,\n",
       "                      0.6323, 0.4299, 0.5664, 0.1354, 0.1442, 0.1843, 0.3882, 0.1470, 0.2532,\n",
       "                      0.1881, 0.1305, 0.1383, 0.1270, 0.3540, 0.1279, 0.1175, 0.4761, 0.1418,\n",
       "                      0.2964, 0.4412, 0.5576, 0.1411, 0.1399, 0.6963, 0.6948, 0.1661, 0.4929,\n",
       "                      0.1326, 0.1825, 0.6489, 0.5708, 0.6128, 0.1669, 0.3848, 0.6143, 0.5391,\n",
       "                      0.1631, 0.6143, 0.6289, 0.6714, 0.3496, 0.5923, 0.6187, 0.4817, 0.3770,\n",
       "                      0.1231, 0.1663, 0.4490, 0.5142, 0.4158, 0.5396, 0.1343, 0.3481, 0.1425,\n",
       "                      0.5537, 0.6743, 0.1400, 0.2688, 0.1353, 0.1555, 0.6196, 0.3262, 0.6143,\n",
       "                      0.4678, 0.7510, 0.1377, 0.1483, 0.1588, 0.6489, 0.3906, 0.5684, 0.1475,\n",
       "                      0.1366, 0.4705, 0.5366, 0.1399, 0.6597, 0.7314, 0.1421, 0.4331, 0.1504,\n",
       "                      0.6343, 0.6499, 0.4846, 0.4287, 0.1768, 0.4421, 0.1348, 0.1561, 0.4546,\n",
       "                      0.1469, 0.6929, 0.1396, 0.1741, 0.1338, 0.1555, 0.7686, 0.1458, 0.1414,\n",
       "                      0.1808, 0.1310, 0.4961, 0.7036, 0.2568, 0.1731, 0.7358, 0.6821, 0.1235,\n",
       "                      0.2130, 0.2991, 0.5776, 0.7363, 0.5635, 0.1259, 0.4519, 0.4790, 0.6553,\n",
       "                      0.6274, 0.1714, 0.1664, 0.1448, 0.5405, 0.1379, 0.6846, 0.7920, 0.4534,\n",
       "                      0.6802, 0.2585, 0.1948, 0.6255, 0.1348, 0.1436, 0.3767, 0.5557, 0.6753,\n",
       "                      0.7178, 0.5552, 0.3237, 0.1385, 0.1302, 0.5249, 0.5820, 0.4912, 0.4956,\n",
       "                      0.1600, 0.1401, 0.1898, 0.1611, 0.1252, 0.3123, 0.2651, 0.1327, 0.1498,\n",
       "                      0.5918, 0.1334, 0.1735, 0.1342, 0.1638, 0.1487, 0.4519, 0.6006, 0.1760,\n",
       "                      0.9453, 0.1603, 0.6309, 0.5073, 0.3701, 0.3879, 0.1376, 0.4844, 0.7451,\n",
       "                      0.1237, 0.5474, 0.1646, 0.6528, 0.1659, 0.4548, 0.2046, 0.5483, 0.3638,\n",
       "                      0.5674, 0.1337, 0.1990, 0.1815, 0.7192, 0.2209, 0.6440, 0.5254, 0.1250,\n",
       "                      0.1936, 0.4885, 0.3831, 0.6470, 0.1611, 0.5791, 0.6655, 0.1202, 0.6313,\n",
       "                      0.7993, 0.5190, 0.3677, 0.5327, 0.5386, 0.5918, 0.2510, 0.1373, 0.6382,\n",
       "                      0.5132, 0.5518, 0.1442, 0.1559, 0.7231, 0.7842, 0.5693, 0.6831, 0.6133,\n",
       "                      0.6514, 0.5303, 0.4507, 0.7236, 0.4399, 0.1323, 0.4910, 0.1344, 0.4810,\n",
       "                      0.1788, 0.1392, 0.6172, 0.1272, 0.5688, 0.6064, 0.5884, 0.4978, 0.4055,\n",
       "                      0.1571, 0.1460, 0.6528, 0.5146, 0.6631, 0.1781, 0.1326, 0.6289, 0.1345,\n",
       "                      0.1334, 0.4929, 0.4226, 0.3877, 0.1248, 0.5469, 0.4421, 0.1250, 0.2096,\n",
       "                      0.2328, 0.1499, 0.7524, 0.6064, 0.1271, 0.6860, 0.1348, 0.4846, 0.7505,\n",
       "                      0.1560, 0.4084, 0.4221, 0.1672, 0.7295, 0.1791, 0.1298, 0.1469, 0.4819,\n",
       "                      0.5444, 0.3796, 0.1616, 0.5454, 0.1394, 0.1312, 0.4941, 0.1179],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_extractor.conv_layers.6.layer_norm.bias',\n",
       "              tensor([-0.2020, -0.6030, -0.3850, -0.2024, -0.1904, -0.1864, -0.6348, -0.5977,\n",
       "                      -0.3811, -0.5679, -0.4792, -0.7114, -0.1923, -0.5596, -0.5444, -0.5186,\n",
       "                      -0.5444, -0.1875, -0.4832, -0.3877, -0.1870, -0.4673, -0.1810, -0.2053,\n",
       "                      -0.3516, -0.4543, -0.2396, -0.4644, -0.3230, -0.5186, -0.1842, -0.1707,\n",
       "                      -0.4387, -0.4214, -0.4429, -0.1979, -0.1810, -0.4204, -0.3530, -0.3669,\n",
       "                      -0.8521, -0.4199, -0.1904, -0.6758, -0.6089, -0.5439, -0.7188, -0.4841,\n",
       "                      -0.2159, -0.1929, -0.4658, -0.4927, -0.3679, -0.4543, -0.5112, -0.3157,\n",
       "                      -0.4380, -0.3716, -0.1594, -0.6494, -0.1240, -0.9062, -0.1855, -0.4580,\n",
       "                      -0.7954, -0.3909, -0.6396, -0.3423, -0.1913, -0.5381, -0.7905, -0.4548,\n",
       "                      -0.3660, -0.6011, -0.6079, -0.1891, -0.3201, -0.1740, -0.2024, -0.4177,\n",
       "                      -0.2903, -0.5591, -0.2001, -0.5903, -0.4229, -0.1537, -0.1764, -0.7363,\n",
       "                      -0.4226, -0.1801, -0.4563, -0.1731, -0.2247, -0.6216, -0.1879, -0.1548,\n",
       "                      -0.1453, -0.2019, -0.1921, -0.1410, -0.8530, -0.1890, -0.3679, -0.4346,\n",
       "                      -0.1783, -0.3125, -0.4692, -0.2976, -0.1985, -0.2095, -0.1820, -0.5791,\n",
       "                      -0.2108, -0.8188, -0.2201, -0.2030, -0.1499, -0.2134, -0.5347, -0.5132,\n",
       "                      -0.5713, -0.1748, -0.4038, -0.8062, -0.2037, -0.1754, -0.1528, -0.1821,\n",
       "                      -0.5020, -0.6021, -0.7031, -0.3228, -0.6196, -0.6626, -0.5562, -0.5420,\n",
       "                      -0.6099, -0.4888, -0.2167, -0.3049, -0.4373, -0.1904, -0.4766, -0.5972,\n",
       "                      -0.4180, -0.1771, -0.1842, -0.4424, -0.1801, -0.3613, -0.7437, -0.3447,\n",
       "                      -0.1865, -0.6875, -0.3994, -0.2822, -0.3848, -0.1992, -0.2791, -0.7866,\n",
       "                      -0.6528, -0.3130, -0.1719, -0.3755, -0.1599, -0.5854, -0.2954, -0.6724,\n",
       "                      -0.3599, -0.2025, -0.4917, -0.1438, -0.4756, -0.1740, -0.4214, -0.2133,\n",
       "                      -0.3369, -0.3401, -0.6436, -0.3596, -0.1829, -0.4756, -0.7598, -0.3765,\n",
       "                      -0.5474, -0.0626, -0.6484, -0.3789, -0.4021, -0.1754, -0.1995, -0.1869,\n",
       "                      -0.1726, -0.3591, -0.7524, -0.2046, -0.1974, -0.6631, -0.3354, -0.5435,\n",
       "                      -0.1996, -0.1541, -0.6499, -0.4980, -0.2727, -0.2086, -0.1808, -0.4990,\n",
       "                      -0.1620, -0.6831, -0.3499, -0.2217, -0.2059, -0.2007, -0.1807, -0.1781,\n",
       "                      -0.4287, -0.7031, -0.4495, -0.3809, -0.6274, -0.4624, -0.1733, -0.4041,\n",
       "                      -0.2471, -0.5127, -0.5112, -0.5273, -0.2578, -0.1707, -0.4648, -0.3701,\n",
       "                      -0.4312, -0.1794, -0.3782, -0.3340, -0.5039, -0.2108, -0.2135, -0.1906,\n",
       "                      -0.3379, -0.1886, -0.1638, -0.2288, -0.1776, -0.1661, -0.1757, -0.3264,\n",
       "                      -0.2114, -0.1801, -0.4954, -0.1697, -0.2913, -0.4160, -0.5122, -0.1819,\n",
       "                      -0.1797, -0.6802, -0.4380, -0.2317, -0.4316, -0.1744, -0.1953, -0.6641,\n",
       "                      -0.3735, -0.5044, -0.1960, -0.3574, -0.5093, -0.4395, -0.1849, -0.6055,\n",
       "                      -0.8071, -0.5752, -0.3191, -0.5952, -0.4412, -0.4941, -0.3196, -0.1722,\n",
       "                      -0.1998, -0.4099, -0.6958, -0.2419, -0.6377, -0.2017, -0.2024, -0.2256,\n",
       "                      -0.5195, -0.7729, -0.1788, -0.2450, -0.2034, -0.2120, -0.6616, -0.2280,\n",
       "                      -0.5391, -0.3113, -0.4167, -0.1663, -0.1948, -0.1877, -0.7109, -0.3357,\n",
       "                      -0.5239, -0.2128, -0.1978, -0.3367, -0.4038, -0.1863, -0.6963, -0.5225,\n",
       "                      -0.1697, -0.4248, -0.1589, -0.6748, -0.4941, -0.4373, -0.3306, -0.1821,\n",
       "                      -0.3030, -0.1654, -0.1948, -0.3313, -0.1978, -0.4768, -0.2017, -0.2329,\n",
       "                      -0.2019, -0.1949, -0.4370, -0.2115, -0.2266, -0.1790, -0.1903, -0.3726,\n",
       "                      -0.7568, -0.1909, -0.1832, -0.5347, -0.5205, -0.1879, -0.1321, -0.4607,\n",
       "                      -0.6675, -0.4551, -0.6870, -0.1840, -0.5459, -0.4475, -0.5176, -0.7637,\n",
       "                      -0.1929, -0.2075, -0.2010, -0.4983, -0.1788, -0.4451, -0.5737, -0.4114,\n",
       "                      -0.6025, -0.0381, -0.1963, -0.4480, -0.2100, -0.1967, -0.3457, -0.3374,\n",
       "                      -0.5312, -0.3572, -0.4941, -0.3225, -0.2074, -0.1852, -0.5156, -0.6064,\n",
       "                      -0.5225, -0.5254, -0.1910, -0.1788, -0.1782, -0.1387, -0.2034, -0.2747,\n",
       "                      -0.2871, -0.1816, -0.1718, -0.6934, -0.1851, -0.1772, -0.1886, -0.1407,\n",
       "                      -0.2098, -0.4622, -0.5684, -0.1992, -0.5728, -0.2213, -0.6616, -0.5000,\n",
       "                      -0.3079, -0.3987, -0.1917, -0.4824, -0.5254, -0.1870, -0.3062, -0.2128,\n",
       "                      -0.6055, -0.1978, -0.4436, -0.2220, -0.4785, -0.3289, -0.5347, -0.2057,\n",
       "                      -0.2489, -0.1126, -0.4204, -0.1908, -0.5923, -0.5220, -0.1799, -0.1224,\n",
       "                      -0.4172, -0.2869, -0.6084, -0.2064, -0.4263, -0.4771, -0.1691, -0.5703,\n",
       "                      -0.5166, -0.5640, -0.4089, -0.6143, -0.4780, -0.5205, -0.2041, -0.2113,\n",
       "                      -0.6758, -0.4341, -0.5425, -0.1848, -0.1957, -0.8228, -0.4124, -0.5459,\n",
       "                      -0.7271, -0.5439, -0.4607, -0.5762, -0.3152, -0.5864, -0.4949, -0.2111,\n",
       "                      -0.4373, -0.2009, -0.3457, -0.1660, -0.1819, -0.5947, -0.1586, -0.7412,\n",
       "                      -0.4639, -0.5078, -0.2456, -0.4431, -0.1530, -0.1410, -0.6411, -0.4512,\n",
       "                      -0.6172, -0.1841, -0.1804, -0.3267, -0.1846, -0.1908, -0.4470, -0.4368,\n",
       "                      -0.4285, -0.2037, -0.5415, -0.4709, -0.2061, -0.2076, -0.2323, -0.2152,\n",
       "                      -0.6113, -0.5498, -0.1715, -0.5654, -0.1927, -0.5156, -0.4829, -0.1716,\n",
       "                      -0.2529, -0.2954, -0.2133, -0.4661, -0.1688, -0.1696, -0.1748, -0.4883,\n",
       "                      -0.4761, -0.2986, -0.1882, -0.6709, -0.1134, -0.2089, -0.4282, -0.2203],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_projection.layer_norm.weight',\n",
       "              tensor([1.4446, 1.4680, 1.8968, 1.5431, 1.3285, 1.4617, 1.3047, 1.8141, 1.6933,\n",
       "                      1.3897, 1.7088, 1.6192, 1.4382, 1.3701, 1.1397, 1.9625, 1.6340, 1.6033,\n",
       "                      1.3152, 1.5267, 1.5451, 1.8547, 1.4134, 1.5230, 2.1361, 1.4626, 1.7870,\n",
       "                      1.5476, 1.8730, 1.7899, 1.5715, 1.3739, 1.7394, 1.5230, 1.3527, 1.5797,\n",
       "                      1.4255, 1.5673, 1.9314, 1.8934, 1.4458, 1.7989, 1.8902, 1.4876, 1.5021,\n",
       "                      1.4301, 1.3771, 1.1804, 1.4161, 1.5846, 1.4536, 1.4156, 1.4302, 1.6807,\n",
       "                      1.2968, 2.7050, 1.6502, 1.2809, 1.2517, 1.4650, 1.1409, 1.4963, 1.5275,\n",
       "                      1.8613, 1.2864, 1.8412, 1.2123, 1.8950, 1.5024, 1.3736, 1.3470, 1.5698,\n",
       "                      1.5288, 1.3158, 1.5287, 1.5635, 2.2009, 1.4491, 1.5273, 1.6242, 1.7424,\n",
       "                      1.5587, 1.6068, 1.5078, 1.7059, 1.3265, 1.5758, 1.3251, 1.3126, 1.4460,\n",
       "                      1.7523, 1.5059, 1.5838, 1.6658, 1.4272, 1.1432, 0.9929, 1.4630, 1.5582,\n",
       "                      0.8582, 1.6531, 1.5750, 1.4115, 1.8143, 1.6210, 1.9582, 1.8120, 1.3560,\n",
       "                      1.5287, 1.7592, 1.5433, 1.2826, 1.5390, 1.1995, 1.4381, 1.5009, 1.5820,\n",
       "                      1.6924, 1.7322, 1.8584, 1.4049, 1.5631, 2.6817, 1.4277, 1.6660, 1.6575,\n",
       "                      1.5220, 1.2859, 1.6193, 1.3914, 1.3935, 1.7785, 1.1621, 1.4974, 1.4519,\n",
       "                      1.2580, 1.3182, 1.2966, 1.2897, 1.2682, 1.2835, 1.3886, 1.3743, 1.6542,\n",
       "                      1.9315, 1.4054, 1.4477, 1.3257, 1.4995, 1.3121, 1.6749, 2.3663, 1.4802,\n",
       "                      1.7677, 1.2358, 2.1221, 1.6327, 1.5274, 1.4243, 1.0941, 1.5504, 1.7528,\n",
       "                      1.1415, 1.5653, 1.4600, 1.5234, 1.6340, 1.1814, 1.6498, 1.8788, 1.2133,\n",
       "                      1.2443, 1.3002, 1.4853, 1.6236, 1.4642, 1.2104, 1.6203, 2.5752, 1.4610,\n",
       "                      1.4427, 1.3006, 1.3544, 1.5027, 1.3367, 1.3575, 1.2088, 1.8339, 1.1074,\n",
       "                      1.6096, 1.4289, 1.4069, 1.4192, 1.6996, 1.1152, 1.6579, 1.5034, 1.4164,\n",
       "                      3.0729, 1.1836, 1.5339, 1.3371, 1.3831, 1.3873, 1.4851, 1.4707, 1.3919,\n",
       "                      1.2727, 1.3379, 1.1625, 1.0171, 1.4936, 1.5760, 1.5773, 1.4593, 1.3882,\n",
       "                      2.0986, 1.5928, 1.3580, 1.3978, 1.5601, 2.0480, 1.2310, 1.7020, 1.9968,\n",
       "                      1.2382, 1.9008, 1.8341, 2.0164, 1.5136, 1.5156, 1.7951, 1.6698, 1.6546,\n",
       "                      1.1174, 2.0515, 1.4449, 1.4945, 1.6124, 1.6361, 1.7976, 1.5458, 1.4119,\n",
       "                      1.2085, 1.4447, 1.2353, 1.4495, 1.7273, 1.6742, 1.3978, 1.4158, 1.1886,\n",
       "                      1.6796, 1.8708, 1.8310, 1.5604, 1.4786, 1.3101, 1.5845, 1.6377, 2.0134,\n",
       "                      1.5819, 1.5369, 1.0666, 1.2619, 1.4638, 1.4972, 1.3768, 1.7475, 1.4287,\n",
       "                      1.4334, 1.6287, 1.0630, 1.6659, 1.8945, 1.3838, 1.3307, 1.4274, 1.7239,\n",
       "                      1.4255, 1.3916, 1.7336, 1.7201, 1.9810, 1.4712, 1.5492, 2.2928, 1.2428,\n",
       "                      1.4617, 1.6647, 1.5314, 1.7289, 1.3352, 1.4977, 1.4789, 2.4794, 1.6156,\n",
       "                      2.0525, 1.3505, 1.3252, 1.5322, 1.2652, 1.3475, 2.0619, 1.3076, 1.4997,\n",
       "                      1.5646, 1.7505, 1.4845, 1.5097, 1.7493, 1.3210, 1.5361, 1.7462, 1.3752,\n",
       "                      1.4011, 2.0073, 2.0639, 2.1542, 1.5458, 1.8805, 1.4330, 1.5808, 1.4012,\n",
       "                      1.3935, 1.6805, 1.4436, 1.5975, 1.4835, 1.4718, 1.6820, 1.5154, 1.5909,\n",
       "                      1.4130, 1.5296, 2.0187, 1.4179, 1.4689, 1.6370, 1.7233, 1.2044, 1.5090,\n",
       "                      1.0053, 0.5604, 1.5247, 1.4594, 1.2157, 1.4225, 1.1862, 1.1312, 1.8157,\n",
       "                      1.3091, 1.4810, 1.6289, 1.6178, 1.6790, 1.4177, 1.4096, 1.2662, 1.9373,\n",
       "                      1.3217, 0.6537, 1.6367, 1.7283, 1.4447, 1.4403, 1.7651, 1.2411, 1.6036,\n",
       "                      1.1950, 1.0983, 1.4710, 1.2901, 1.5262, 1.4005, 1.3675, 1.4567, 2.1374,\n",
       "                      1.5556, 1.4941, 1.4029, 1.4178, 1.6376, 2.1806, 1.7447, 1.5252, 1.4526,\n",
       "                      1.5293, 1.4256, 1.5377, 1.5080, 1.3661, 1.5584, 1.6694, 1.2824, 1.5353,\n",
       "                      1.5386, 1.5630, 1.2891, 1.7726, 1.6670, 1.6129, 1.5414, 1.3438, 1.4521,\n",
       "                      1.5667, 1.2492, 1.4046, 1.2235, 1.4374, 1.3015, 1.6963, 1.8893, 1.3764,\n",
       "                      1.0988, 1.1349, 1.6777, 1.3017, 1.5158, 1.4341, 1.4252, 2.2121, 1.3576,\n",
       "                      1.1249, 1.3422, 1.3709, 1.3423, 1.2584, 1.4946, 1.5932, 1.4245, 1.9604,\n",
       "                      1.1740, 1.3338, 1.0517, 1.4801, 1.4134, 1.0545, 1.7943, 1.5216, 1.1207,\n",
       "                      1.5683, 1.2389, 1.5033, 1.5475, 1.3275, 1.1823, 1.2544, 1.1758, 1.1229,\n",
       "                      1.4474, 1.3899, 1.5708, 1.3286, 1.5829, 1.6284, 1.2574, 1.5669, 1.8738,\n",
       "                      1.5205, 1.4941, 2.0171, 1.5302, 1.4462, 1.2826, 1.4248, 1.8200, 1.3528,\n",
       "                      1.2421, 1.5129, 1.5386, 1.3764, 1.2521, 1.4786, 1.5047, 1.5549, 1.5038,\n",
       "                      1.4396, 1.5724, 1.7731, 1.6480, 1.6101, 1.8188, 1.8017, 1.5853, 1.3924,\n",
       "                      1.7892, 1.5872, 1.1974, 1.0156, 1.3869, 1.1766, 1.5391, 1.3532, 1.3778,\n",
       "                      1.5413, 2.7331, 1.6996, 1.5231, 1.6698, 1.6449, 1.4586, 1.4635, 1.7256,\n",
       "                      1.6669, 1.6148, 1.3751, 1.4742, 1.1795, 1.6193, 1.4331, 2.0655],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_projection.layer_norm.bias',\n",
       "              tensor([-0.3364,  0.5018,  0.3648, -0.3921, -0.2523, -0.3875,  0.3528,  0.5940,\n",
       "                       0.3760,  0.4681, -0.0015,  0.2919, -0.3427,  0.4452,  0.5437,  0.4587,\n",
       "                       0.4520, -0.3749,  0.2481,  0.2492, -0.3946,  0.7426, -0.3790, -0.3632,\n",
       "                       0.1067,  0.4143, -0.1507,  0.5502,  0.1565,  0.4747, -0.2589, -0.5436,\n",
       "                       0.0772,  0.1780,  0.4199, -0.2999, -0.4199,  0.2573,  0.1753,  0.2454,\n",
       "                       0.5456,  0.3186, -0.0701,  0.3257,  0.4628,  0.3467,  0.3119,  0.2995,\n",
       "                      -0.2091, -0.3976,  0.4869,  0.6510,  0.1678,  0.5299,  0.4427, -0.4193,\n",
       "                       0.4131,  0.2671, -0.1764,  0.5697,  0.0381,  0.7359, -0.3364,  0.4696,\n",
       "                       0.5042,  0.1030,  0.4346, -0.1064, -0.3181,  0.3749,  0.3644,  0.2642,\n",
       "                       0.3751,  0.5009,  0.3690, -0.3082,  0.0435, -0.3987, -0.3427,  0.1922,\n",
       "                      -0.0866,  0.4473, -0.2048,  0.4916,  0.0571, -0.1724, -0.2025,  0.4108,\n",
       "                       0.4747, -0.4516,  0.4358, -0.3681, -0.3455,  0.4705, -0.3776, -0.2957,\n",
       "                      -0.1841, -0.3447, -0.1963, -0.1301,  0.2697, -0.3980,  0.3875,  0.3814,\n",
       "                      -0.0260,  0.0528,  0.2787,  0.2416, -0.3948, -0.2018, -0.3318,  0.5143,\n",
       "                      -0.3511,  0.3390,  0.0813, -0.3262, -0.4531, -0.2350,  0.3754,  0.4674,\n",
       "                       0.4524, -0.2695,  0.0655,  0.3300, -0.2072, -0.0950, -0.3945, -0.2385,\n",
       "                       0.3284,  0.3232,  0.5459, -0.1356,  0.5367,  0.4860,  0.3819,  0.4231,\n",
       "                       0.4743,  0.3615, -0.0668,  0.2287,  0.5527, -0.4004,  0.5023,  0.4345,\n",
       "                       0.1733, -0.3630, -0.2731,  0.4334, -0.3498,  0.1707,  0.5741,  0.0155,\n",
       "                      -0.2785,  0.4486,  0.2344, -0.1246,  0.1774, -0.2957,  0.0279,  0.4627,\n",
       "                       0.4541,  0.2277,  0.0865,  0.2962, -0.0499,  0.3305, -0.0789,  0.5891,\n",
       "                       0.0316,  0.1100,  0.3718, -0.1457,  0.4248, -0.3542,  0.2972, -0.2095,\n",
       "                       0.3577,  0.2223,  0.0493,  0.0374, -0.3639,  0.4929,  0.5657,  0.4016,\n",
       "                       0.3714, -0.0879,  0.6079,  0.4220,  0.3616, -0.4309, -0.4158, -0.4371,\n",
       "                      -0.3614,  0.5569,  0.3580, -0.2981, -0.3298,  0.5775, -0.2589,  0.6738,\n",
       "                      -0.2719, -0.1350,  0.3218,  0.4164, -0.0878, -0.2251, -0.4133,  0.2938,\n",
       "                      -0.1470,  0.4712,  0.2636, -0.5015, -0.2754, -0.2581, -0.2100, -0.2147,\n",
       "                       0.1676,  0.4161,  0.3381,  0.1890,  0.4113,  0.3940, -0.2600,  0.1272,\n",
       "                      -0.0099,  0.5835,  0.3803,  0.3015,  0.1142, -0.4452,  0.2647, -0.1418,\n",
       "                       0.2887, -0.1990,  0.3949,  0.1686,  0.2294, -0.3871, -0.2565, -0.2403,\n",
       "                       0.1694, -0.4597, -0.3584,  0.0548, -0.3727, -0.1655, -0.3999, -0.0377,\n",
       "                      -0.2988, -0.4190,  0.3458, -0.2769,  0.0481,  0.2418,  0.5155, -0.3833,\n",
       "                      -0.4043,  0.5391,  0.3641, -0.2579,  0.1288, -0.3267, -0.2543,  0.4518,\n",
       "                       0.5462,  0.3609, -0.3568,  0.2369,  0.3026,  0.4815, -0.2005,  0.5531,\n",
       "                       0.4665,  0.2629,  0.0164,  0.4703,  0.6015,  0.3388,  0.1627, -0.2520,\n",
       "                      -0.4569,  0.1973,  0.1833,  0.0149,  0.4860, -0.3707, -0.1479, -0.0575,\n",
       "                       0.5023,  0.6726, -0.3827, -0.0783, -0.4610, -0.1777,  0.5289, -0.2834,\n",
       "                       0.2503,  0.2326,  0.6282, -0.3333, -0.3546, -0.2512,  0.4034, -0.0085,\n",
       "                       0.3245, -0.3762, -0.3103,  0.2087,  0.3096, -0.3434,  0.3433,  0.5606,\n",
       "                      -0.0484,  0.2303, -0.2536,  0.5585,  0.3037,  0.1855,  0.1293, -0.1450,\n",
       "                       0.1460, -0.2601, -0.2675,  0.3052, -0.3339,  0.5499, -0.3784, -0.1819,\n",
       "                      -0.3583, -0.2999,  0.3307, -0.4266, -0.3816, -0.4045, -0.3868,  0.2173,\n",
       "                       0.5939, -0.1088,  0.0348,  0.5687,  0.2800, -0.4652, -0.0442, -0.1042,\n",
       "                       0.3820,  0.6129,  0.4872, -0.4545,  0.2520,  0.4132,  0.3792,  0.4956,\n",
       "                      -0.2816, -0.2629, -0.2949,  0.5606, -0.3683,  0.4747,  0.4539,  0.2540,\n",
       "                       0.4096,  0.0506, -0.1696,  0.1269, -0.3577, -0.4743,  0.0832,  0.3506,\n",
       "                       0.4971,  0.2102,  0.4591, -0.0058, -0.0140, -0.4052,  0.2423,  0.7191,\n",
       "                       0.4821,  0.2140, -0.0942, -0.4305, -0.0986,  0.0547, -0.3641, -0.1418,\n",
       "                      -0.1027, -0.1087, -0.4723,  0.4857, -0.4723, -0.2716, -0.4109, -0.0485,\n",
       "                      -0.3268,  0.3333,  0.4866, -0.3069,  0.6157, -0.2420,  0.5400,  0.3844,\n",
       "                       0.0653,  0.0562, -0.4484,  0.4122,  0.3660, -0.4473,  0.4384, -0.2409,\n",
       "                       0.4535, -0.3524,  0.2902, -0.1885,  0.2959, -0.0034,  0.5810, -0.1689,\n",
       "                      -0.2404,  0.0438,  0.4626, -0.3399,  0.5601,  0.0579, -0.2945,  0.0532,\n",
       "                       0.1808,  0.1381,  0.4976,  0.0658,  0.4180,  0.3836, -0.4903,  0.4754,\n",
       "                       0.4512,  0.5683,  0.3325,  0.2979,  0.4146,  0.4324, -0.1765, -0.3196,\n",
       "                       0.4962,  0.3124,  0.3971, -0.4046, -0.4558,  0.2382,  0.4866,  0.4324,\n",
       "                       0.5133,  0.4300,  0.4658,  0.5109,  0.1353,  0.6398,  0.2849, -0.2866,\n",
       "                       0.2460, -0.3883,  0.3479, -0.0949, -0.3049,  0.5137, -0.3523,  0.3587,\n",
       "                       0.3626,  0.5165,  0.4122,  0.3465, -0.2969,  0.0674,  0.4831,  0.3524,\n",
       "                       0.6410, -0.3215, -0.3168,  0.3963, -0.3591, -0.4016,  0.3058,  0.3467,\n",
       "                       0.1312, -0.2976,  0.0890,  0.0671, -0.3946, -0.0611,  0.0963, -0.4024,\n",
       "                       0.5056,  0.5577, -0.5229,  0.5637, -0.1933,  0.3571,  0.3511, -0.1842,\n",
       "                      -0.0740,  0.0147, -0.1880,  0.2935,  0.1148, -0.4296, -0.2889,  0.2233,\n",
       "                       0.0864,  0.0898, -0.2287,  0.2913, -0.1291, -0.3428,  0.3550, -0.1827],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.feature_projection.projection.weight',\n",
       "              tensor([[ 2.6344e-02, -1.2667e-02,  2.4751e-04,  ...,  4.9673e-02,\n",
       "                       -3.0290e-02,  3.7999e-01],\n",
       "                      [-2.8602e-01,  5.2311e-03, -1.0892e-02,  ...,  1.8951e-01,\n",
       "                        4.2592e-02, -2.1246e-01],\n",
       "                      [ 6.0683e-02, -9.1011e-02, -1.0854e-02,  ...,  2.4868e-01,\n",
       "                       -1.2687e-02,  1.7698e-01],\n",
       "                      ...,\n",
       "                      [ 1.7535e-01,  4.5202e-03, -8.7661e-03,  ..., -3.7988e-01,\n",
       "                       -6.6857e-03, -4.4819e-03],\n",
       "                      [ 6.4656e-02,  1.1953e-02, -8.4174e-03,  ...,  6.9762e-02,\n",
       "                        1.5257e-02,  1.9000e-01],\n",
       "                      [ 1.1948e-01,  9.4148e-03, -5.9433e-03,  ...,  1.2717e-01,\n",
       "                       -3.7064e-02,  3.0279e-02]], device='cuda:0')),\n",
       "             ('wav2vec2.feature_projection.projection.bias',\n",
       "              tensor([ 0.0299,  0.0861, -0.3360,  ...,  0.0298, -0.0327, -0.0780],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.pos_conv_embed.conv.bias',\n",
       "              tensor([-0.3185, -0.2718,  1.0411,  ..., -0.3314,  0.2833,  0.1328],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.pos_conv_embed.conv.weight_g',\n",
       "              tensor([[[0.3242, 0.1334, 0.0968, 0.1026, 0.0969, 0.0749, 0.0610, 0.0654,\n",
       "                        0.0640, 0.0775, 0.0868, 0.0831, 0.0823, 0.0840, 0.0806, 0.0749,\n",
       "                        0.0692, 0.0851, 0.0930, 0.0914, 0.0854, 0.0981, 0.1061, 0.1010,\n",
       "                        0.1019, 0.1014, 0.1129, 0.1064, 0.1101, 0.1093, 0.1202, 0.1234,\n",
       "                        0.1244, 0.1351, 0.1207, 0.1413, 0.1314, 0.1441, 0.1452, 0.1437,\n",
       "                        0.1590, 0.1683, 0.1707, 0.1824, 0.1860, 0.1899, 0.2098, 0.2157,\n",
       "                        0.2482, 0.2735, 0.3204, 0.3580, 0.4004, 0.4460, 0.4976, 0.6189,\n",
       "                        0.6895, 0.7433, 0.8547, 0.9627, 1.2527, 1.4761, 2.0968, 4.6091,\n",
       "                        8.7427, 4.3431, 1.8510, 1.3097, 1.0507, 0.8757, 0.7461, 0.6414,\n",
       "                        0.5483, 0.4751, 0.4031, 0.3637, 0.3469, 0.3100, 0.2977, 0.2717,\n",
       "                        0.2528, 0.2242, 0.2222, 0.2181, 0.2183, 0.1937, 0.1795, 0.1745,\n",
       "                        0.1742, 0.1668, 0.1576, 0.1477, 0.1490, 0.1427, 0.1314, 0.1327,\n",
       "                        0.1369, 0.1255, 0.1290, 0.1193, 0.1223, 0.1139, 0.1192, 0.1077,\n",
       "                        0.1079, 0.1007, 0.0987, 0.0977, 0.0997, 0.0912, 0.0879, 0.0913,\n",
       "                        0.0830, 0.0911, 0.0958, 0.0906, 0.0807, 0.0970, 0.0935, 0.0799,\n",
       "                        0.0801, 0.0783, 0.0810, 0.0908, 0.0976, 0.1105, 0.1353, 0.3602]]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.pos_conv_embed.conv.weight_v',\n",
       "              tensor([[[ 1.4424e-02, -6.2053e-03,  1.3046e-02,  ...,  1.2778e-02,\n",
       "                         2.7203e-02,  4.7139e-02],\n",
       "                       [-1.1244e-02, -3.7094e-02, -2.2959e-02,  ..., -1.6171e-02,\n",
       "                        -1.7366e-02, -1.6687e-02],\n",
       "                       [ 5.2330e-02,  2.8765e-02,  1.8040e-02,  ...,  2.0919e-04,\n",
       "                        -3.6009e-03,  1.6923e-03],\n",
       "                       ...,\n",
       "                       [ 4.6339e-02,  2.5617e-02,  2.1589e-02,  ..., -1.2066e-02,\n",
       "                        -1.0606e-02,  1.6486e-02],\n",
       "                       [-3.8792e-03, -1.9194e-02, -1.7703e-02,  ..., -9.7605e-03,\n",
       "                        -2.8995e-02, -4.4986e-02],\n",
       "                       [-3.3538e-03, -1.3205e-02, -1.2435e-02,  ...,  3.8488e-03,\n",
       "                        -2.8648e-03, -2.2509e-03]],\n",
       "              \n",
       "                      [[ 9.1014e-04,  2.1759e-02, -4.6657e-04,  ...,  1.8661e-02,\n",
       "                         5.6222e-03, -6.5753e-03],\n",
       "                       [-8.9360e-03, -1.0034e-02, -8.6529e-03,  ..., -7.4442e-03,\n",
       "                        -1.3153e-02, -3.4003e-02],\n",
       "                       [ 7.9283e-02,  4.1832e-02,  3.5833e-02,  ...,  2.0648e-02,\n",
       "                         1.9688e-02,  3.9761e-02],\n",
       "                       ...,\n",
       "                       [ 8.8727e-02,  4.5179e-02,  2.7423e-02,  ..., -5.9522e-03,\n",
       "                        -1.0218e-02,  8.9648e-03],\n",
       "                       [-2.2394e-02,  3.9294e-03,  5.0255e-03,  ..., -3.6512e-03,\n",
       "                        -8.2149e-03, -3.7249e-02],\n",
       "                       [ 2.6247e-02,  4.6901e-03, -4.8956e-03,  ...,  1.0961e-02,\n",
       "                         1.1412e-02,  5.8183e-03]],\n",
       "              \n",
       "                      [[ 9.8864e-02,  2.4623e-02, -5.8868e-02,  ...,  7.4930e-02,\n",
       "                         1.0608e-01,  1.3500e-01],\n",
       "                       [ 2.2080e-01,  1.5237e-01,  1.2874e-01,  ..., -6.8108e-02,\n",
       "                        -4.9663e-02, -1.1979e-02],\n",
       "                       [-8.2683e-02,  5.6750e-02,  8.3949e-02,  ..., -9.2568e-02,\n",
       "                        -6.7125e-02, -1.7401e-01],\n",
       "                       ...,\n",
       "                       [-4.2660e-01, -2.0603e-01, -1.3580e-01,  ...,  3.5648e-03,\n",
       "                         8.3072e-03, -4.6678e-02],\n",
       "                       [ 6.6135e-02,  1.1685e-02,  3.1982e-02,  ...,  1.3467e-02,\n",
       "                         5.3755e-02,  4.5440e-03],\n",
       "                       [ 1.3682e-01,  1.4482e-01,  1.5205e-01,  ...,  7.6581e-02,\n",
       "                         8.6060e-02,  5.7939e-02]],\n",
       "              \n",
       "                      ...,\n",
       "              \n",
       "                      [[ 1.1865e-02,  5.9571e-03, -8.3761e-03,  ..., -5.8671e-03,\n",
       "                        -5.8353e-03, -1.5884e-02],\n",
       "                       [ 2.1369e-02,  3.3215e-03,  9.9678e-03,  ..., -1.4906e-04,\n",
       "                        -1.4306e-02,  6.3706e-03],\n",
       "                       [ 1.6733e-02,  1.4137e-02,  1.6255e-02,  ...,  4.8421e-03,\n",
       "                         6.4937e-03,  3.2781e-03],\n",
       "                       ...,\n",
       "                       [ 2.0685e-03, -1.0869e-02,  5.3385e-03,  ...,  1.9970e-02,\n",
       "                         1.7058e-02,  2.1766e-02],\n",
       "                       [-3.7907e-02, -2.9942e-02, -3.1849e-02,  ..., -1.3094e-02,\n",
       "                        -1.2140e-02, -1.7553e-02],\n",
       "                       [ 1.8043e-02,  1.5232e-02,  2.6351e-02,  ..., -1.2220e-02,\n",
       "                         2.1411e-03, -8.0527e-04]],\n",
       "              \n",
       "                      [[ 2.9070e-02,  9.7065e-03,  4.9773e-02,  ...,  2.4412e-02,\n",
       "                        -2.2531e-02,  6.7887e-03],\n",
       "                       [ 2.5633e-02,  8.0056e-03,  2.8764e-02,  ..., -3.3575e-02,\n",
       "                         4.9334e-03,  7.8681e-03],\n",
       "                       [-1.0039e-03, -2.8478e-02,  4.3876e-02,  ...,  1.9964e-02,\n",
       "                         3.4281e-02,  6.9813e-02],\n",
       "                       ...,\n",
       "                       [ 6.2002e-02,  3.0719e-02, -1.1294e-02,  ..., -3.0161e-02,\n",
       "                         9.8971e-03,  1.9267e-02],\n",
       "                       [-1.0379e-01, -8.0459e-02, -5.7399e-02,  ..., -1.0847e-01,\n",
       "                        -8.7446e-02, -1.6558e-01],\n",
       "                       [ 4.5195e-03, -5.8207e-03,  6.6313e-02,  ..., -4.5058e-03,\n",
       "                        -8.0895e-03, -2.5703e-03]],\n",
       "              \n",
       "                      [[ 1.4761e-02,  3.6863e-02,  5.5135e-02,  ..., -1.0867e-02,\n",
       "                        -3.7914e-03,  7.7000e-03],\n",
       "                       [ 9.8720e-02,  5.7007e-02,  2.8873e-03,  ..., -1.1948e-02,\n",
       "                         1.2534e-02,  1.9561e-02],\n",
       "                       [ 4.7040e-02, -1.3744e-03,  3.2543e-03,  ...,  6.1925e-03,\n",
       "                         2.4791e-02,  1.7735e-02],\n",
       "                       ...,\n",
       "                       [-1.0076e-01, -9.7916e-02, -4.9382e-02,  ..., -1.4100e-03,\n",
       "                        -7.8170e-03,  1.8770e-02],\n",
       "                       [-3.1102e-02,  1.1001e-03, -9.3775e-03,  ..., -7.5548e-04,\n",
       "                        -2.3087e-03, -3.9874e-02],\n",
       "                       [-1.1940e-01, -1.2081e-01, -5.4408e-02,  ..., -4.5960e-02,\n",
       "                        -7.2085e-02, -1.6351e-01]]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layer_norm.weight',\n",
       "              tensor([0.2858, 0.3905, 0.2084,  ..., 0.4207, 0.3822, 0.2519], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layer_norm.bias',\n",
       "              tensor([-0.0578,  0.0934,  0.0107,  ...,  0.0341, -0.0845,  0.0234],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.k_proj.weight',\n",
       "              tensor([[ 0.0034, -0.0094, -0.0348,  ...,  0.0121,  0.0236,  0.1424],\n",
       "                      [ 0.0303,  0.0127,  0.0109,  ...,  0.0281, -0.0207, -0.1543],\n",
       "                      [ 0.0303,  0.0112,  0.0080,  ..., -0.0129, -0.0009, -0.0542],\n",
       "                      ...,\n",
       "                      [ 0.0237, -0.0045, -0.0243,  ...,  0.1029, -0.0575,  0.1113],\n",
       "                      [-0.0271,  0.0204,  0.0053,  ..., -0.0202, -0.0393, -0.0065],\n",
       "                      [-0.0003, -0.0174, -0.0220,  ...,  0.0056,  0.0225,  0.0767]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.k_proj.bias',\n",
       "              tensor([ 0.0316,  0.0004, -0.0081,  ..., -0.0017,  0.0059, -0.0143],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.v_proj.weight',\n",
       "              tensor([[ 1.6352e-02, -8.4738e-03,  1.2746e-02,  ..., -1.1907e-01,\n",
       "                       -2.3274e-01,  3.6517e-02],\n",
       "                      [ 8.8664e-03,  6.1765e-03,  2.9845e-02,  ..., -2.6692e-02,\n",
       "                        2.9905e-01,  6.3739e-03],\n",
       "                      [ 7.2936e-02,  3.9058e-03,  6.2839e-03,  ...,  1.3629e-02,\n",
       "                        2.6746e-01, -5.9620e-02],\n",
       "                      ...,\n",
       "                      [ 1.2332e-02, -1.1889e-02,  9.1507e-03,  ..., -4.7113e-02,\n",
       "                       -2.7278e-02, -1.3039e-01],\n",
       "                      [-1.9463e-02, -1.0264e-02,  1.7683e-04,  ..., -1.0366e-02,\n",
       "                        2.9494e-01, -4.1985e-03],\n",
       "                      [ 5.1310e-02, -1.1835e-03,  2.1818e-02,  ..., -5.1368e-02,\n",
       "                        1.6597e-01,  2.4864e-02]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.v_proj.bias',\n",
       "              tensor([-0.0048,  0.0108, -0.0197,  ...,  0.0007, -0.0066, -0.0095],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.q_proj.weight',\n",
       "              tensor([[ 0.0051,  0.0160,  0.0102,  ..., -0.0528,  0.0883,  0.0749],\n",
       "                      [-0.0403,  0.0060,  0.0323,  ..., -0.0273, -0.0515, -0.0753],\n",
       "                      [ 0.0907, -0.0025, -0.0090,  ...,  0.0081,  0.0694, -0.0512],\n",
       "                      ...,\n",
       "                      [ 0.0109,  0.0390,  0.0219,  ..., -0.0579,  0.0182,  0.0497],\n",
       "                      [ 0.0213, -0.0168, -0.0217,  ...,  0.0758, -0.0476,  0.0922],\n",
       "                      [ 0.0034,  0.0342,  0.0184,  ...,  0.0036,  0.0334, -0.0852]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.q_proj.bias',\n",
       "              tensor([-0.4591,  0.0072,  0.0338,  ..., -0.2484, -0.2176,  0.0681],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.out_proj.weight',\n",
       "              tensor([[ 0.0129,  0.0557,  0.0317,  ...,  0.0235, -0.0513, -0.0222],\n",
       "                      [-0.1685,  0.2022, -0.0053,  ...,  0.0336,  0.0181,  0.0098],\n",
       "                      [ 0.0033,  0.0672,  0.0012,  ..., -0.0124,  0.0452, -0.0469],\n",
       "                      ...,\n",
       "                      [-0.1093, -0.0564, -0.0465,  ..., -0.0045,  0.0722,  0.0077],\n",
       "                      [-0.1574,  0.1419,  0.3187,  ...,  0.0358, -0.0856, -0.1232],\n",
       "                      [ 0.0175,  0.0368, -0.0499,  ...,  0.0643, -0.0833, -0.0280]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.attention.out_proj.bias',\n",
       "              tensor([ 0.0327,  0.0513, -0.2833,  ...,  0.1139, -0.0388, -0.0416],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.layer_norm.weight',\n",
       "              tensor([0.2191, 0.0004, 0.0025,  ..., 0.2080, 0.2027, 0.1881], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.layer_norm.bias',\n",
       "              tensor([ 0.0634, -0.0002, -0.0020,  ...,  0.0493, -0.0383, -0.0268],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.0824, -0.0175,  0.0135,  ...,  0.0479,  0.0322, -0.0494],\n",
       "                      [ 0.0460,  0.1802,  0.0018,  ..., -0.1827,  0.0197,  0.3195],\n",
       "                      [-0.0017,  0.0843,  0.0593,  ..., -0.0300,  0.0579,  0.0020],\n",
       "                      ...,\n",
       "                      [ 0.1101,  0.0306, -0.0123,  ...,  0.0220,  0.0097,  0.1066],\n",
       "                      [-0.0031,  0.0133, -0.0592,  ...,  0.0433, -0.0261, -0.0570],\n",
       "                      [ 0.0173,  0.0889,  0.0558,  ...,  0.1348, -0.1436,  0.0507]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.1215, -0.1287, -0.0606,  ..., -0.0837, -0.0837, -0.0859],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0543,  0.1162, -0.0391,  ..., -0.0425,  0.0536, -0.0138],\n",
       "                      [ 0.0963, -0.0373, -0.0144,  ...,  0.0538,  0.0674, -0.0177],\n",
       "                      [-0.0367, -0.0359,  0.0015,  ..., -0.0982,  0.0523, -0.0243],\n",
       "                      ...,\n",
       "                      [-0.0091,  0.0694, -0.0854,  ...,  0.0007, -0.0443, -0.0080],\n",
       "                      [-0.0848,  0.0029, -0.0150,  ...,  0.0327,  0.0626,  0.0684],\n",
       "                      [ 0.0905, -0.1769, -0.0368,  ..., -0.2333, -0.0019,  0.0517]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0316, -0.0389, -0.0802,  ...,  0.1192, -0.0170, -0.1214],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.final_layer_norm.weight',\n",
       "              tensor([0.2076, 0.0908, 0.0424,  ..., 0.1606, 0.1354, 0.1532], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.0.final_layer_norm.bias',\n",
       "              tensor([ 0.0493, -0.0420, -0.0356,  ...,  0.0350, -0.0071, -0.0374],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.k_proj.weight',\n",
       "              tensor([[-0.2829,  0.0693, -0.0496,  ..., -0.1730,  0.0164, -0.1461],\n",
       "                      [-0.1192, -0.1712,  0.0439,  ..., -0.1276,  0.0381, -0.0675],\n",
       "                      [ 0.0892,  0.0826,  0.0111,  ...,  0.0898,  0.0009,  0.0294],\n",
       "                      ...,\n",
       "                      [-0.1159, -0.0248,  0.0342,  ..., -0.1123,  0.0860, -0.1111],\n",
       "                      [-0.0036, -0.0877,  0.0625,  ...,  0.0983, -0.0189, -0.1035],\n",
       "                      [ 0.1269,  0.0413, -0.0702,  ...,  0.0669,  0.0759,  0.1168]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.k_proj.bias',\n",
       "              tensor([ 0.0140,  0.0046,  0.0286,  ..., -0.0077, -0.0138, -0.0048],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.v_proj.weight',\n",
       "              tensor([[-0.1141, -0.0998,  0.0458,  ..., -0.0457,  0.0577, -0.1037],\n",
       "                      [ 0.1625,  0.1585,  0.0331,  ..., -0.0622, -0.3605,  0.0305],\n",
       "                      [ 0.3691,  0.0186, -0.0094,  ...,  0.0267, -0.1221, -0.0918],\n",
       "                      ...,\n",
       "                      [ 0.1620, -0.0735,  0.0032,  ...,  0.2001,  0.1151, -0.0269],\n",
       "                      [-0.1806, -0.0488,  0.0625,  ...,  0.0621, -0.0902, -0.0702],\n",
       "                      [-0.0668, -0.1014, -0.0843,  ..., -0.0793, -0.0182, -0.0344]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.v_proj.bias',\n",
       "              tensor([ 0.0054, -0.0051,  0.0192,  ..., -0.0423,  0.0259,  0.0281],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.q_proj.weight',\n",
       "              tensor([[-0.2028, -0.0636, -0.0807,  ..., -0.0085,  0.0121, -0.0415],\n",
       "                      [-0.0602,  0.1047, -0.0265,  ...,  0.0857, -0.0186, -0.0029],\n",
       "                      [ 0.0488,  0.0900, -0.0848,  ...,  0.0701, -0.0619,  0.0559],\n",
       "                      ...,\n",
       "                      [ 0.1495,  0.0584, -0.0013,  ..., -0.1361, -0.0836, -0.0099],\n",
       "                      [-0.1227, -0.0589,  0.0753,  ..., -0.0779,  0.0211, -0.0533],\n",
       "                      [-0.0634,  0.0973, -0.1156,  ...,  0.0990,  0.0262,  0.2194]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.q_proj.bias',\n",
       "              tensor([-0.0057, -0.2011, -0.1260,  ...,  0.1622, -0.4674,  0.1137],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.out_proj.weight',\n",
       "              tensor([[ 0.1010, -0.0900, -0.2789,  ..., -0.1364,  0.0006,  0.0456],\n",
       "                      [ 0.1489, -0.0945, -0.0530,  ..., -0.0406, -0.1492,  0.0144],\n",
       "                      [ 0.0345,  0.0029,  0.0253,  ...,  0.0004, -0.0172, -0.0340],\n",
       "                      ...,\n",
       "                      [ 0.0219,  0.0229, -0.0262,  ..., -0.1079, -0.0211, -0.0478],\n",
       "                      [-0.0874,  0.2790,  0.0309,  ...,  0.0616,  0.0674, -0.1339],\n",
       "                      [ 0.1341, -0.0327,  0.0796,  ..., -0.0058,  0.1330, -0.0572]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.attention.out_proj.bias',\n",
       "              tensor([-0.0278,  0.0151, -0.0452,  ...,  0.1265, -0.0177, -0.0818],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.layer_norm.weight',\n",
       "              tensor([0.2885, 0.1628, 0.1290,  ..., 0.2344, 0.2722, 0.1378], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.layer_norm.bias',\n",
       "              tensor([ 3.6917e-02, -8.8901e-05, -1.7713e-04,  ...,  7.8131e-04,\n",
       "                      -2.0026e-02,  1.1361e-02], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0446,  0.2569, -0.0156,  ..., -0.0375,  0.0756, -0.0348],\n",
       "                      [-0.0583,  0.0206,  0.0323,  ..., -0.1438, -0.0135, -0.0289],\n",
       "                      [-0.1012, -0.0230,  0.1095,  ..., -0.1501, -0.0800, -0.1593],\n",
       "                      ...,\n",
       "                      [ 0.1378,  0.0428, -0.0591,  ...,  0.0032,  0.1669, -0.1552],\n",
       "                      [-0.0589,  0.3096, -0.0213,  ...,  0.0633,  0.0421,  0.1264],\n",
       "                      [ 0.0885, -0.0024,  0.0550,  ...,  0.0667, -0.0912,  0.0597]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0084, -0.0052, -0.0841,  ..., -0.0432, -0.0542, -0.0511],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0117,  0.1649, -0.1122,  ...,  0.0881,  0.0882,  0.0500],\n",
       "                      [ 0.0340,  0.0290,  0.1325,  ..., -0.0947, -0.0625, -0.0465],\n",
       "                      [-0.0128, -0.0355,  0.0199,  ...,  0.0257, -0.0164,  0.0538],\n",
       "                      ...,\n",
       "                      [-0.0486,  0.0244, -0.0457,  ...,  0.0183,  0.1135,  0.0534],\n",
       "                      [ 0.1447, -0.0150, -0.0868,  ..., -0.0144, -0.0437, -0.1006],\n",
       "                      [-0.0655, -0.0156, -0.0092,  ..., -0.0956,  0.0251,  0.0604]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0114,  0.0141, -0.0650,  ...,  0.0343,  0.0053, -0.0267],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.final_layer_norm.weight',\n",
       "              tensor([0.3395, 0.2610, 0.1770,  ..., 0.2909, 0.2883, 0.2312], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.1.final_layer_norm.bias',\n",
       "              tensor([-0.0079, -0.0364, -0.0214,  ...,  0.0386, -0.0447, -0.0941],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.k_proj.weight',\n",
       "              tensor([[ 1.6297e-02, -7.3382e-02,  3.8943e-02,  ..., -9.2870e-02,\n",
       "                        8.0758e-02,  2.5640e-02],\n",
       "                      [ 8.6599e-02, -3.9211e-03, -9.4413e-02,  ..., -4.7311e-02,\n",
       "                       -3.7311e-02,  1.4513e-01],\n",
       "                      [ 1.5825e-02, -1.1829e-01, -8.1813e-02,  ..., -2.2926e-04,\n",
       "                        8.5109e-02,  2.8741e-02],\n",
       "                      ...,\n",
       "                      [ 9.0706e-02, -1.9482e-01, -7.2465e-01,  ..., -9.4776e-02,\n",
       "                        1.2484e-02,  1.2909e-01],\n",
       "                      [ 1.2383e-01,  4.8293e-02, -5.7895e-01,  ...,  5.5878e-02,\n",
       "                       -5.7925e-02,  8.5515e-02],\n",
       "                      [-9.0963e-02,  5.5280e-02, -3.1517e-01,  ...,  1.4835e-01,\n",
       "                        2.1312e-02, -1.1604e-01]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.k_proj.bias',\n",
       "              tensor([ 0.0088,  0.0114,  0.0009,  ..., -0.0710,  0.1172,  0.0330],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.v_proj.weight',\n",
       "              tensor([[ 0.1445,  0.0452, -0.0046,  ..., -0.1614,  0.0378,  0.0183],\n",
       "                      [-0.0401, -0.0944,  0.0173,  ..., -0.1548, -0.0483, -0.0402],\n",
       "                      [-0.0451, -0.0937,  0.0112,  ...,  0.1439,  0.1508,  0.0250],\n",
       "                      ...,\n",
       "                      [-0.0777, -0.0454, -0.0288,  ...,  0.0583,  0.0273, -0.0012],\n",
       "                      [ 0.0623, -0.0498,  0.0090,  ..., -0.0192,  0.0717,  0.0034],\n",
       "                      [ 0.0761,  0.0329, -0.0251,  ..., -0.0335,  0.0187,  0.0249]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.v_proj.bias',\n",
       "              tensor([ 0.0249, -0.0343,  0.1169,  ..., -0.0003, -0.0017,  0.0054],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.q_proj.weight',\n",
       "              tensor([[ 0.0638,  0.1078,  0.0895,  ...,  0.1012,  0.1212,  0.0612],\n",
       "                      [-0.0192,  0.0602, -0.0040,  ...,  0.0234, -0.0440,  0.1203],\n",
       "                      [ 0.0947,  0.0326, -0.0347,  ..., -0.0955, -0.1139,  0.0246],\n",
       "                      ...,\n",
       "                      [ 0.1291, -0.0083, -0.4675,  ..., -0.0124, -0.0557,  0.0313],\n",
       "                      [ 0.0305,  0.0560, -0.4129,  ...,  0.1340, -0.0162,  0.0464],\n",
       "                      [ 0.0609,  0.1972, -0.1074,  ..., -0.1848, -0.0788, -0.0329]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.q_proj.bias',\n",
       "              tensor([-0.1255,  0.5548,  0.0435,  ..., -0.2995,  0.2302,  0.0048],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.out_proj.weight',\n",
       "              tensor([[-0.1660,  0.0407,  0.1552,  ...,  0.0911, -0.1686, -0.2060],\n",
       "                      [-0.0406, -0.0505, -0.1171,  ...,  0.0879,  0.0157,  0.0068],\n",
       "                      [-0.0227, -0.0564, -0.0360,  ..., -0.0143,  0.0152,  0.0509],\n",
       "                      ...,\n",
       "                      [ 0.0380,  0.0695, -0.1679,  ..., -0.0022, -0.1617, -0.0301],\n",
       "                      [-0.0528,  0.1081, -0.1983,  ...,  0.0458,  0.0121,  0.0383],\n",
       "                      [-0.0889,  0.0495,  0.0150,  ...,  0.1293,  0.0539, -0.0095]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.attention.out_proj.bias',\n",
       "              tensor([ 0.0007, -0.0336, -0.0417,  ...,  0.0970,  0.0238, -0.0717],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.layer_norm.weight',\n",
       "              tensor([0.3061, 0.2418, 0.2864,  ..., 0.2532, 0.2957, 0.1815], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.layer_norm.bias',\n",
       "              tensor([-0.0209,  0.0423,  0.0013,  ..., -0.0100, -0.0152,  0.0290],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-8.8790e-02, -3.1132e-02, -1.1971e-01,  ..., -8.6871e-03,\n",
       "                       -5.3573e-02, -1.9731e-01],\n",
       "                      [ 1.2726e-04,  6.1563e-02, -4.4537e-02,  ...,  1.1975e-01,\n",
       "                        9.5133e-02,  1.5990e-01],\n",
       "                      [ 7.6157e-03, -8.7615e-03,  7.2864e-02,  ..., -4.0548e-02,\n",
       "                        2.7660e-02,  3.7607e-02],\n",
       "                      ...,\n",
       "                      [ 1.7616e-01, -1.1987e-02,  1.4833e-02,  ..., -2.9650e-02,\n",
       "                       -3.7827e-03,  1.8840e-01],\n",
       "                      [ 1.1761e-01,  1.0911e-01,  2.6353e-02,  ...,  1.1574e-01,\n",
       "                       -8.1724e-02, -1.6765e-02],\n",
       "                      [-2.5391e-02,  1.0741e-01,  7.3954e-02,  ..., -3.5276e-02,\n",
       "                       -4.4665e-02, -5.2952e-02]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0410, -0.0251,  0.0033,  ..., -0.0397,  0.0326, -0.0212],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.0300,  0.0123, -0.0560,  ...,  0.0005,  0.1645, -0.0083],\n",
       "                      [ 0.0778,  0.1540,  0.0542,  ..., -0.0663,  0.0368,  0.0608],\n",
       "                      [ 0.0607, -0.0083,  0.0205,  ..., -0.0893,  0.0391, -0.0249],\n",
       "                      ...,\n",
       "                      [-0.0170, -0.0242, -0.0205,  ...,  0.0165,  0.1549, -0.0708],\n",
       "                      [ 0.0271,  0.0078, -0.0479,  ..., -0.0791, -0.0924,  0.1401],\n",
       "                      [ 0.2441, -0.0163,  0.0102,  ...,  0.0524,  0.0097, -0.0176]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0440, -0.0454, -0.0324,  ...,  0.0713, -0.0197, -0.0468],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.final_layer_norm.weight',\n",
       "              tensor([0.3207, 0.2869, 0.1276,  ..., 0.2959, 0.2821, 0.2649], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.2.final_layer_norm.bias',\n",
       "              tensor([-0.0862,  0.0614, -0.2002,  ...,  0.0106, -0.0256, -0.0733],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.k_proj.weight',\n",
       "              tensor([[ 0.0200, -0.0658,  0.0519,  ..., -0.0978,  0.0330,  0.0627],\n",
       "                      [ 0.0628,  0.0279, -0.0147,  ..., -0.0125,  0.0199, -0.0535],\n",
       "                      [-0.0783, -0.0101,  0.0052,  ...,  0.0336,  0.0783,  0.1276],\n",
       "                      ...,\n",
       "                      [-0.0231, -0.0402, -0.0356,  ...,  0.0675,  0.0228, -0.1023],\n",
       "                      [-0.1786,  0.0194,  0.0670,  ..., -0.0082, -0.0562,  0.0499],\n",
       "                      [ 0.1750,  0.0097, -0.1717,  ...,  0.0311, -0.1063, -0.1704]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.k_proj.bias',\n",
       "              tensor([-0.0059,  0.0182,  0.0020,  ..., -0.0179,  0.0221,  0.0337],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.v_proj.weight',\n",
       "              tensor([[ 0.1307, -0.0924, -0.0115,  ...,  0.1105, -0.0202,  0.0184],\n",
       "                      [-0.1248, -0.0490, -0.0491,  ...,  0.0941, -0.0735, -0.1795],\n",
       "                      [-0.0135, -0.0915, -0.0466,  ..., -0.1309,  0.1490,  0.0433],\n",
       "                      ...,\n",
       "                      [ 0.0730, -0.2146, -0.0020,  ..., -0.0328,  0.0878, -0.0384],\n",
       "                      [-0.1186, -0.1334,  0.0206,  ...,  0.1573, -0.0285,  0.0284],\n",
       "                      [ 0.0570,  0.0429, -0.0119,  ..., -0.2065,  0.0022,  0.1305]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.v_proj.bias',\n",
       "              tensor([-0.0211, -0.0053,  0.0378,  ..., -0.0006, -0.1287, -0.0190],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.q_proj.weight',\n",
       "              tensor([[ 1.0705e-01,  5.6310e-02,  1.1893e-02,  ...,  5.3247e-03,\n",
       "                       -2.9575e-05,  1.1019e-01],\n",
       "                      [ 1.0415e-01,  1.6980e-02,  6.5154e-02,  ..., -1.2958e-01,\n",
       "                       -7.5502e-02, -6.4326e-02],\n",
       "                      [ 4.4001e-02,  8.2917e-02, -4.1481e-02,  ..., -4.4211e-02,\n",
       "                       -5.7606e-02,  5.0808e-02],\n",
       "                      ...,\n",
       "                      [-2.2219e-03, -7.3808e-03, -6.3760e-02,  ...,  1.9912e-02,\n",
       "                        4.2495e-02, -2.6558e-02],\n",
       "                      [-1.0748e-01, -1.0326e-01,  3.5654e-02,  ..., -3.6551e-02,\n",
       "                       -4.4523e-02,  1.3152e-01],\n",
       "                      [ 3.0739e-02,  1.3265e-02, -1.6064e-01,  ..., -6.0213e-02,\n",
       "                        1.3490e-01,  3.0273e-02]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.q_proj.bias',\n",
       "              tensor([ 0.1537, -0.1088, -0.0499,  ...,  0.1869,  0.1466,  0.0714],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.out_proj.weight',\n",
       "              tensor([[-0.1888,  0.0708,  0.0685,  ...,  0.0527,  0.0486, -0.1630],\n",
       "                      [-0.0668, -0.1211, -0.0331,  ..., -0.0002,  0.0430, -0.0635],\n",
       "                      [-0.0045,  0.0295,  0.0559,  ..., -0.0010, -0.0034,  0.0665],\n",
       "                      ...,\n",
       "                      [-0.0782, -0.0754,  0.0408,  ...,  0.0244, -0.0597,  0.0525],\n",
       "                      [ 0.0589,  0.0963, -0.0206,  ..., -0.0259,  0.0835,  0.0627],\n",
       "                      [-0.0444, -0.0652,  0.1189,  ..., -0.1229, -0.0471, -0.0857]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.attention.out_proj.bias',\n",
       "              tensor([ 0.0952, -0.0239, -0.1014,  ...,  0.0512, -0.0482, -0.0889],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.layer_norm.weight',\n",
       "              tensor([0.3022, 0.2321, 0.4688,  ..., 0.2795, 0.2805, 0.2366], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.layer_norm.bias',\n",
       "              tensor([-0.0016, -0.0031,  0.0390,  ...,  0.0057,  0.0172,  0.0232],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.0312, -0.1710,  0.0247,  ..., -0.0425,  0.0073,  0.1272],\n",
       "                      [ 0.1482, -0.0101, -0.0167,  ..., -0.0990, -0.0132, -0.1926],\n",
       "                      [-0.0429, -0.2456, -0.0160,  ..., -0.0472, -0.0958,  0.0018],\n",
       "                      ...,\n",
       "                      [-0.0179, -0.0150,  0.0976,  ...,  0.0421,  0.0188,  0.2394],\n",
       "                      [ 0.0062,  0.1625,  0.0275,  ..., -0.1488,  0.3113,  0.1961],\n",
       "                      [-0.1053, -0.1545,  0.0723,  ..., -0.1259,  0.0563,  0.0179]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0252,  0.0118, -0.0298,  ..., -0.0516, -0.0261, -0.0369],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.1248, -0.0332,  0.0641,  ..., -0.0238, -0.0585, -0.1648],\n",
       "                      [ 0.0730, -0.0629,  0.0587,  ..., -0.0223,  0.0826,  0.1190],\n",
       "                      [-0.0069,  0.0521, -0.0110,  ...,  0.0170, -0.0250,  0.1306],\n",
       "                      ...,\n",
       "                      [ 0.0190,  0.0538,  0.0018,  ...,  0.0130,  0.1287,  0.0906],\n",
       "                      [ 0.0666, -0.0218,  0.0376,  ..., -0.0827,  0.1051, -0.0078],\n",
       "                      [ 0.0364,  0.1766, -0.0866,  ..., -0.0268, -0.1214, -0.0541]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0338, -0.0230, -0.0337,  ..., -0.0417,  0.0075, -0.0231],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.final_layer_norm.weight',\n",
       "              tensor([0.2977, 0.2884, 0.0907,  ..., 0.3000, 0.3102, 0.2804], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.3.final_layer_norm.bias',\n",
       "              tensor([ 0.0339, -0.0451, -0.2057,  ...,  0.0721, -0.0982, -0.0757],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.k_proj.weight',\n",
       "              tensor([[-0.1502, -0.0655, -0.0464,  ..., -0.0716,  0.0628, -0.0141],\n",
       "                      [ 0.0973, -0.0968, -0.0685,  ...,  0.0277,  0.0281, -0.0655],\n",
       "                      [ 0.1241,  0.0006,  0.0855,  ..., -0.0012,  0.0443,  0.0102],\n",
       "                      ...,\n",
       "                      [ 0.0062, -0.0818,  0.0021,  ...,  0.1731, -0.1352,  0.1789],\n",
       "                      [ 0.0913,  0.1076,  0.0331,  ...,  0.0845,  0.1491,  0.0097],\n",
       "                      [ 0.1780,  0.1662, -0.0206,  ...,  0.1726, -0.1259,  0.0331]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.k_proj.bias',\n",
       "              tensor([-0.0122, -0.0045, -0.0035,  ..., -0.0055,  0.0098, -0.0198],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.v_proj.weight',\n",
       "              tensor([[-0.0137, -0.1230,  0.0173,  ..., -0.0407, -0.0259,  0.1208],\n",
       "                      [ 0.0603, -0.0947,  0.0621,  ..., -0.2213,  0.2030,  0.0100],\n",
       "                      [ 0.0388,  0.1585,  0.0654,  ..., -0.1092,  0.2483,  0.0741],\n",
       "                      ...,\n",
       "                      [ 0.1994, -0.2035,  0.0271,  ..., -0.0180, -0.0141, -0.1684],\n",
       "                      [ 0.0077,  0.0315,  0.0234,  ..., -0.0817, -0.0016,  0.0654],\n",
       "                      [ 0.0201, -0.1476, -0.0460,  ...,  0.0079, -0.0613, -0.0580]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.v_proj.bias',\n",
       "              tensor([-0.0748, -0.0151, -0.0098,  ..., -0.0177,  0.0221,  0.0096],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.q_proj.weight',\n",
       "              tensor([[-0.1482, -0.0313, -0.0261,  ...,  0.0143, -0.0727, -0.2050],\n",
       "                      [-0.1048,  0.0374, -0.0288,  ...,  0.0113, -0.0789, -0.0877],\n",
       "                      [-0.1944, -0.1960,  0.0334,  ..., -0.0271, -0.0679,  0.0382],\n",
       "                      ...,\n",
       "                      [-0.0759, -0.0040,  0.0032,  ..., -0.0587,  0.0014,  0.1272],\n",
       "                      [ 0.1044,  0.0782,  0.0436,  ...,  0.0217,  0.1007,  0.1067],\n",
       "                      [ 0.1711,  0.0303, -0.0457,  ...,  0.0661,  0.0205,  0.2080]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.q_proj.bias',\n",
       "              tensor([ 0.1145, -0.5338, -0.0045,  ...,  0.3750, -0.1059,  0.1378],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.out_proj.weight',\n",
       "              tensor([[ 0.0487, -0.0594, -0.0408,  ..., -0.1650,  0.0977, -0.0252],\n",
       "                      [ 0.1145,  0.0395, -0.1234,  ...,  0.2131, -0.0003,  0.1512],\n",
       "                      [-0.0614, -0.0751, -0.1269,  ..., -0.0975, -0.0543, -0.0281],\n",
       "                      ...,\n",
       "                      [-0.0918,  0.1854,  0.0849,  ...,  0.0201,  0.0785, -0.0563],\n",
       "                      [-0.0008, -0.0120, -0.1019,  ..., -0.0070,  0.0394, -0.0927],\n",
       "                      [-0.0279,  0.0379, -0.0435,  ...,  0.1431,  0.0953, -0.0249]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.attention.out_proj.bias',\n",
       "              tensor([ 0.0347, -0.0280, -0.0536,  ...,  0.0371, -0.0627, -0.0486],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.layer_norm.weight',\n",
       "              tensor([0.2921, 0.2657, 0.3213,  ..., 0.2705, 0.2848, 0.2550], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.layer_norm.bias',\n",
       "              tensor([-0.0101,  0.0150,  0.0392,  ..., -0.0178,  0.0423, -0.0033],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0902, -0.0038,  0.0692,  ..., -0.1333, -0.0415, -0.0100],\n",
       "                      [ 0.0125,  0.0420,  0.0105,  ...,  0.2528,  0.0909, -0.2038],\n",
       "                      [ 0.1759, -0.1963,  0.0629,  ...,  0.0077, -0.0309,  0.0479],\n",
       "                      ...,\n",
       "                      [-0.0072,  0.0497,  0.0847,  ..., -0.0163,  0.0302,  0.0028],\n",
       "                      [ 0.0571, -0.0950, -0.0644,  ...,  0.2633, -0.0726, -0.0075],\n",
       "                      [-0.0781,  0.0758, -0.0056,  ...,  0.1174,  0.0163,  0.0666]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0177, -0.0601, -0.0621,  ..., -0.0069, -0.0191, -0.0178],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0348, -0.0204,  0.2162,  ..., -0.0688, -0.0133,  0.0988],\n",
       "                      [-0.0241,  0.0325, -0.0941,  ..., -0.1623,  0.0238, -0.0216],\n",
       "                      [-0.0551,  0.0972,  0.0242,  ...,  0.0745,  0.0597,  0.0021],\n",
       "                      ...,\n",
       "                      [ 0.1635, -0.0185,  0.0334,  ..., -0.1604, -0.1293, -0.1636],\n",
       "                      [-0.0302,  0.0257, -0.0373,  ..., -0.1074,  0.0175, -0.0698],\n",
       "                      [ 0.0935, -0.1543, -0.1689,  ...,  0.1699,  0.0109, -0.0779]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0106, -0.0359, -0.0235,  ..., -0.0268, -0.0035, -0.0529],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.final_layer_norm.weight',\n",
       "              tensor([0.2974, 0.3008, 0.1787,  ..., 0.2958, 0.3168, 0.2826], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.4.final_layer_norm.bias',\n",
       "              tensor([-0.0016,  0.0887, -0.0941,  ..., -0.0098, -0.0092, -0.1247],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.k_proj.weight',\n",
       "              tensor([[-0.1083,  0.1882,  0.0275,  ..., -0.0930,  0.0249, -0.0907],\n",
       "                      [-0.0025, -0.0376,  0.0356,  ...,  0.0301, -0.1799,  0.0594],\n",
       "                      [-0.2200,  0.2874,  0.0415,  ...,  0.0166, -0.0642, -0.0735],\n",
       "                      ...,\n",
       "                      [ 0.0119, -0.0368,  0.0681,  ..., -0.0003, -0.0827,  0.0022],\n",
       "                      [ 0.0138,  0.1648, -0.0059,  ...,  0.0464,  0.0604, -0.0494],\n",
       "                      [ 0.1433, -0.0899, -0.0176,  ...,  0.2785, -0.0438,  0.0639]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.k_proj.bias',\n",
       "              tensor([-0.0154,  0.0009,  0.0034,  ..., -0.0044,  0.0559, -0.0204],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.v_proj.weight',\n",
       "              tensor([[ 0.1065, -0.0753, -0.0980,  ..., -0.0195,  0.0155, -0.0426],\n",
       "                      [-0.0560, -0.0637,  0.0430,  ..., -0.0229,  0.2150, -0.1534],\n",
       "                      [ 0.0993, -0.0682,  0.0321,  ..., -0.0287, -0.0292,  0.0589],\n",
       "                      ...,\n",
       "                      [ 0.0101, -0.0089, -0.0317,  ..., -0.0377, -0.0183,  0.1921],\n",
       "                      [-0.0282, -0.1333,  0.0552,  ..., -0.0822, -0.1391, -0.2454],\n",
       "                      [-0.1026,  0.0628, -0.0491,  ..., -0.0828, -0.0427, -0.1064]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.v_proj.bias',\n",
       "              tensor([-0.0233,  0.0290, -0.0555,  ..., -0.0087, -0.0494,  0.0049],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.q_proj.weight',\n",
       "              tensor([[-0.1212,  0.0468, -0.0745,  ..., -0.1494, -0.1471,  0.0701],\n",
       "                      [ 0.0728, -0.2170,  0.0783,  ..., -0.0715, -0.0154, -0.0021],\n",
       "                      [ 0.0203,  0.0970, -0.0291,  ..., -0.1226,  0.0313,  0.0214],\n",
       "                      ...,\n",
       "                      [-0.0472, -0.0136,  0.1642,  ...,  0.1491,  0.0209,  0.1453],\n",
       "                      [ 0.0958,  0.1156, -0.0315,  ..., -0.1868,  0.1290, -0.0433],\n",
       "                      [ 0.0459, -0.0160,  0.0016,  ...,  0.0757,  0.1750,  0.0698]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.q_proj.bias',\n",
       "              tensor([-0.0144,  0.1063, -0.4193,  ..., -0.2547,  0.1319, -0.0994],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.out_proj.weight',\n",
       "              tensor([[-0.1520,  0.0383, -0.0501,  ..., -0.0930,  0.0480,  0.0814],\n",
       "                      [ 0.0703, -0.0346,  0.2088,  ...,  0.0016,  0.0414, -0.1032],\n",
       "                      [ 0.0490, -0.0133,  0.0699,  ...,  0.0418, -0.0129,  0.0478],\n",
       "                      ...,\n",
       "                      [-0.0290,  0.0423, -0.0775,  ..., -0.0093, -0.0909,  0.0445],\n",
       "                      [ 0.0543, -0.1600,  0.0592,  ..., -0.0594,  0.0139,  0.0218],\n",
       "                      [-0.0763,  0.0614, -0.0901,  ..., -0.2531,  0.3076,  0.1138]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.attention.out_proj.bias',\n",
       "              tensor([ 0.0269,  0.0277, -0.0976,  ..., -0.0272, -0.0563, -0.1119],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.layer_norm.weight',\n",
       "              tensor([0.2966, 0.2679, 0.3514,  ..., 0.3170, 0.2913, 0.2573], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.layer_norm.bias',\n",
       "              tensor([-0.0020, -0.0239,  0.0367,  ..., -0.0030,  0.0506, -0.0471],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.2537, -0.1094,  0.0471,  ..., -0.0347, -0.1293,  0.0898],\n",
       "                      [ 0.0304, -0.0560, -0.0448,  ...,  0.0165,  0.0405,  0.1743],\n",
       "                      [-0.1345, -0.0218, -0.0590,  ..., -0.0259,  0.0577, -0.1115],\n",
       "                      ...,\n",
       "                      [-0.0442,  0.0896, -0.0302,  ...,  0.0163, -0.1050,  0.1119],\n",
       "                      [ 0.1591, -0.1917,  0.0564,  ..., -0.1987,  0.0887, -0.0793],\n",
       "                      [-0.1561, -0.0667, -0.0613,  ...,  0.1583, -0.1308, -0.0804]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0726,  0.0205, -0.0720,  ...,  0.0452, -0.0534, -0.0657],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.2649, -0.1274, -0.0029,  ...,  0.0418, -0.0020,  0.1431],\n",
       "                      [ 0.0712,  0.0211, -0.0141,  ..., -0.0221,  0.0577, -0.0360],\n",
       "                      [ 0.0307,  0.0725,  0.0109,  ...,  0.0110, -0.0600, -0.1174],\n",
       "                      ...,\n",
       "                      [-0.1453, -0.0785, -0.1277,  ..., -0.0663,  0.2582,  0.2073],\n",
       "                      [ 0.0108,  0.0292, -0.0432,  ...,  0.1548,  0.0629, -0.1260],\n",
       "                      [ 0.0870, -0.0636,  0.0276,  ..., -0.1357,  0.0789, -0.3189]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0263, -0.0359, -0.0037,  ..., -0.0643, -0.0520, -0.0516],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.final_layer_norm.weight',\n",
       "              tensor([0.3075, 0.2646, 0.1213,  ..., 0.2998, 0.3011, 0.2866], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.5.final_layer_norm.bias',\n",
       "              tensor([-0.0076,  0.0294, -0.1609,  ...,  0.0428, -0.0421, -0.1653],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.k_proj.weight',\n",
       "              tensor([[ 0.0458,  0.1068,  0.0406,  ...,  0.0095, -0.0283, -0.1045],\n",
       "                      [-0.0531,  0.1343,  0.0963,  ...,  0.0569, -0.0356,  0.1120],\n",
       "                      [ 0.0521,  0.0038,  0.1317,  ..., -0.1186, -0.1129,  0.0335],\n",
       "                      ...,\n",
       "                      [ 0.1267, -0.0677,  0.1930,  ...,  0.1495,  0.0366, -0.0038],\n",
       "                      [ 0.0118,  0.0425,  0.3247,  ...,  0.0980, -0.0540, -0.1834],\n",
       "                      [ 0.1667,  0.1935, -0.2099,  ...,  0.0957, -0.0285,  0.0214]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.k_proj.bias',\n",
       "              tensor([ 0.0111, -0.0078, -0.0175,  ..., -0.0449, -0.0017, -0.0076],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.v_proj.weight',\n",
       "              tensor([[ 0.0243, -0.2390,  0.1659,  ...,  0.0092, -0.1053, -0.1521],\n",
       "                      [-0.1888, -0.0191, -0.0896,  ..., -0.0188,  0.1186,  0.2407],\n",
       "                      [-0.2908, -0.1042,  0.1412,  ...,  0.1572,  0.2493,  0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0239, -0.1181,  0.0283,  ...,  0.1060,  0.0333, -0.0203],\n",
       "                      [-0.0353, -0.0019, -0.0411,  ..., -0.0090, -0.0811,  0.0931],\n",
       "                      [-0.0851, -0.0305,  0.0282,  ..., -0.0041,  0.1109, -0.1044]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.v_proj.bias',\n",
       "              tensor([-0.0476, -0.0644,  0.0347,  ...,  0.0310, -0.0153,  0.0190],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.q_proj.weight',\n",
       "              tensor([[ 0.0955,  0.2855,  0.0872,  ...,  0.1037, -0.1067, -0.0224],\n",
       "                      [-0.0310, -0.0953,  0.0813,  ..., -0.1237,  0.0605, -0.0621],\n",
       "                      [-0.0216,  0.0339,  0.1444,  ..., -0.0617, -0.0970, -0.0276],\n",
       "                      ...,\n",
       "                      [-0.0871,  0.0329,  0.0397,  ...,  0.0418, -0.0116, -0.1459],\n",
       "                      [ 0.0877,  0.1116,  0.1397,  ..., -0.1270,  0.0458, -0.0719],\n",
       "                      [-0.0516, -0.0233,  0.4097,  ..., -0.2265, -0.1153, -0.0247]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.q_proj.bias',\n",
       "              tensor([ 0.2523,  0.1725,  0.1885,  ...,  0.5249, -0.0226,  0.5791],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.out_proj.weight',\n",
       "              tensor([[-3.5622e-02,  8.6844e-02,  2.1572e-01,  ..., -4.8987e-02,\n",
       "                       -5.8903e-02,  1.6960e-01],\n",
       "                      [ 1.4580e-01,  5.5663e-02,  7.5421e-02,  ..., -4.9076e-02,\n",
       "                        2.7617e-02, -1.5888e-01],\n",
       "                      [-7.4971e-01,  3.7030e-03, -5.8752e-01,  ...,  2.0447e-01,\n",
       "                       -2.5356e-03, -1.9202e-01],\n",
       "                      ...,\n",
       "                      [ 1.7552e-03,  2.8262e-02,  3.1943e-03,  ...,  2.4045e-02,\n",
       "                       -4.7784e-02,  3.3811e-02],\n",
       "                      [ 9.0217e-02, -1.0115e-01, -2.2230e-01,  ...,  2.0728e-02,\n",
       "                       -1.2048e-01,  4.2031e-02],\n",
       "                      [ 1.3922e-01, -2.3958e-01, -3.6004e-02,  ...,  3.6417e-03,\n",
       "                       -9.4666e-02,  4.0464e-04]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.attention.out_proj.bias',\n",
       "              tensor([ 0.0138,  0.0365, -0.0377,  ..., -0.0217, -0.0794, -0.0241],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.layer_norm.weight',\n",
       "              tensor([0.2915, 0.2447, 0.3949,  ..., 0.2873, 0.2495, 0.2475], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.layer_norm.bias',\n",
       "              tensor([ 0.0318, -0.0048,  0.0414,  ..., -0.0127,  0.0526, -0.0148],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.0151, -0.0933, -0.0487,  ..., -0.1015, -0.1050,  0.0791],\n",
       "                      [-0.0657, -0.0063,  0.1824,  ..., -0.0228, -0.0453, -0.1530],\n",
       "                      [ 0.2109,  0.0373,  0.0759,  ..., -0.0368, -0.0279, -0.1026],\n",
       "                      ...,\n",
       "                      [-0.0473,  0.1246,  0.0618,  ...,  0.0820,  0.0979, -0.0571],\n",
       "                      [ 0.0458,  0.1138, -0.0345,  ..., -0.1157, -0.0948, -0.0555],\n",
       "                      [-0.0746,  0.1019, -0.0040,  ..., -0.0706,  0.1844, -0.0350]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0285, -0.0505, -0.0809,  ..., -0.0521, -0.0214, -0.0303],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.1683,  0.0112,  0.1388,  ...,  0.0354,  0.0730, -0.0687],\n",
       "                      [ 0.0706, -0.1132,  0.0030,  ..., -0.1580, -0.0242,  0.0527],\n",
       "                      [ 0.0058,  0.1221, -0.0627,  ...,  0.1359, -0.0081, -0.0364],\n",
       "                      ...,\n",
       "                      [-0.0020,  0.1827, -0.1228,  ..., -0.1194, -0.0236,  0.0120],\n",
       "                      [-0.0026,  0.0022,  0.0944,  ..., -0.0208, -0.1833, -0.0796],\n",
       "                      [ 0.0038, -0.1808, -0.1070,  ...,  0.0120,  0.1234,  0.2279]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0198,  0.0178,  0.0231,  ..., -0.0598, -0.0718, -0.0034],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.final_layer_norm.weight',\n",
       "              tensor([0.2950, 0.3130, 0.1532,  ..., 0.2774, 0.2943, 0.2884], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.6.final_layer_norm.bias',\n",
       "              tensor([ 0.0069,  0.0668, -0.0945,  ...,  0.0119,  0.0045, -0.0810],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.k_proj.weight',\n",
       "              tensor([[-0.0835, -0.1020,  0.2266,  ..., -0.0284, -0.0482, -0.0721],\n",
       "                      [-0.0533,  0.0463,  0.0932,  ..., -0.0098,  0.2319, -0.1336],\n",
       "                      [ 0.0005, -0.0019,  0.0548,  ..., -0.0367, -0.1450,  0.0034],\n",
       "                      ...,\n",
       "                      [ 0.0005, -0.0148, -0.0228,  ...,  0.0761, -0.1317,  0.0593],\n",
       "                      [-0.1486, -0.0067,  0.1679,  ..., -0.1251,  0.1967,  0.0035],\n",
       "                      [ 0.1625,  0.0164, -0.0460,  ..., -0.0802,  0.0174,  0.1565]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.k_proj.bias',\n",
       "              tensor([ 0.0069, -0.0222, -0.0443,  ..., -0.1297, -0.0706, -0.0159],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.v_proj.weight',\n",
       "              tensor([[-0.1856,  0.1261,  0.0478,  ...,  0.1768, -0.1066, -0.1072],\n",
       "                      [ 0.1171,  0.2300, -0.0047,  ...,  0.2785,  0.0641, -0.0063],\n",
       "                      [ 0.0319, -0.1443, -0.0007,  ..., -0.1551,  0.0521,  0.0004],\n",
       "                      ...,\n",
       "                      [ 0.1962,  0.0898,  0.0976,  ...,  0.1742,  0.1512, -0.0478],\n",
       "                      [-0.2520, -0.0069,  0.1080,  ..., -0.1610,  0.0273,  0.0439],\n",
       "                      [-0.1201,  0.0602,  0.0347,  ...,  0.0468, -0.1094,  0.0497]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.v_proj.bias',\n",
       "              tensor([-0.0231, -0.0154, -0.0199,  ...,  0.0523, -0.0340, -0.0639],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.q_proj.weight',\n",
       "              tensor([[ 0.0782, -0.0918,  0.1771,  ..., -0.2299, -0.0242,  0.0552],\n",
       "                      [-0.1429,  0.0887,  0.0434,  ..., -0.0171,  0.0438, -0.0077],\n",
       "                      [-0.0934, -0.1847,  0.2038,  ...,  0.0277, -0.0906,  0.0296],\n",
       "                      ...,\n",
       "                      [-0.0654, -0.0551,  0.0225,  ..., -0.0841,  0.0240,  0.0657],\n",
       "                      [ 0.0213,  0.3120, -0.0577,  ..., -0.1740,  0.0534,  0.2049],\n",
       "                      [ 0.0827,  0.0790, -0.0982,  ...,  0.1862,  0.0204, -0.0368]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.q_proj.bias',\n",
       "              tensor([ 0.0742,  0.0263,  0.3139,  ...,  0.2023,  0.1736, -0.0148],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.out_proj.weight',\n",
       "              tensor([[ 0.1060, -0.1020, -0.1650,  ...,  0.0324,  0.1000, -0.1714],\n",
       "                      [-0.0356, -0.1473,  0.0623,  ...,  0.1018, -0.0326, -0.0818],\n",
       "                      [-0.0593, -0.1445,  0.0522,  ..., -0.1585,  0.1007,  0.0906],\n",
       "                      ...,\n",
       "                      [ 0.0080, -0.0781,  0.0921,  ...,  0.1654,  0.0899,  0.0596],\n",
       "                      [ 0.0638, -0.1158, -0.0581,  ...,  0.0418, -0.1076, -0.0446],\n",
       "                      [ 0.0552, -0.0074,  0.0085,  ..., -0.0171, -0.0777, -0.1338]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.attention.out_proj.bias',\n",
       "              tensor([-0.0260,  0.0537, -0.0452,  ..., -0.0146, -0.0622, -0.0380],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.layer_norm.weight',\n",
       "              tensor([0.3291, 0.2713, 0.2065,  ..., 0.2877, 0.2811, 0.2975], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.layer_norm.bias',\n",
       "              tensor([ 0.0252, -0.0121,  0.0275,  ..., -0.0127,  0.0411,  0.0067],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.0152, -0.0769, -0.0760,  ...,  0.0129, -0.0731,  0.1756],\n",
       "                      [ 0.0363,  0.0298,  0.0436,  ...,  0.0107, -0.2092, -0.0828],\n",
       "                      [-0.0691,  0.0475, -0.0329,  ...,  0.1414,  0.0246,  0.0586],\n",
       "                      ...,\n",
       "                      [ 0.2360, -0.0515,  0.0013,  ...,  0.0354,  0.0512,  0.0983],\n",
       "                      [ 0.0166, -0.1108, -0.0043,  ...,  0.0163,  0.2043, -0.1333],\n",
       "                      [-0.0787, -0.2722,  0.0927,  ..., -0.2377, -0.0820,  0.1618]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0505, -0.0552,  0.0071,  ..., -0.0461, -0.0584, -0.0803],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.1455, -0.0785,  0.1346,  ...,  0.0549,  0.0086,  0.0355],\n",
       "                      [-0.3280,  0.0094, -0.0140,  ..., -0.0724,  0.1739, -0.0255],\n",
       "                      [-0.0457, -0.0906,  0.0970,  ...,  0.0028,  0.1139,  0.0658],\n",
       "                      ...,\n",
       "                      [-0.0683,  0.1172, -0.1289,  ...,  0.1833,  0.1975, -0.0786],\n",
       "                      [-0.0583,  0.2376, -0.0076,  ...,  0.2063, -0.0554, -0.0243],\n",
       "                      [ 0.1475, -0.1537, -0.1676,  ..., -0.0289,  0.1116, -0.0360]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0096, -0.0239,  0.0460,  ..., -0.0405, -0.0859,  0.0500],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.final_layer_norm.weight',\n",
       "              tensor([0.3463, 0.3219, 0.1595,  ..., 0.3102, 0.3182, 0.2821], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.7.final_layer_norm.bias',\n",
       "              tensor([-0.0701,  0.1050, -0.1128,  ..., -0.0039, -0.0325, -0.0767],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.k_proj.weight',\n",
       "              tensor([[ 0.0207,  0.0107, -0.0825,  ..., -0.1852,  0.0065,  0.0357],\n",
       "                      [ 0.1477, -0.1155,  0.0054,  ...,  0.1521, -0.0401, -0.0667],\n",
       "                      [ 0.0295,  0.0510,  0.0121,  ...,  0.0636,  0.0613,  0.0375],\n",
       "                      ...,\n",
       "                      [-0.1248, -0.0445,  0.2041,  ...,  0.1673,  0.0036,  0.0392],\n",
       "                      [ 0.0617, -0.2184, -0.0042,  ...,  0.0860,  0.0883,  0.1243],\n",
       "                      [-0.0482, -0.0210,  0.0470,  ..., -0.1305,  0.0548,  0.2116]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.k_proj.bias',\n",
       "              tensor([-0.0074,  0.0060,  0.0524,  ..., -0.0302,  0.0190, -0.0144],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.v_proj.weight',\n",
       "              tensor([[ 0.0289,  0.1551, -0.0248,  ..., -0.1380, -0.0449,  0.0074],\n",
       "                      [ 0.0126, -0.0048, -0.0525,  ...,  0.0904,  0.1770, -0.0345],\n",
       "                      [ 0.0904,  0.0108,  0.0308,  ...,  0.0027,  0.0597,  0.1372],\n",
       "                      ...,\n",
       "                      [-0.0189,  0.0385, -0.0071,  ..., -0.0534, -0.0653,  0.1202],\n",
       "                      [-0.0806,  0.1480, -0.0159,  ...,  0.0516, -0.1306, -0.1627],\n",
       "                      [ 0.1235,  0.1671, -0.0863,  ..., -0.0808,  0.0733,  0.0003]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.v_proj.bias',\n",
       "              tensor([ 0.0149,  0.0030,  0.0081,  ..., -0.0137, -0.0031,  0.0850],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.q_proj.weight',\n",
       "              tensor([[ 0.1108, -0.1929, -0.1638,  ..., -0.1218,  0.1225, -0.1166],\n",
       "                      [ 0.1660, -0.0580,  0.1096,  ..., -0.0894, -0.1619,  0.0759],\n",
       "                      [-0.0425, -0.1487, -0.0110,  ...,  0.1268, -0.1856, -0.0047],\n",
       "                      ...,\n",
       "                      [-0.0211,  0.1555,  0.0537,  ...,  0.1519, -0.0439, -0.0458],\n",
       "                      [ 0.0869, -0.0359, -0.1165,  ..., -0.0140,  0.0837, -0.0610],\n",
       "                      [-0.0450,  0.1324,  0.2071,  ...,  0.0422,  0.0606, -0.0840]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.q_proj.bias',\n",
       "              tensor([-0.0566, -0.4449, -0.3524,  ..., -0.0395, -0.0547,  0.2805],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.out_proj.weight',\n",
       "              tensor([[ 0.1349, -0.0901, -0.1499,  ..., -0.0597, -0.0797, -0.0584],\n",
       "                      [-0.1832, -0.0462, -0.1220,  ..., -0.0186,  0.0612, -0.1799],\n",
       "                      [-0.0611,  0.1079, -0.0618,  ..., -0.1951,  0.1313,  0.0116],\n",
       "                      ...,\n",
       "                      [ 0.0871,  0.0434, -0.0186,  ..., -0.1306,  0.0486,  0.2171],\n",
       "                      [ 0.1258, -0.1508, -0.1072,  ..., -0.0491,  0.0404,  0.1476],\n",
       "                      [ 0.0706, -0.2402, -0.1376,  ..., -0.0737, -0.0658, -0.1316]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.attention.out_proj.bias',\n",
       "              tensor([ 0.0049,  0.0879, -0.0193,  ...,  0.0504, -0.0817,  0.0591],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.layer_norm.weight',\n",
       "              tensor([0.2607, 0.2352, 0.2638,  ..., 0.2906, 0.2618, 0.2490], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.layer_norm.bias',\n",
       "              tensor([ 0.0087, -0.0198,  0.0099,  ..., -0.0087,  0.0168,  0.0235],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.1393, -0.0154, -0.0320,  ..., -0.1114,  0.2161,  0.0076],\n",
       "                      [ 0.2622, -0.3780,  0.0535,  ..., -0.0317,  0.0553, -0.1640],\n",
       "                      [ 0.2126, -0.1486,  0.1336,  ...,  0.1599, -0.0117,  0.0263],\n",
       "                      ...,\n",
       "                      [ 0.1350, -0.1350,  0.1163,  ..., -0.1313, -0.0965,  0.1716],\n",
       "                      [ 0.0971, -0.2287,  0.0678,  ...,  0.0553,  0.0934,  0.0047],\n",
       "                      [ 0.1079, -0.1596, -0.0949,  ...,  0.2183, -0.1359, -0.0612]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0885, -0.1108, -0.0621,  ..., -0.0603, -0.0886, -0.0341],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.1993,  0.2038,  0.3749,  ..., -0.0224,  0.0983,  0.0094],\n",
       "                      [ 0.0351,  0.1393,  0.0286,  ..., -0.0203, -0.1266,  0.0843],\n",
       "                      [-0.0909, -0.0147, -0.0246,  ..., -0.0418,  0.0189,  0.0672],\n",
       "                      ...,\n",
       "                      [-0.1550,  0.1127,  0.1293,  ...,  0.2045,  0.1265,  0.0303],\n",
       "                      [-0.1484,  0.2419, -0.1692,  ...,  0.0549,  0.2148,  0.0231],\n",
       "                      [-0.1475,  0.0813, -0.0180,  ...,  0.1605, -0.2380, -0.0460]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0229, -0.0230,  0.0653,  ..., -0.0088, -0.0743,  0.0494],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.final_layer_norm.weight',\n",
       "              tensor([0.3447, 0.3058, 0.1635,  ..., 0.3250, 0.3160, 0.2683], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.8.final_layer_norm.bias',\n",
       "              tensor([-0.0673,  0.0443, -0.0936,  ...,  0.0326, -0.0141,  0.0183],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.k_proj.weight',\n",
       "              tensor([[ 0.0876,  0.0395, -0.0947,  ..., -0.0893,  0.0712,  0.0514],\n",
       "                      [-0.0614, -0.0584, -0.0263,  ...,  0.1799,  0.1165,  0.0357],\n",
       "                      [-0.1602,  0.2214,  0.0748,  ..., -0.0810,  0.1983, -0.0599],\n",
       "                      ...,\n",
       "                      [ 0.0347,  0.0163,  0.1433,  ...,  0.2101, -0.0013,  0.0880],\n",
       "                      [-0.0949,  0.0486,  0.5731,  ...,  0.0386, -0.0347, -0.0685],\n",
       "                      [ 0.0584,  0.0211,  0.0478,  ...,  0.0986, -0.0385,  0.1095]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.k_proj.bias',\n",
       "              tensor([ 0.0261,  0.1612, -0.0154,  ...,  0.0383,  0.0766, -0.0343],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.v_proj.weight',\n",
       "              tensor([[-0.0487, -0.1233, -0.0337,  ...,  0.0990, -0.1381, -0.0827],\n",
       "                      [ 0.0796,  0.0434,  0.0235,  ..., -0.1108,  0.1431,  0.0990],\n",
       "                      [ 0.0645,  0.0096,  0.0217,  ...,  0.0061,  0.1220, -0.2526],\n",
       "                      ...,\n",
       "                      [-0.1708, -0.1004,  0.0093,  ..., -0.0492, -0.1323, -0.0606],\n",
       "                      [ 0.0254,  0.0293,  0.0896,  ..., -0.0733, -0.0360, -0.0776],\n",
       "                      [ 0.1944, -0.0261,  0.0299,  ...,  0.1974,  0.0879,  0.0411]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.v_proj.bias',\n",
       "              tensor([ 0.0237, -0.1781,  0.0132,  ..., -0.0214, -0.0115, -0.0339],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.q_proj.weight',\n",
       "              tensor([[-0.1949, -0.0733, -0.0165,  ..., -0.1863,  0.1903, -0.0737],\n",
       "                      [-0.0531, -0.0288,  0.0251,  ...,  0.1404, -0.1563,  0.0937],\n",
       "                      [ 0.0059,  0.1474,  0.1051,  ...,  0.1651,  0.0994, -0.1072],\n",
       "                      ...,\n",
       "                      [-0.0559,  0.0150,  0.3621,  ..., -0.0714, -0.0457,  0.0681],\n",
       "                      [-0.0432, -0.0646,  0.9687,  ...,  0.0845, -0.0917,  0.1377],\n",
       "                      [ 0.0619,  0.0296,  0.1174,  ..., -0.0043, -0.0929,  0.0165]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.q_proj.bias',\n",
       "              tensor([-0.2463, -0.3608, -0.1581,  ...,  0.0205, -0.3099,  0.0850],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.out_proj.weight',\n",
       "              tensor([[-0.0090, -0.0056, -0.0349,  ...,  0.0250, -0.0765, -0.0358],\n",
       "                      [ 0.0562, -0.0216,  0.0687,  ...,  0.1349,  0.1133,  0.0872],\n",
       "                      [-0.0042,  0.0716,  0.0115,  ...,  0.1116,  0.2829, -0.0405],\n",
       "                      ...,\n",
       "                      [ 0.1220,  0.0758, -0.2172,  ...,  0.0849, -0.0831, -0.0752],\n",
       "                      [ 0.0606, -0.0855, -0.1659,  ...,  0.0063, -0.0218, -0.0384],\n",
       "                      [ 0.1277, -0.0973, -0.0502,  ..., -0.1511,  0.1767,  0.0499]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.attention.out_proj.bias',\n",
       "              tensor([ 0.0427, -0.0037,  0.0108,  ..., -0.0162, -0.0044,  0.0280],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.layer_norm.weight',\n",
       "              tensor([0.2938, 0.2637, 0.3289,  ..., 0.3033, 0.3068, 0.2734], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.layer_norm.bias',\n",
       "              tensor([-0.0072, -0.0092, -0.0093,  ..., -0.0104,  0.0090,  0.0091],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0699,  0.0115,  0.0597,  ..., -0.0870, -0.0914, -0.0132],\n",
       "                      [-0.2725, -0.0154, -0.1580,  ...,  0.0276, -0.1834, -0.1457],\n",
       "                      [-0.0631,  0.0425,  0.0462,  ..., -0.1425, -0.1743,  0.1393],\n",
       "                      ...,\n",
       "                      [-0.0055,  0.1142,  0.3842,  ...,  0.1027,  0.1427, -0.0272],\n",
       "                      [-0.0390,  0.0132, -0.0605,  ..., -0.0414,  0.0623,  0.0495],\n",
       "                      [-0.1508,  0.0465,  0.0268,  ...,  0.0420, -0.1479,  0.0310]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias',\n",
       "              tensor([ 0.1654, -0.0893, -0.0555,  ..., -0.0266,  0.0300, -0.0279],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.1319,  0.0169, -0.1681,  ...,  0.0075,  0.0180,  0.1951],\n",
       "                      [ 0.0047,  0.1921,  0.1237,  ..., -0.0509, -0.0434, -0.0909],\n",
       "                      [ 0.1375, -0.1033, -0.0513,  ..., -0.0059,  0.0057,  0.0648],\n",
       "                      ...,\n",
       "                      [ 0.1101,  0.0198, -0.0591,  ...,  0.0410,  0.0862,  0.0222],\n",
       "                      [ 0.0178,  0.0427, -0.2628,  ..., -0.1331,  0.0213, -0.0221],\n",
       "                      [ 0.0063, -0.1526,  0.1113,  ..., -0.0140, -0.0466, -0.0104]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0114, -0.0364,  0.0482,  ..., -0.0191, -0.0683,  0.0076],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.final_layer_norm.weight',\n",
       "              tensor([0.3853, 0.3548, 0.1912,  ..., 0.3706, 0.3570, 0.3137], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.9.final_layer_norm.bias',\n",
       "              tensor([-0.0687,  0.1382, -0.0940,  ...,  0.0689,  0.0182, -0.0440],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.k_proj.weight',\n",
       "              tensor([[-0.1683, -0.1340, -0.1638,  ..., -0.1601, -0.0404,  0.0097],\n",
       "                      [-0.1643, -0.0214, -0.0037,  ...,  0.0818,  0.1141,  0.0041],\n",
       "                      [-0.0066, -0.0654, -0.0135,  ..., -0.0152,  0.0884,  0.0682],\n",
       "                      ...,\n",
       "                      [ 0.0376, -0.1811, -0.0827,  ..., -0.1699, -0.1580,  0.0882],\n",
       "                      [ 0.1210,  0.0733,  0.1679,  ..., -0.1194,  0.1992,  0.2239],\n",
       "                      [-0.0763,  0.1088, -0.1508,  ..., -0.0923,  0.1229, -0.0675]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.k_proj.bias',\n",
       "              tensor([ 0.0023, -0.0061, -0.0027,  ...,  0.0166,  0.0368, -0.0662],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.v_proj.weight',\n",
       "              tensor([[-0.0135, -0.0021, -0.0827,  ...,  0.1416, -0.0811,  0.0267],\n",
       "                      [-0.1454, -0.0026,  0.0093,  ..., -0.0981,  0.0191, -0.0382],\n",
       "                      [ 0.1313, -0.0359, -0.1874,  ..., -0.0955, -0.0041,  0.2307],\n",
       "                      ...,\n",
       "                      [-0.0003, -0.0597, -0.0021,  ..., -0.0172, -0.1024,  0.1473],\n",
       "                      [ 0.0242,  0.0214, -0.0598,  ...,  0.0672,  0.0675,  0.0950],\n",
       "                      [ 0.0061,  0.0879,  0.0600,  ...,  0.1820,  0.0653, -0.1758]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.v_proj.bias',\n",
       "              tensor([ 0.0413, -0.0141, -0.0267,  ...,  0.0361, -0.0192,  0.0699],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.q_proj.weight',\n",
       "              tensor([[ 0.0245,  0.1270, -0.0720,  ...,  0.0721,  0.0175, -0.0234],\n",
       "                      [ 0.2036,  0.0589,  0.2449,  ...,  0.0358, -0.1150, -0.0850],\n",
       "                      [ 0.0003, -0.0065, -0.1554,  ..., -0.0937, -0.1383,  0.1988],\n",
       "                      ...,\n",
       "                      [ 0.1377, -0.1087, -0.0496,  ...,  0.0692, -0.1493, -0.0091],\n",
       "                      [-0.0636,  0.0005,  0.1230,  ..., -0.0481,  0.0416,  0.2235],\n",
       "                      [-0.1730,  0.1542, -0.1061,  ..., -0.0014,  0.2677, -0.0497]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.q_proj.bias',\n",
       "              tensor([ 0.1800,  0.0245,  0.1723,  ...,  0.1922,  0.1727, -0.0673],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.out_proj.weight',\n",
       "              tensor([[-0.1263, -0.0759,  0.0122,  ...,  0.0353,  0.0122, -0.0864],\n",
       "                      [ 0.1585,  0.0638, -0.2296,  ...,  0.0552, -0.0242, -0.0591],\n",
       "                      [ 0.1689,  0.1568,  0.1593,  ...,  0.0313,  0.0498, -0.0857],\n",
       "                      ...,\n",
       "                      [-0.0433, -0.0132,  0.0592,  ..., -0.0332, -0.0595, -0.1347],\n",
       "                      [-0.0867, -0.1078, -0.0228,  ...,  0.0843, -0.0175, -0.0188],\n",
       "                      [ 0.0383,  0.0541, -0.0450,  ..., -0.1048, -0.0698,  0.1508]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.attention.out_proj.bias',\n",
       "              tensor([ 0.0097, -0.0202,  0.0797,  ...,  0.0200,  0.0316, -0.0107],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.layer_norm.weight',\n",
       "              tensor([0.3127, 0.2577, 0.2522,  ..., 0.2942, 0.3053, 0.2710], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.layer_norm.bias',\n",
       "              tensor([ 0.0229, -0.0233,  0.0231,  ...,  0.0148,  0.0182,  0.0069],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0178,  0.0416,  0.0658,  ...,  0.0471,  0.1039,  0.0287],\n",
       "                      [ 0.0381,  0.0526, -0.1271,  ..., -0.0783, -0.0064,  0.1798],\n",
       "                      [ 0.0519, -0.0737,  0.0125,  ..., -0.0081, -0.0765, -0.1019],\n",
       "                      ...,\n",
       "                      [-0.1234, -0.1060,  0.2590,  ...,  0.0975,  0.0789,  0.1002],\n",
       "                      [-0.0376,  0.0256,  0.1628,  ..., -0.3761, -0.0188,  0.1330],\n",
       "                      [ 0.0433, -0.2847,  0.0774,  ..., -0.0800,  0.0966,  0.0516]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias',\n",
       "              tensor([ 0.0454, -0.0058,  0.0253,  ..., -0.0625, -0.0547, -0.0727],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.0327,  0.1133, -0.0884,  ..., -0.0358, -0.1046, -0.0281],\n",
       "                      [ 0.0232,  0.0405, -0.0792,  ..., -0.0295, -0.0494,  0.0077],\n",
       "                      [-0.0477,  0.0954,  0.0071,  ..., -0.0235,  0.2176,  0.1445],\n",
       "                      ...,\n",
       "                      [-0.0338,  0.1468,  0.0954,  ...,  0.0435, -0.0277, -0.0496],\n",
       "                      [-0.0747, -0.0866,  0.0174,  ...,  0.3760,  0.0032,  0.0209],\n",
       "                      [-0.0349,  0.1064,  0.0737,  ...,  0.1704, -0.0584,  0.3650]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0120, -0.0487,  0.0911,  ...,  0.0117, -0.0250,  0.0175],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.final_layer_norm.weight',\n",
       "              tensor([0.3992, 0.3662, 0.2122,  ..., 0.3777, 0.4057, 0.3765], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.10.final_layer_norm.bias',\n",
       "              tensor([-0.0069,  0.0548, -0.0325,  ...,  0.0705,  0.1413, -0.1424],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.k_proj.weight',\n",
       "              tensor([[-0.0298, -0.0064,  0.0190,  ...,  0.0127, -0.0797,  0.0604],\n",
       "                      [ 0.1781, -0.0154, -0.0716,  ..., -0.0573, -0.2334, -0.0129],\n",
       "                      [ 0.1033, -0.1689, -0.0591,  ...,  0.0402,  0.0328,  0.1770],\n",
       "                      ...,\n",
       "                      [-0.0228,  0.0854,  0.1121,  ...,  0.0823,  0.1691, -0.0799],\n",
       "                      [ 0.0228, -0.0712,  0.0601,  ...,  0.0565, -0.0322, -0.0476],\n",
       "                      [ 0.1119, -0.2063, -0.0377,  ..., -0.0441, -0.0050,  0.1197]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.k_proj.bias',\n",
       "              tensor([0.0042, 0.0031, 0.0098,  ..., 0.0113, 0.0066, 0.0100], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.v_proj.weight',\n",
       "              tensor([[ 0.0062,  0.0050, -0.2112,  ..., -0.0071,  0.1661, -0.0703],\n",
       "                      [ 0.0535,  0.1251, -0.1132,  ..., -0.0323,  0.0294,  0.0652],\n",
       "                      [-0.0640,  0.0145,  0.1749,  ...,  0.1684,  0.0638, -0.0803],\n",
       "                      ...,\n",
       "                      [ 0.1099,  0.2201, -0.0985,  ...,  0.0894,  0.0357,  0.1390],\n",
       "                      [ 0.0277, -0.0153,  0.1146,  ..., -0.0636,  0.3705,  0.2174],\n",
       "                      [-0.2031, -0.0222, -0.0411,  ...,  0.0243, -0.1275,  0.2215]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.v_proj.bias',\n",
       "              tensor([-0.0078,  0.0023, -0.0984,  ..., -0.0133, -0.0100, -0.0155],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.q_proj.weight',\n",
       "              tensor([[-0.0410,  0.0551, -0.0310,  ..., -0.0175,  0.0052, -0.0525],\n",
       "                      [-0.1059, -0.1189,  0.0145,  ...,  0.0503, -0.2511,  0.0513],\n",
       "                      [-0.0591,  0.0057, -0.0534,  ..., -0.1344,  0.0055,  0.1202],\n",
       "                      ...,\n",
       "                      [ 0.0947,  0.0900,  0.2075,  ..., -0.1967, -0.0167, -0.0525],\n",
       "                      [ 0.0525, -0.1018,  0.0747,  ..., -0.0271, -0.1959,  0.0426],\n",
       "                      [-0.0542, -0.0954,  0.0398,  ..., -0.0281,  0.0029, -0.0860]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.q_proj.bias',\n",
       "              tensor([-0.1476, -0.0536, -0.1333,  ..., -0.2815,  0.3717,  0.0974],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.out_proj.weight',\n",
       "              tensor([[-0.0374, -0.1463,  0.1460,  ...,  0.0474, -0.1590,  0.2041],\n",
       "                      [ 0.0725, -0.1538, -0.0842,  ..., -0.0571, -0.0029, -0.0550],\n",
       "                      [ 0.2010, -0.0392, -0.0236,  ...,  0.1529, -0.2362, -0.0865],\n",
       "                      ...,\n",
       "                      [ 0.1579, -0.0535, -0.2337,  ..., -0.0291,  0.0923,  0.0314],\n",
       "                      [-0.0836, -0.0467, -0.0346,  ..., -0.0269, -0.2290,  0.0776],\n",
       "                      [-0.1253,  0.0167,  0.0815,  ..., -0.0850, -0.2168, -0.2250]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.attention.out_proj.bias',\n",
       "              tensor([ 0.0305, -0.0567,  0.0236,  ...,  0.0020,  0.0216,  0.0005],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.layer_norm.weight',\n",
       "              tensor([0.3394, 0.2760, 0.2672,  ..., 0.3040, 0.3002, 0.2784], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.layer_norm.bias',\n",
       "              tensor([-0.0167, -0.0139,  0.0295,  ...,  0.0304,  0.0258, -0.0138],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.0121, -0.0306,  0.0677,  ..., -0.0035,  0.0006,  0.0113],\n",
       "                      [ 0.1277,  0.0585,  0.1760,  ..., -0.0895,  0.1544, -0.0110],\n",
       "                      [-0.0687, -0.0053, -0.1868,  ..., -0.1245,  0.0451,  0.1639],\n",
       "                      ...,\n",
       "                      [ 0.2536, -0.1949, -0.0295,  ..., -0.0250,  0.0035, -0.2116],\n",
       "                      [ 0.0705,  0.1333, -0.0538,  ...,  0.1972,  0.0357,  0.0031],\n",
       "                      [ 0.2928, -0.0299,  0.0270,  ...,  0.1451, -0.0179, -0.3509]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias',\n",
       "              tensor([ 0.0490, -0.0146,  0.0037,  ..., -0.0677, -0.0461, -0.0836],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0623, -0.1659, -0.2409,  ...,  0.2114,  0.0936,  0.2784],\n",
       "                      [-0.0235,  0.0692,  0.1331,  ..., -0.1665,  0.1361,  0.2220],\n",
       "                      [-0.0855, -0.0147, -0.0893,  ...,  0.1585, -0.2179, -0.1530],\n",
       "                      ...,\n",
       "                      [ 0.0429, -0.0878,  0.0210,  ..., -0.0905,  0.0909,  0.1514],\n",
       "                      [-0.0418, -0.0764,  0.1156,  ..., -0.1502,  0.2206,  0.2551],\n",
       "                      [ 0.0128, -0.0553, -0.2007,  ..., -0.3723, -0.0976, -0.3105]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.feed_forward.output_dense.bias',\n",
       "              tensor([-0.1047, -0.0643,  0.0246,  ...,  0.0173, -0.0534,  0.0586],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.final_layer_norm.weight',\n",
       "              tensor([0.4363, 0.4132, 0.2385,  ..., 0.4163, 0.4530, 0.4197], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.11.final_layer_norm.bias',\n",
       "              tensor([-0.0599,  0.0718,  0.0113,  ...,  0.0825,  0.1562, -0.2039],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.k_proj.weight',\n",
       "              tensor([[-0.0296, -0.1670, -0.2056,  ..., -0.0839, -0.1283, -0.1810],\n",
       "                      [-0.0031, -0.0560,  0.0282,  ...,  0.0581,  0.1253,  0.0821],\n",
       "                      [ 0.1810, -0.1267,  0.0151,  ...,  0.0177,  0.1498,  0.2203],\n",
       "                      ...,\n",
       "                      [ 0.0378,  0.1045, -0.0812,  ..., -0.1504,  0.0060, -0.1040],\n",
       "                      [ 0.1180,  0.1008, -0.0900,  ...,  0.1016,  0.0350, -0.0008],\n",
       "                      [ 0.0944,  0.0650, -0.0511,  ...,  0.0126, -0.1475, -0.1902]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.k_proj.bias',\n",
       "              tensor([0.0019, 0.0012, 0.0070,  ..., 0.0420, 0.1932, 0.0051], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.v_proj.weight',\n",
       "              tensor([[ 0.0683, -0.0155, -0.0171,  ..., -0.1329,  0.2030, -0.0155],\n",
       "                      [ 0.0396,  0.0381, -0.0408,  ...,  0.0296,  0.0319,  0.1161],\n",
       "                      [ 0.1674,  0.0777,  0.1754,  ...,  0.0103,  0.0850,  0.0885],\n",
       "                      ...,\n",
       "                      [-0.1102, -0.0851, -0.0148,  ..., -0.0028, -0.2064,  0.0278],\n",
       "                      [ 0.0122, -0.1330,  0.0642,  ...,  0.1230, -0.2444,  0.0990],\n",
       "                      [-0.1051, -0.1802, -0.1140,  ..., -0.0871, -0.0696, -0.0089]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.v_proj.bias',\n",
       "              tensor([-0.0209,  0.0015, -0.0720,  ..., -0.0004,  0.0426, -0.0471],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.q_proj.weight',\n",
       "              tensor([[ 0.0749, -0.0639, -0.0080,  ...,  0.0435, -0.0265, -0.0877],\n",
       "                      [ 0.1845,  0.2142,  0.0064,  ...,  0.0144, -0.0727,  0.0992],\n",
       "                      [-0.0898,  0.0018,  0.0469,  ..., -0.1150,  0.2492,  0.0342],\n",
       "                      ...,\n",
       "                      [-0.0195, -0.0333,  0.0232,  ...,  0.0625, -0.0402, -0.0408],\n",
       "                      [-0.0206,  0.0703, -0.0195,  ...,  0.1190, -0.0847, -0.0155],\n",
       "                      [ 0.0921,  0.1520, -0.0247,  ..., -0.0038, -0.2099, -0.0649]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.q_proj.bias',\n",
       "              tensor([ 0.6118, -0.0156,  0.0272,  ..., -0.5455, -0.7676, -0.1832],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.out_proj.weight',\n",
       "              tensor([[ 0.1360,  0.0391,  0.0203,  ...,  0.1444, -0.0122,  0.0987],\n",
       "                      [-0.0324,  0.0246, -0.0748,  ...,  0.0217,  0.0569,  0.1622],\n",
       "                      [-0.0477,  0.1096, -0.1808,  ..., -0.0849, -0.0628,  0.1634],\n",
       "                      ...,\n",
       "                      [-0.0324,  0.1019,  0.0747,  ..., -0.0364, -0.1472,  0.0927],\n",
       "                      [ 0.0189, -0.2495, -0.0846,  ...,  0.1785,  0.1950,  0.0130],\n",
       "                      [ 0.0299,  0.0201,  0.1485,  ..., -0.0128, -0.0489, -0.0260]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.attention.out_proj.bias',\n",
       "              tensor([-0.0396, -0.0176,  0.1124,  ...,  0.0344,  0.0315, -0.0341],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.layer_norm.weight',\n",
       "              tensor([0.3482, 0.2845, 0.2836,  ..., 0.3325, 0.3288, 0.2994], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.layer_norm.bias',\n",
       "              tensor([-0.0156,  0.0092,  0.0380,  ...,  0.0442,  0.0225, -0.0121],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.3451,  0.0900,  0.1673,  ..., -0.0723,  0.0339,  0.0621],\n",
       "                      [-0.0384, -0.1500,  0.0528,  ..., -0.0702,  0.0783,  0.1522],\n",
       "                      [ 0.0525, -0.1169,  0.1449,  ..., -0.1211, -0.0577,  0.1031],\n",
       "                      ...,\n",
       "                      [ 0.1876, -0.2729, -0.1639,  ..., -0.0927,  0.0676, -0.0685],\n",
       "                      [-0.1888, -0.2868, -0.0025,  ..., -0.2319, -0.0920,  0.0085],\n",
       "                      [ 0.1135, -0.0171,  0.0896,  ...,  0.0479,  0.0025,  0.2093]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0722, -0.0678, -0.0104,  ..., -0.0900, -0.0649,  0.0063],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.2477, -0.0044, -0.0469,  ...,  0.0643, -0.2544,  0.0270],\n",
       "                      [ 0.0760, -0.3173,  0.0367,  ...,  0.0456,  0.1087,  0.1389],\n",
       "                      [ 0.2511,  0.2894, -0.1028,  ...,  0.0048, -0.0150, -0.1363],\n",
       "                      ...,\n",
       "                      [-0.0873,  0.0294,  0.0187,  ...,  0.0646, -0.2009,  0.0490],\n",
       "                      [ 0.0193, -0.0515,  0.1276,  ..., -0.2259,  0.0222, -0.0979],\n",
       "                      [-0.0384, -0.1145, -0.0369,  ..., -0.0676, -0.0344, -0.0690]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0518,  0.0334,  0.0611,  ..., -0.0125,  0.0194,  0.0268],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.final_layer_norm.weight',\n",
       "              tensor([0.4326, 0.4213, 0.2224,  ..., 0.4252, 0.4567, 0.3995], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.12.final_layer_norm.bias',\n",
       "              tensor([ 0.0059,  0.1183,  0.0011,  ...,  0.0967,  0.1576, -0.1876],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.k_proj.weight',\n",
       "              tensor([[-0.1629, -0.0773, -0.0530,  ..., -0.0248,  0.1060,  0.1245],\n",
       "                      [-0.0068,  0.1173, -0.2693,  ...,  0.1210,  0.0993,  0.0172],\n",
       "                      [-0.0266, -0.0576,  0.1012,  ..., -0.1073,  0.0583,  0.0280],\n",
       "                      ...,\n",
       "                      [ 0.0682, -0.0097, -0.0378,  ..., -0.2170,  0.1151, -0.0544],\n",
       "                      [ 0.0972, -0.0598, -0.0130,  ..., -0.0861, -0.0218, -0.1529],\n",
       "                      [-0.1868, -0.0777, -0.0308,  ...,  0.0192,  0.1219,  0.0279]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.k_proj.bias',\n",
       "              tensor([ 0.2195, -0.8257, -0.1682,  ...,  0.1370, -0.0649, -0.0677],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.v_proj.weight',\n",
       "              tensor([[ 0.0502, -0.0741, -0.0007,  ...,  0.0217,  0.0573,  0.1318],\n",
       "                      [ 0.2228,  0.1321, -0.1022,  ...,  0.0690, -0.0437,  0.1667],\n",
       "                      [-0.0990,  0.0920, -0.1069,  ...,  0.3172,  0.0971, -0.1422],\n",
       "                      ...,\n",
       "                      [ 0.1534, -0.0465,  0.0482,  ..., -0.0513, -0.0555, -0.0159],\n",
       "                      [-0.1265,  0.0622, -0.0352,  ..., -0.0725,  0.0476, -0.1028],\n",
       "                      [-0.2104,  0.2599, -0.0238,  ..., -0.0414, -0.0422, -0.0152]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.v_proj.bias',\n",
       "              tensor([ 0.0195, -0.0161, -0.1501,  ...,  0.0040,  0.0477,  0.0563],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.q_proj.weight',\n",
       "              tensor([[-0.0558,  0.0337, -0.0875,  ..., -0.0400,  0.1178,  0.0498],\n",
       "                      [ 0.0207,  0.0667, -0.0466,  ...,  0.1920, -0.2487, -0.1636],\n",
       "                      [ 0.0698,  0.2191,  0.0028,  ...,  0.0332,  0.1571,  0.0231],\n",
       "                      ...,\n",
       "                      [-0.0743, -0.1963, -0.0522,  ...,  0.0630,  0.0730,  0.0328],\n",
       "                      [ 0.1009, -0.1168,  0.0288,  ..., -0.0382, -0.0266, -0.0274],\n",
       "                      [ 0.0796,  0.0826, -0.1310,  ...,  0.1424,  0.0640,  0.1445]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.q_proj.bias',\n",
       "              tensor([ 0.0672, -0.7182, -0.0558,  ..., -0.6951,  0.5169,  0.5309],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.out_proj.weight',\n",
       "              tensor([[ 0.0101,  0.1172, -0.0335,  ...,  0.0620, -0.0085,  0.0248],\n",
       "                      [ 0.0222,  0.0756,  0.1649,  ..., -0.0371,  0.1334,  0.0759],\n",
       "                      [ 0.0152, -0.1801,  0.0089,  ..., -0.0149,  0.1186, -0.0684],\n",
       "                      ...,\n",
       "                      [ 0.0994, -0.0227,  0.0864,  ...,  0.1116,  0.1683, -0.1656],\n",
       "                      [-0.0199, -0.0892, -0.0848,  ...,  0.0195,  0.2305,  0.0570],\n",
       "                      [ 0.1050,  0.0217,  0.0435,  ...,  0.0686,  0.0873, -0.0467]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.attention.out_proj.bias',\n",
       "              tensor([ 0.0287,  0.0525,  0.0733,  ...,  0.1063, -0.0076, -0.0339],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.layer_norm.weight',\n",
       "              tensor([0.3756, 0.3086, 0.2707,  ..., 0.3296, 0.3296, 0.3051], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.layer_norm.bias',\n",
       "              tensor([ 0.0059,  0.0104,  0.0017,  ...,  0.0491,  0.0569, -0.0180],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.1648,  0.0364, -0.0407,  ..., -0.1119, -0.1316, -0.0264],\n",
       "                      [ 0.0917,  0.3265,  0.1172,  ...,  0.0815,  0.0088,  0.2860],\n",
       "                      [-0.0239, -0.0598, -0.0260,  ..., -0.0302,  0.0658, -0.0721],\n",
       "                      ...,\n",
       "                      [-0.0895, -0.0288,  0.3653,  ..., -0.0504,  0.0463,  0.0387],\n",
       "                      [-0.0530, -0.1519, -0.1514,  ...,  0.1729,  0.1084,  0.3932],\n",
       "                      [-0.0612, -0.0530,  0.0184,  ...,  0.0288,  0.1037, -0.0963]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0675, -0.0418, -0.0639,  ..., -0.0234, -0.0932,  0.0113],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.1077,  0.0129, -0.1340,  ..., -0.0267, -0.1587,  0.0408],\n",
       "                      [-0.1954, -0.1915,  0.0626,  ..., -0.0409,  0.0576,  0.0601],\n",
       "                      [ 0.0149,  0.0781, -0.1187,  ...,  0.1080, -0.1413,  0.0961],\n",
       "                      ...,\n",
       "                      [ 0.1826, -0.0743,  0.0600,  ..., -0.0554,  0.3244, -0.0312],\n",
       "                      [-0.2354, -0.0293,  0.1189,  ..., -0.0163,  0.0847, -0.0735],\n",
       "                      [ 0.0750,  0.0648,  0.0491,  ..., -0.0241,  0.4055,  0.0356]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0490, -0.0323,  0.0442,  ...,  0.0371, -0.0022,  0.0668],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.final_layer_norm.weight',\n",
       "              tensor([0.3789, 0.3830, 0.2144,  ..., 0.3885, 0.4106, 0.3572], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.13.final_layer_norm.bias',\n",
       "              tensor([ 0.0359,  0.0367, -0.0629,  ...,  0.0583,  0.1129, -0.0823],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.k_proj.weight',\n",
       "              tensor([[ 0.0489,  0.0300, -0.2336,  ..., -0.0546,  0.0252, -0.0960],\n",
       "                      [ 0.0674,  0.0662, -0.0252,  ...,  0.0236,  0.0362, -0.0783],\n",
       "                      [ 0.0110,  0.0975,  0.1105,  ...,  0.0404,  0.0780, -0.0863],\n",
       "                      ...,\n",
       "                      [ 0.0367,  0.1395,  0.0744,  ..., -0.1115,  0.0557, -0.0247],\n",
       "                      [ 0.0010,  0.0236, -0.0461,  ..., -0.0486,  0.0137,  0.1566],\n",
       "                      [ 0.1350,  0.0277, -0.1526,  ...,  0.0397, -0.0285, -0.0299]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.k_proj.bias',\n",
       "              tensor([-3.0083e-01, -4.5420e-03,  4.4906e-02,  ..., -2.2314e-03,\n",
       "                      -8.6945e-03,  6.3728e-05], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.v_proj.weight',\n",
       "              tensor([[ 0.1276,  0.0352,  0.0678,  ..., -0.0397, -0.0282, -0.1385],\n",
       "                      [ 0.1296,  0.0182,  0.0012,  ..., -0.0542,  0.0311,  0.1244],\n",
       "                      [ 0.0317,  0.0091,  0.0970,  ...,  0.0199,  0.1461,  0.1612],\n",
       "                      ...,\n",
       "                      [-0.0104,  0.0163,  0.0240,  ...,  0.1174,  0.0365, -0.0274],\n",
       "                      [-0.0471,  0.0726, -0.0049,  ...,  0.1776,  0.0819, -0.2591],\n",
       "                      [-0.0881,  0.0981,  0.1117,  ...,  0.0540, -0.0980, -0.1680]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.v_proj.bias',\n",
       "              tensor([-0.0164,  0.0444, -0.0170,  ...,  0.0098,  0.0574, -0.0160],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.q_proj.weight',\n",
       "              tensor([[-0.0253, -0.0038, -0.2530,  ..., -0.0436, -0.1365,  0.0687],\n",
       "                      [-0.0946, -0.0460,  0.1895,  ...,  0.0304,  0.0263, -0.1126],\n",
       "                      [ 0.0081, -0.0236,  0.2020,  ...,  0.0467,  0.0474,  0.0407],\n",
       "                      ...,\n",
       "                      [ 0.1081,  0.0931,  0.1567,  ...,  0.0560,  0.0790,  0.0454],\n",
       "                      [-0.0081, -0.0764, -0.0137,  ..., -0.0445, -0.0619,  0.0483],\n",
       "                      [ 0.1263, -0.0419,  0.0668,  ...,  0.0956, -0.0057,  0.0584]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.q_proj.bias',\n",
       "              tensor([-0.5843, -0.7076,  0.3278,  ...,  0.1128, -0.0919, -0.0351],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.out_proj.weight',\n",
       "              tensor([[-0.1125,  0.0633,  0.0482,  ...,  0.1527, -0.0789,  0.3513],\n",
       "                      [-0.0354,  0.1316,  0.0118,  ..., -0.0609, -0.1259, -0.0956],\n",
       "                      [ 0.0086, -0.1311,  0.0109,  ..., -0.0061, -0.1674,  0.0186],\n",
       "                      ...,\n",
       "                      [ 0.1416, -0.0042,  0.1811,  ..., -0.0814,  0.0021,  0.1912],\n",
       "                      [-0.0447, -0.0635,  0.1820,  ..., -0.1250, -0.0494, -0.0589],\n",
       "                      [ 0.0987, -0.1350,  0.0403,  ...,  0.0805,  0.2318,  0.1870]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.attention.out_proj.bias',\n",
       "              tensor([ 0.0538, -0.0189,  0.0787,  ...,  0.0645,  0.0221, -0.0340],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.layer_norm.weight',\n",
       "              tensor([0.3471, 0.3066, 0.2647,  ..., 0.3494, 0.3479, 0.3225], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.layer_norm.bias',\n",
       "              tensor([-0.0224,  0.0412, -0.0039,  ...,  0.0443,  0.0315, -0.0482],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.1580, -0.1075,  0.0377,  ..., -0.1843,  0.0243,  0.0516],\n",
       "                      [-0.0241,  0.0043,  0.1234,  ..., -0.0534, -0.0466,  0.1448],\n",
       "                      [-0.2111, -0.0298,  0.1805,  ..., -0.1951, -0.0424,  0.1339],\n",
       "                      ...,\n",
       "                      [ 0.0458,  0.0222, -0.0048,  ...,  0.0448,  0.0347,  0.0157],\n",
       "                      [-0.0238, -0.2202, -0.1674,  ..., -0.1122, -0.0837, -0.1027],\n",
       "                      [-0.1516, -0.0954,  0.2648,  ..., -0.0423, -0.0120, -0.0314]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0379, -0.0557, -0.0236,  ..., -0.0512, -0.0473, -0.0893],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.0043,  0.1816,  0.2071,  ...,  0.1409, -0.0280,  0.0410],\n",
       "                      [-0.0153,  0.0605, -0.0603,  ...,  0.0276,  0.0604,  0.0409],\n",
       "                      [-0.0581,  0.0410, -0.2404,  ...,  0.1619,  0.1746, -0.0676],\n",
       "                      ...,\n",
       "                      [ 0.0236,  0.0714,  0.2133,  ...,  0.0762, -0.0193,  0.0122],\n",
       "                      [-0.0761, -0.1022,  0.0006,  ...,  0.1054,  0.2510, -0.1348],\n",
       "                      [ 0.0492,  0.0716, -0.1004,  ..., -0.1809, -0.1219,  0.1805]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0603, -0.0314,  0.0760,  ..., -0.0016,  0.1003,  0.0001],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.final_layer_norm.weight',\n",
       "              tensor([0.3806, 0.3833, 0.2051,  ..., 0.3799, 0.3633, 0.3019], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.14.final_layer_norm.bias',\n",
       "              tensor([ 0.0107,  0.0512, -0.0714,  ...,  0.0267,  0.0416, -0.0976],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.k_proj.weight',\n",
       "              tensor([[ 0.2021, -0.0978,  0.1644,  ..., -0.0022, -0.1081, -0.1033],\n",
       "                      [ 0.0456, -0.0514, -0.1429,  ..., -0.0658,  0.0778,  0.2134],\n",
       "                      [ 0.1105,  0.0786,  0.0598,  ...,  0.1275,  0.0760,  0.1102],\n",
       "                      ...,\n",
       "                      [-0.0150,  0.0019,  0.0315,  ..., -0.0357, -0.1038, -0.0607],\n",
       "                      [ 0.0402, -0.1535,  0.2799,  ..., -0.0095, -0.1126,  0.1334],\n",
       "                      [ 0.0377, -0.1237,  0.1994,  ..., -0.1209, -0.0770, -0.0703]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.k_proj.bias',\n",
       "              tensor([ 0.0070,  0.0069,  0.0056,  ..., -0.0969,  0.0758, -0.0776],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.v_proj.weight',\n",
       "              tensor([[-0.0800,  0.0648, -0.0332,  ...,  0.0765, -0.1330,  0.0779],\n",
       "                      [-0.2581,  0.0412, -0.1032,  ...,  0.0733, -0.0710, -0.0610],\n",
       "                      [-0.0160, -0.0664, -0.0954,  ...,  0.0231, -0.1022,  0.0963],\n",
       "                      ...,\n",
       "                      [ 0.0340,  0.0957, -0.0479,  ..., -0.1028, -0.0519,  0.0619],\n",
       "                      [-0.1343, -0.0785, -0.0818,  ...,  0.0171,  0.0006, -0.0446],\n",
       "                      [-0.0724,  0.2005, -0.0443,  ...,  0.0373, -0.0144, -0.0811]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.v_proj.bias',\n",
       "              tensor([-0.0111, -0.0642,  0.0199,  ...,  0.0027,  0.0180,  0.0068],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.q_proj.weight',\n",
       "              tensor([[-0.0317,  0.1053, -0.0341,  ..., -0.0431, -0.1240, -0.2328],\n",
       "                      [ 0.0082, -0.0365, -0.0112,  ..., -0.2512, -0.0025,  0.1421],\n",
       "                      [-0.0918, -0.0159, -0.1501,  ...,  0.0299, -0.0814, -0.1814],\n",
       "                      ...,\n",
       "                      [ 0.0584,  0.0341, -0.0485,  ...,  0.0333, -0.0470, -0.0524],\n",
       "                      [-0.1561,  0.0981,  0.1480,  ...,  0.0174,  0.2819, -0.0214],\n",
       "                      [ 0.0821,  0.0954,  0.0471,  ...,  0.1833,  0.0313,  0.0086]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.q_proj.bias',\n",
       "              tensor([-0.1056,  0.0043, -0.1674,  ...,  0.4743, -0.0205,  0.1807],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.out_proj.weight',\n",
       "              tensor([[-0.0761, -0.0142, -0.2504,  ..., -0.0786,  0.1975,  0.1076],\n",
       "                      [-0.0423,  0.1770, -0.0712,  ...,  0.0985, -0.0288, -0.0202],\n",
       "                      [ 0.1659,  0.0153,  0.0550,  ..., -0.1382,  0.0474,  0.1914],\n",
       "                      ...,\n",
       "                      [-0.1323, -0.0607,  0.1023,  ..., -0.1227,  0.0167,  0.0746],\n",
       "                      [-0.0237,  0.0392, -0.1792,  ..., -0.1954,  0.0487, -0.1977],\n",
       "                      [-0.0788, -0.1033,  0.0986,  ..., -0.0691, -0.0026, -0.0343]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.attention.out_proj.bias',\n",
       "              tensor([ 0.0773, -0.1119,  0.1062,  ...,  0.0450,  0.0592,  0.0384],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.layer_norm.weight',\n",
       "              tensor([0.3658, 0.3248, 0.2968,  ..., 0.3908, 0.3718, 0.3310], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.layer_norm.bias',\n",
       "              tensor([-0.0282,  0.0675, -0.0107,  ...,  0.0502,  0.0002, -0.0645],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0417,  0.0563,  0.1798,  ..., -0.1063, -0.0006,  0.0007],\n",
       "                      [ 0.0808, -0.0693,  0.1664,  ..., -0.2582, -0.0604,  0.1496],\n",
       "                      [-0.0661,  0.1932, -0.1722,  ...,  0.0957, -0.1945, -0.0780],\n",
       "                      ...,\n",
       "                      [ 0.1233,  0.1545,  0.1558,  ...,  0.1087,  0.0393, -0.0286],\n",
       "                      [ 0.1408, -0.0419, -0.0343,  ..., -0.0151, -0.0504,  0.0041],\n",
       "                      [ 0.0785, -0.0913,  0.0853,  ...,  0.0312, -0.2333,  0.2410]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0731, -0.0703, -0.0965,  ..., -0.0666,  0.0474, -0.1022],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.1872,  0.2688,  0.1868,  ..., -0.1115, -0.1931,  0.1413],\n",
       "                      [-0.0489,  0.0123, -0.0579,  ..., -0.0243,  0.0816,  0.0053],\n",
       "                      [-0.1034,  0.0086,  0.1077,  ..., -0.0977, -0.0492,  0.0419],\n",
       "                      ...,\n",
       "                      [ 0.0358,  0.0780,  0.1982,  ..., -0.0491, -0.0651,  0.1779],\n",
       "                      [ 0.0507,  0.1738,  0.1429,  ..., -0.0100, -0.0855,  0.0807],\n",
       "                      [-0.1678, -0.0860,  0.1747,  ...,  0.1663,  0.0171, -0.1409]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0187,  0.0097, -0.0259,  ..., -0.0194,  0.0885,  0.0630],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.final_layer_norm.weight',\n",
       "              tensor([0.3746, 0.3649, 0.2053,  ..., 0.3884, 0.3583, 0.3489], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.15.final_layer_norm.bias',\n",
       "              tensor([ 0.0108,  0.0562, -0.0630,  ..., -0.0063,  0.0200, -0.0941],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.k_proj.weight',\n",
       "              tensor([[ 0.0545, -0.1920,  0.0979,  ..., -0.0018,  0.1087,  0.1643],\n",
       "                      [-0.0990, -0.0281, -0.0855,  ...,  0.0139,  0.0121,  0.1174],\n",
       "                      [-0.0146,  0.0338, -0.1663,  ..., -0.0384, -0.2601, -0.1100],\n",
       "                      ...,\n",
       "                      [-0.2124, -0.0432, -0.1185,  ...,  0.0402,  0.0040, -0.0095],\n",
       "                      [-0.0362,  0.0989,  0.0099,  ...,  0.0085, -0.0014, -0.0315],\n",
       "                      [-0.0273,  0.0141, -0.0354,  ...,  0.1243,  0.0987, -0.0004]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.k_proj.bias',\n",
       "              tensor([ 0.0063, -0.0004,  0.0014,  ...,  0.0071,  0.0041,  0.0184],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.v_proj.weight',\n",
       "              tensor([[ 0.1011, -0.1190,  0.1218,  ...,  0.0338,  0.0292, -0.1324],\n",
       "                      [ 0.2923,  0.0503, -0.1560,  ...,  0.0791,  0.0544,  0.1307],\n",
       "                      [-0.0266,  0.1281,  0.0497,  ..., -0.0017, -0.0706, -0.1179],\n",
       "                      ...,\n",
       "                      [-0.0588, -0.0261,  0.0959,  ...,  0.0082,  0.0971,  0.1154],\n",
       "                      [ 0.2264,  0.0280,  0.0457,  ...,  0.0012,  0.0698, -0.0384],\n",
       "                      [-0.2202,  0.0314,  0.1101,  ...,  0.0780, -0.2101, -0.1358]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.v_proj.bias',\n",
       "              tensor([-0.0292, -0.0092,  0.0224,  ...,  0.0257, -0.0023, -0.0385],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.q_proj.weight',\n",
       "              tensor([[ 0.0166, -0.0140, -0.0388,  ...,  0.1793, -0.2386, -0.0933],\n",
       "                      [ 0.0629,  0.1062,  0.1255,  ...,  0.1386, -0.0110,  0.0678],\n",
       "                      [-0.1330,  0.0668, -0.1329,  ...,  0.0671,  0.0088, -0.1215],\n",
       "                      ...,\n",
       "                      [-0.0586,  0.0492,  0.0814,  ...,  0.0211,  0.1274, -0.0084],\n",
       "                      [ 0.1304, -0.0137, -0.0224,  ...,  0.0739, -0.0118, -0.0992],\n",
       "                      [-0.1409, -0.1561, -0.1234,  ...,  0.0161, -0.0017, -0.0530]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.q_proj.bias',\n",
       "              tensor([-0.0537,  0.0771, -0.0552,  ...,  0.0539, -0.0005, -0.0133],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.out_proj.weight',\n",
       "              tensor([[-0.0922,  0.0249, -0.0700,  ..., -0.0077, -0.2125,  0.0304],\n",
       "                      [ 0.1155, -0.1201,  0.0089,  ...,  0.0348, -0.1551,  0.0913],\n",
       "                      [-0.0898,  0.1861, -0.0732,  ..., -0.0138, -0.1873, -0.1646],\n",
       "                      ...,\n",
       "                      [ 0.0929, -0.1080,  0.0115,  ...,  0.1817, -0.0639, -0.0160],\n",
       "                      [-0.1361, -0.0704, -0.1162,  ..., -0.0714,  0.1106,  0.2456],\n",
       "                      [ 0.1320, -0.2549,  0.0785,  ..., -0.0668, -0.2170,  0.1419]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.attention.out_proj.bias',\n",
       "              tensor([ 0.1028, -0.0558, -0.0480,  ...,  0.0322,  0.0809,  0.0275],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.layer_norm.weight',\n",
       "              tensor([0.3443, 0.3342, 0.2419,  ..., 0.3747, 0.3755, 0.2960], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.layer_norm.bias',\n",
       "              tensor([ 0.0220,  0.0678, -0.0018,  ...,  0.0445, -0.0056, -0.0714],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.1862,  0.0314,  0.0878,  ...,  0.0352,  0.0107, -0.1072],\n",
       "                      [-0.0209, -0.0791, -0.0724,  ...,  0.3985,  0.0744, -0.0365],\n",
       "                      [ 0.0470,  0.0867, -0.0135,  ..., -0.1285,  0.0773,  0.0111],\n",
       "                      ...,\n",
       "                      [ 0.0761, -0.0542, -0.0068,  ...,  0.0719, -0.1119, -0.0968],\n",
       "                      [ 0.1628,  0.1549,  0.1220,  ..., -0.0934, -0.0064,  0.2794],\n",
       "                      [-0.0398,  0.1671,  0.1209,  ...,  0.0266, -0.0272, -0.0056]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.feed_forward.intermediate_dense.bias',\n",
       "              tensor([ 0.0021, -0.0671, -0.0775,  ..., -0.0762, -0.0816, -0.0992],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.1208, -0.0831, -0.0581,  ...,  0.0056,  0.0385,  0.0428],\n",
       "                      [ 0.0567, -0.0383, -0.0945,  ..., -0.0468, -0.1010, -0.2069],\n",
       "                      [-0.0771, -0.1888, -0.0014,  ...,  0.1347, -0.1538,  0.0447],\n",
       "                      ...,\n",
       "                      [ 0.0266, -0.0317,  0.0925,  ..., -0.0680,  0.0362,  0.0252],\n",
       "                      [-0.0704, -0.0875, -0.0996,  ..., -0.0990,  0.1372, -0.0969],\n",
       "                      [ 0.0931,  0.0571, -0.0368,  ..., -0.1652,  0.0245,  0.0759]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.0826, -0.0566, -0.0388,  ..., -0.0315, -0.0805,  0.0437],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.final_layer_norm.weight',\n",
       "              tensor([0.3737, 0.3997, 0.2112,  ..., 0.4092, 0.4166, 0.3489], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.16.final_layer_norm.bias',\n",
       "              tensor([ 0.0702,  0.0526, -0.0494,  ..., -0.0457,  0.0262, -0.1299],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.k_proj.weight',\n",
       "              tensor([[ 0.1076,  0.1342,  0.0768,  ...,  0.0371, -0.1249, -0.2511],\n",
       "                      [ 0.1861, -0.0736, -0.0813,  ..., -0.0053, -0.1522, -0.1619],\n",
       "                      [ 0.1395, -0.1380,  0.0870,  ..., -0.2052, -0.0887,  0.0110],\n",
       "                      ...,\n",
       "                      [ 0.0737,  0.0858, -0.0073,  ..., -0.0492,  0.0040,  0.0081],\n",
       "                      [-0.1151, -0.0157, -0.0993,  ..., -0.0141,  0.0445,  0.0378],\n",
       "                      [ 0.0492, -0.0405,  0.0690,  ..., -0.1008,  0.0392,  0.0623]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.k_proj.bias',\n",
       "              tensor([-0.0012, -0.0046, -0.0020,  ..., -0.1706, -0.6543, -0.7660],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.v_proj.weight',\n",
       "              tensor([[ 0.0155,  0.1187,  0.0468,  ..., -0.0238,  0.1431, -0.0429],\n",
       "                      [-0.0331,  0.0319, -0.2060,  ..., -0.1919, -0.0316,  0.0623],\n",
       "                      [ 0.0098,  0.0367,  0.1386,  ...,  0.0280,  0.1433, -0.0628],\n",
       "                      ...,\n",
       "                      [-0.0509, -0.0782,  0.0887,  ..., -0.1615, -0.0409, -0.0355],\n",
       "                      [ 0.0682, -0.2225,  0.0909,  ..., -0.1260, -0.0289, -0.2085],\n",
       "                      [ 0.0056,  0.0139, -0.1804,  ...,  0.0821, -0.0202, -0.0693]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.v_proj.bias',\n",
       "              tensor([-0.0142,  0.0146,  0.0485,  ...,  0.0306, -0.0462, -0.0365],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.q_proj.weight',\n",
       "              tensor([[ 0.0357, -0.0572,  0.0493,  ...,  0.1321,  0.0391, -0.0134],\n",
       "                      [ 0.0615,  0.0619, -0.1594,  ...,  0.0142, -0.0461, -0.0309],\n",
       "                      [ 0.0953, -0.0650,  0.0145,  ..., -0.1899, -0.1577, -0.0223],\n",
       "                      ...,\n",
       "                      [-0.0145,  0.0338, -0.0434,  ..., -0.1194,  0.0369,  0.0979],\n",
       "                      [ 0.0164, -0.0963, -0.0193,  ...,  0.0192,  0.0316, -0.1705],\n",
       "                      [-0.1434,  0.0824,  0.0026,  ..., -0.1013,  0.0611,  0.0069]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.q_proj.bias',\n",
       "              tensor([-0.2409,  0.1287,  0.0375,  ..., -0.0252,  0.0258, -0.2473],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.out_proj.weight',\n",
       "              tensor([[-0.0585, -0.0716, -0.0531,  ...,  0.0978,  0.1611, -0.0495],\n",
       "                      [-0.0243,  0.0193,  0.0181,  ...,  0.3076,  0.3297,  0.0128],\n",
       "                      [-0.0646,  0.0289, -0.1129,  ...,  0.1830,  0.1540,  0.0634],\n",
       "                      ...,\n",
       "                      [-0.0523,  0.0028,  0.0737,  ..., -0.0301, -0.0349,  0.1182],\n",
       "                      [-0.0021,  0.0936, -0.1149,  ...,  0.0838,  0.0512,  0.0677],\n",
       "                      [ 0.1588,  0.1085, -0.1998,  ..., -0.1593, -0.1539,  0.1175]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.attention.out_proj.bias',\n",
       "              tensor([ 0.1518, -0.0452,  0.0382,  ..., -0.0626, -0.0847,  0.0459],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.layer_norm.weight',\n",
       "              tensor([0.3633, 0.3741, 0.2713,  ..., 0.4048, 0.3760, 0.3222], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.layer_norm.bias',\n",
       "              tensor([ 0.0201,  0.0333,  0.0262,  ...,  0.0521, -0.0080, -0.0150],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0786, -0.1382,  0.0582,  ...,  0.1009, -0.0972,  0.0725],\n",
       "                      [-0.1765, -0.0673, -0.1118,  ...,  0.0577, -0.1593, -0.0696],\n",
       "                      [ 0.1191, -0.0437,  0.0113,  ...,  0.1584, -0.0886, -0.1759],\n",
       "                      ...,\n",
       "                      [ 0.1757, -0.0688,  0.0767,  ...,  0.0131,  0.0977,  0.1099],\n",
       "                      [ 0.0931,  0.0120, -0.0450,  ..., -0.0400, -0.1663,  0.0627],\n",
       "                      [ 0.0151, -0.0504, -0.0957,  ...,  0.0480,  0.0701, -0.0322]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0400, -0.0382, -0.0879,  ..., -0.0350, -0.0637, -0.0650],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0268,  0.1538,  0.1955,  ..., -0.0467,  0.0321, -0.0616],\n",
       "                      [ 0.0534, -0.0417, -0.1888,  ..., -0.0050,  0.3639, -0.2539],\n",
       "                      [-0.1684,  0.1365,  0.0189,  ..., -0.0929, -0.0632, -0.0441],\n",
       "                      ...,\n",
       "                      [-0.0990,  0.1455, -0.2737,  ..., -0.1164,  0.1629, -0.0979],\n",
       "                      [ 0.0873, -0.0710, -0.0169,  ..., -0.1288,  0.0859,  0.0906],\n",
       "                      [-0.1802,  0.1278, -0.1705,  ..., -0.0890,  0.0783, -0.0041]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0015, -0.0703, -0.0508,  ..., -0.0459, -0.0564,  0.0790],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.final_layer_norm.weight',\n",
       "              tensor([0.3683, 0.4392, 0.2470,  ..., 0.4127, 0.4023, 0.3429], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.17.final_layer_norm.bias',\n",
       "              tensor([ 0.0750,  0.0447, -0.0232,  ..., -0.0015,  0.0229, -0.0579],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.k_proj.weight',\n",
       "              tensor([[-0.1540, -0.1174,  0.1343,  ..., -0.0039,  0.0329, -0.1489],\n",
       "                      [-0.0941, -0.0381, -0.0052,  ..., -0.0464, -0.0128, -0.0188],\n",
       "                      [ 0.0002, -0.1749, -0.0276,  ...,  0.0311,  0.0277,  0.1083],\n",
       "                      ...,\n",
       "                      [ 0.0752,  0.0607,  0.0031,  ..., -0.0714,  0.1204,  0.1087],\n",
       "                      [-0.0335,  0.0415,  0.0102,  ...,  0.1594, -0.1637,  0.0738],\n",
       "                      [-0.1310,  0.0435, -0.0323,  ...,  0.0150,  0.0437, -0.0923]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.k_proj.bias',\n",
       "              tensor([ 0.2909,  0.3319,  0.0302,  ..., -0.0020, -0.0009,  0.0016],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.v_proj.weight',\n",
       "              tensor([[-0.0404,  0.1025,  0.0029,  ..., -0.0699,  0.0353,  0.0350],\n",
       "                      [ 0.0688, -0.0575,  0.0537,  ...,  0.0721, -0.0396,  0.0193],\n",
       "                      [-0.0313,  0.0498, -0.1289,  ..., -0.0626,  0.0123,  0.0074],\n",
       "                      ...,\n",
       "                      [ 0.0469, -0.3114,  0.0255,  ...,  0.1326, -0.1383,  0.0503],\n",
       "                      [ 0.0218, -0.1914,  0.1941,  ...,  0.1396,  0.0185,  0.0173],\n",
       "                      [ 0.0016,  0.0691,  0.0987,  ..., -0.0081,  0.1975, -0.0468]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.v_proj.bias',\n",
       "              tensor([ 0.0568,  0.0095, -0.0488,  ..., -0.0596, -0.0109, -0.0077],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.q_proj.weight',\n",
       "              tensor([[-0.0213,  0.0155, -0.0183,  ...,  0.2003,  0.0177,  0.0635],\n",
       "                      [ 0.0114,  0.0427, -0.0262,  ...,  0.0341, -0.0421, -0.0696],\n",
       "                      [-0.0152,  0.1001,  0.0126,  ..., -0.0166,  0.0140,  0.1934],\n",
       "                      ...,\n",
       "                      [ 0.0222, -0.0942,  0.0105,  ...,  0.0051, -0.1287, -0.0195],\n",
       "                      [-0.0205, -0.0609, -0.0100,  ..., -0.1831, -0.0723, -0.0737],\n",
       "                      [ 0.0446, -0.0507, -0.0672,  ..., -0.0096, -0.0249, -0.1346]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.q_proj.bias',\n",
       "              tensor([ 0.1608, -0.1227,  0.0178,  ...,  0.0809, -0.1209, -0.1413],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.out_proj.weight',\n",
       "              tensor([[ 0.0555, -0.0358, -0.0166,  ..., -0.1419, -0.1916,  0.0567],\n",
       "                      [-0.1079,  0.0344,  0.0581,  ...,  0.2320,  0.0918, -0.1873],\n",
       "                      [ 0.0066, -0.0444, -0.0157,  ...,  0.2448, -0.0874,  0.0205],\n",
       "                      ...,\n",
       "                      [ 0.0043, -0.0775,  0.0124,  ..., -0.1164, -0.0092, -0.1060],\n",
       "                      [-0.0735,  0.0452,  0.0115,  ...,  0.1575,  0.0270,  0.0061],\n",
       "                      [-0.0564, -0.0297,  0.0003,  ...,  0.0141, -0.1077, -0.0740]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.attention.out_proj.bias',\n",
       "              tensor([ 0.0271, -0.0456, -0.1059,  ...,  0.0205, -0.0213,  0.0276],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.layer_norm.weight',\n",
       "              tensor([0.3695, 0.4123, 0.3023,  ..., 0.3887, 0.3833, 0.3259], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.layer_norm.bias',\n",
       "              tensor([ 0.0483,  0.0244,  0.0997,  ...,  0.0322, -0.0054,  0.0026],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0788, -0.1028, -0.1396,  ...,  0.0810,  0.0469, -0.0741],\n",
       "                      [ 0.1180, -0.1036,  0.1180,  ...,  0.0729,  0.0094, -0.2178],\n",
       "                      [-0.1385, -0.2231,  0.1177,  ..., -0.0954, -0.2250, -0.0403],\n",
       "                      ...,\n",
       "                      [-0.0145,  0.0858, -0.0154,  ..., -0.1408, -0.0110,  0.0704],\n",
       "                      [ 0.0087,  0.0252, -0.0067,  ..., -0.0548, -0.1027,  0.0039],\n",
       "                      [ 0.0141,  0.0254,  0.0670,  ...,  0.1104, -0.0443, -0.1028]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0635, -0.0611, -0.1338,  ..., -0.0689, -0.1583, -0.1101],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0053, -0.0308,  0.3007,  ...,  0.1641,  0.1490,  0.0861],\n",
       "                      [ 0.1292,  0.0366,  0.1455,  ...,  0.0877,  0.1655,  0.1119],\n",
       "                      [ 0.1672,  0.1206, -0.0600,  ..., -0.1510,  0.0734, -0.1202],\n",
       "                      ...,\n",
       "                      [-0.0499, -0.0309,  0.1202,  ...,  0.0592,  0.3024,  0.3141],\n",
       "                      [-0.0394, -0.0289,  0.0323,  ...,  0.0127, -0.1440, -0.0419],\n",
       "                      [ 0.2038, -0.0142, -0.2084,  ..., -0.0938, -0.0347,  0.2995]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.1394,  0.0015, -0.1377,  ..., -0.0658, -0.0571,  0.0767],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.final_layer_norm.weight',\n",
       "              tensor([0.3831, 0.4842, 0.2683,  ..., 0.4466, 0.4328, 0.3232], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.18.final_layer_norm.bias',\n",
       "              tensor([ 0.0673,  0.0448, -0.0101,  ..., -0.0204,  0.0126, -0.0487],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.k_proj.weight',\n",
       "              tensor([[ 0.1632, -0.0303, -0.1709,  ...,  0.1136,  0.1843,  0.2995],\n",
       "                      [ 0.0323,  0.0210, -0.0073,  ...,  0.0109,  0.1264,  0.0240],\n",
       "                      [ 0.0552,  0.0424, -0.0543,  ..., -0.0022,  0.0936, -0.0194],\n",
       "                      ...,\n",
       "                      [-0.0058,  0.0528, -0.0882,  ..., -0.1309, -0.1202,  0.1279],\n",
       "                      [ 0.1455, -0.0667,  0.1197,  ...,  0.0670,  0.0574, -0.0471],\n",
       "                      [-0.0170,  0.0810, -0.0675,  ..., -0.1098, -0.0244,  0.1325]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.k_proj.bias',\n",
       "              tensor([-0.0226, -0.0024, -0.0645,  ...,  1.3797, -0.4127,  0.1621],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.v_proj.weight',\n",
       "              tensor([[ 0.1256, -0.0025,  0.1818,  ..., -0.1280, -0.1408, -0.2042],\n",
       "                      [ 0.1230,  0.0752, -0.0220,  ..., -0.0096, -0.0283, -0.1883],\n",
       "                      [ 0.0496,  0.0561, -0.0893,  ..., -0.1804,  0.0713,  0.0420],\n",
       "                      ...,\n",
       "                      [ 0.0260, -0.0620,  0.1639,  ..., -0.0432, -0.0887,  0.0252],\n",
       "                      [ 0.1109, -0.0983,  0.0038,  ..., -0.0057, -0.0722, -0.1502],\n",
       "                      [-0.1341, -0.1385, -0.1826,  ...,  0.0942,  0.0664, -0.0999]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.v_proj.bias',\n",
       "              tensor([ 0.0149, -0.0080, -0.0205,  ..., -0.0352,  0.0169, -0.0534],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.q_proj.weight',\n",
       "              tensor([[ 0.0638,  0.0058, -0.2079,  ...,  0.1056,  0.1965, -0.0093],\n",
       "                      [ 0.0268, -0.0422,  0.0553,  ..., -0.1071,  0.0409, -0.0776],\n",
       "                      [ 0.0517,  0.1049, -0.0482,  ...,  0.0623,  0.0759, -0.0391],\n",
       "                      ...,\n",
       "                      [-0.0131,  0.1221, -0.0579,  ...,  0.0296, -0.0481,  0.2203],\n",
       "                      [ 0.1173, -0.0987,  0.0329,  ...,  0.0569,  0.0813, -0.0129],\n",
       "                      [-0.0121,  0.0757, -0.0546,  ..., -0.1377, -0.0346,  0.1308]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.q_proj.bias',\n",
       "              tensor([-0.1789, -0.1052, -0.2167,  ..., -0.5953,  0.0313, -0.2482],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.out_proj.weight',\n",
       "              tensor([[-0.0694,  0.0535, -0.2220,  ...,  0.2391, -0.0328, -0.1196],\n",
       "                      [ 0.0363,  0.0235,  0.0121,  ...,  0.0213,  0.0723,  0.0492],\n",
       "                      [ 0.1316, -0.0272, -0.0765,  ..., -0.1202, -0.0896, -0.0425],\n",
       "                      ...,\n",
       "                      [ 0.0984,  0.0982,  0.1632,  ..., -0.0212,  0.1368, -0.1409],\n",
       "                      [-0.0667,  0.3282, -0.2363,  ...,  0.0344, -0.0893, -0.1717],\n",
       "                      [-0.1395,  0.0554, -0.1001,  ..., -0.0476, -0.1756,  0.0550]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.attention.out_proj.bias',\n",
       "              tensor([ 0.3049, -0.0391, -0.1847,  ..., -0.0571, -0.0716,  0.1806],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.layer_norm.weight',\n",
       "              tensor([0.4015, 0.5021, 0.3129,  ..., 0.4454, 0.4411, 0.3674], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.layer_norm.bias',\n",
       "              tensor([ 0.0017,  0.0458,  0.1196,  ...,  0.0537, -0.0066, -0.0408],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.1525, -0.0579, -0.0666,  ..., -0.0519,  0.0487, -0.0615],\n",
       "                      [-0.0399, -0.0895,  0.1175,  ...,  0.0199,  0.1337,  0.0443],\n",
       "                      [-0.1105,  0.2133, -0.0248,  ...,  0.0029,  0.0263, -0.0475],\n",
       "                      ...,\n",
       "                      [ 0.1043, -0.0443, -0.0340,  ..., -0.0272, -0.0027, -0.1472],\n",
       "                      [-0.1328,  0.0428, -0.0986,  ...,  0.0076, -0.0578, -0.0931],\n",
       "                      [-0.0604, -0.1008,  0.1155,  ..., -0.0277,  0.1889,  0.1841]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.feed_forward.intermediate_dense.bias',\n",
       "              tensor([ 0.0652, -0.0864, -0.0886,  ..., -0.0615, -0.1130, -0.1379],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0867,  0.2158, -0.0397,  ...,  0.0840,  0.0322,  0.3091],\n",
       "                      [ 0.0858,  0.0424,  0.0060,  ..., -0.1160,  0.0366,  0.2220],\n",
       "                      [ 0.0514, -0.0884,  0.1088,  ..., -0.0786, -0.0226, -0.2249],\n",
       "                      ...,\n",
       "                      [ 0.1225,  0.1346,  0.0039,  ..., -0.0713, -0.0691,  0.0561],\n",
       "                      [-0.0731,  0.0947, -0.0197,  ...,  0.0024, -0.0292,  0.0125],\n",
       "                      [ 0.0881,  0.1398, -0.1274,  ..., -0.0320,  0.1860,  0.0881]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.feed_forward.output_dense.bias',\n",
       "              tensor([ 0.2011,  0.2456,  0.1931,  ...,  0.1443, -0.1456,  0.0726],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.final_layer_norm.weight',\n",
       "              tensor([0.4718, 0.5497, 0.2854,  ..., 0.4900, 0.4715, 0.3969], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.19.final_layer_norm.bias',\n",
       "              tensor([ 0.0488,  0.1131,  0.0111,  ..., -0.0402,  0.0399, -0.1120],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.k_proj.weight',\n",
       "              tensor([[-0.0128,  0.0135,  0.0614,  ...,  0.0215,  0.0955,  0.0332],\n",
       "                      [ 0.0005, -0.0888,  0.1448,  ..., -0.2152, -0.1546, -0.0590],\n",
       "                      [ 0.0018, -0.0403, -0.0757,  ...,  0.0040,  0.0335,  0.0133],\n",
       "                      ...,\n",
       "                      [ 0.0287,  0.0639, -0.0342,  ..., -0.0134,  0.0566,  0.0482],\n",
       "                      [ 0.0101, -0.0369, -0.0003,  ...,  0.0776, -0.0370, -0.0626],\n",
       "                      [-0.0693, -0.0300, -0.1320,  ...,  0.0333,  0.0142, -0.0510]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.k_proj.bias',\n",
       "              tensor([ 0.4507,  0.1327, -0.8731,  ..., -1.2928, -0.3476, -1.3614],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.v_proj.weight',\n",
       "              tensor([[ 0.0625,  0.0357,  0.0017,  ..., -0.0989, -0.0357,  0.0389],\n",
       "                      [ 0.0881,  0.0749,  0.0329,  ...,  0.0009, -0.0128,  0.0278],\n",
       "                      [ 0.1291, -0.0643,  0.1129,  ..., -0.0015,  0.1083, -0.0021],\n",
       "                      ...,\n",
       "                      [-0.0253,  0.0800, -0.0418,  ...,  0.0339,  0.0005, -0.0495],\n",
       "                      [-0.0165, -0.0028,  0.1214,  ...,  0.0104,  0.0251, -0.0562],\n",
       "                      [ 0.0409, -0.0287, -0.0259,  ...,  0.0045,  0.0375, -0.1508]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.v_proj.bias',\n",
       "              tensor([-0.0093,  0.0109, -0.0134,  ...,  0.0179, -0.0382, -0.0067],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.q_proj.weight',\n",
       "              tensor([[-0.1469, -0.0512,  0.0086,  ...,  0.0035,  0.0421,  0.1215],\n",
       "                      [-0.1041, -0.0274,  0.1427,  ..., -0.2463, -0.0443, -0.0924],\n",
       "                      [ 0.0851, -0.0654, -0.0552,  ..., -0.1604, -0.0973, -0.0140],\n",
       "                      ...,\n",
       "                      [ 0.0551,  0.1927, -0.1254,  ..., -0.2841,  0.2789, -0.0968],\n",
       "                      [-0.0713,  0.0451, -0.1013,  ..., -0.1659,  0.0734, -0.0701],\n",
       "                      [-0.0874,  0.0310,  0.0728,  ..., -0.2041,  0.3377,  0.0345]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.q_proj.bias',\n",
       "              tensor([-0.0632,  0.0670, -0.0112,  ...,  0.1119,  0.0746,  0.0255],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.out_proj.weight',\n",
       "              tensor([[-0.0442,  0.0314, -0.1456,  ..., -0.0083,  0.0675,  0.0405],\n",
       "                      [-0.0762,  0.0453,  0.0542,  ...,  0.0324,  0.0861,  0.0463],\n",
       "                      [-0.0571,  0.0477,  0.0217,  ..., -0.0032, -0.0305,  0.0597],\n",
       "                      ...,\n",
       "                      [ 0.0411,  0.1100, -0.0931,  ..., -0.0004,  0.0015,  0.0252],\n",
       "                      [-0.0022, -0.0299, -0.1096,  ..., -0.0146,  0.0158, -0.0222],\n",
       "                      [ 0.0538,  0.0545,  0.0077,  ...,  0.0299,  0.0297,  0.0500]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.attention.out_proj.bias',\n",
       "              tensor([ 0.2923,  0.2769,  0.1213,  ...,  0.1550, -0.2149,  0.0833],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.layer_norm.weight',\n",
       "              tensor([0.5258, 0.7791, 0.3637,  ..., 0.6947, 0.8655, 0.4832], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.layer_norm.bias',\n",
       "              tensor([-0.0237,  0.0818,  0.0052,  ..., -0.0721,  0.2621, -0.0867],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.1120, -0.0289, -0.1257,  ..., -0.0686,  0.0284,  0.0550],\n",
       "                      [ 0.0258,  0.0812, -0.0851,  ...,  0.0197, -0.1400, -0.0398],\n",
       "                      [-0.1123,  0.0326,  0.0505,  ...,  0.0043, -0.0018, -0.1162],\n",
       "                      ...,\n",
       "                      [-0.0868, -0.1811, -0.0863,  ...,  0.0409,  0.0188,  0.0559],\n",
       "                      [-0.0221, -0.1370,  0.0319,  ...,  0.0579, -0.0307,  0.0804],\n",
       "                      [-0.0114, -0.0055, -0.0511,  ...,  0.0004, -0.0150,  0.0144]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.1278, -0.0059, -0.0922,  ..., -0.1078, -0.0594, -0.0216],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.2130,  0.0711, -0.0630,  ..., -0.0592,  0.0380,  0.0334],\n",
       "                      [ 0.2598,  0.0398,  0.0911,  ...,  0.0762,  0.1360,  0.0449],\n",
       "                      [-0.1939,  0.0568,  0.0514,  ...,  0.0767,  0.1712,  0.1512],\n",
       "                      ...,\n",
       "                      [-0.0766, -0.1151,  0.1393,  ..., -0.0847, -0.0968,  0.1044],\n",
       "                      [ 0.1237,  0.0575, -0.0105,  ...,  0.2113,  0.0542,  0.0895],\n",
       "                      [ 0.0684,  0.0165, -0.0010,  ...,  0.0211,  0.0492, -0.0778]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0791, -0.1221,  0.1059,  ...,  0.0579, -0.0319, -0.0011],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.final_layer_norm.weight',\n",
       "              tensor([0.5318, 0.6338, 0.3051,  ..., 0.6336, 0.5775, 0.4297], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.20.final_layer_norm.bias',\n",
       "              tensor([-0.0467, -0.0642,  0.0793,  ..., -0.0220, -0.0115, -0.1242],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.k_proj.weight',\n",
       "              tensor([[-0.0111,  0.0012, -0.0025,  ..., -0.0178, -0.0045, -0.0055],\n",
       "                      [-0.0232, -0.0453, -0.0046,  ...,  0.0118,  0.0042,  0.0398],\n",
       "                      [ 0.0105,  0.0022, -0.0071,  ...,  0.0008,  0.0113,  0.0098],\n",
       "                      ...,\n",
       "                      [-0.0192,  0.0091, -0.0061,  ..., -0.0380,  0.0305, -0.0135],\n",
       "                      [-0.0023,  0.0211, -0.0190,  ..., -0.0183,  0.0523, -0.0082],\n",
       "                      [-0.0122, -0.0101,  0.0015,  ...,  0.0194, -0.0499,  0.0388]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.k_proj.bias',\n",
       "              tensor([-0.0477, -0.1356,  0.1586,  ...,  0.0230,  0.1987, -0.2090],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.v_proj.weight',\n",
       "              tensor([[-0.0271,  0.0331,  0.0166,  ...,  0.0386,  0.0047, -0.0347],\n",
       "                      [ 0.0402, -0.0194,  0.0217,  ..., -0.0200, -0.0674,  0.0033],\n",
       "                      [ 0.0162, -0.0293,  0.0207,  ...,  0.0101,  0.0178,  0.0135],\n",
       "                      ...,\n",
       "                      [-0.0494,  0.0323,  0.0262,  ..., -0.0033, -0.0022, -0.0194],\n",
       "                      [-0.0111,  0.0195, -0.0346,  ...,  0.0224, -0.0156, -0.0482],\n",
       "                      [-0.0151, -0.1106, -0.0013,  ..., -0.0242,  0.0790, -0.0195]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.v_proj.bias',\n",
       "              tensor([ 0.0002,  0.0017,  0.0014,  ...,  0.0119,  0.0102, -0.0080],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.q_proj.weight',\n",
       "              tensor([[ 0.0165,  0.0165,  0.0086,  ..., -0.0092,  0.0154, -0.0209],\n",
       "                      [ 0.0150, -0.0064,  0.0145,  ...,  0.0209,  0.0316, -0.0054],\n",
       "                      [-0.0351, -0.0034, -0.0020,  ..., -0.0099, -0.0381, -0.0178],\n",
       "                      ...,\n",
       "                      [ 0.0267, -0.0204, -0.0158,  ..., -0.0141, -0.0080, -0.0054],\n",
       "                      [ 0.0034, -0.0298, -0.0066,  ...,  0.0041,  0.0121, -0.0147],\n",
       "                      [ 0.0094,  0.0046,  0.0492,  ...,  0.0006, -0.0078, -0.0178]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.q_proj.bias',\n",
       "              tensor([ 0.0118,  0.0457, -0.0114,  ..., -0.0131, -0.1020,  0.1034],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.out_proj.weight',\n",
       "              tensor([[-0.0253,  0.0261, -0.0082,  ...,  0.0197,  0.0299,  0.0236],\n",
       "                      [-0.0063, -0.0069, -0.0032,  ..., -0.0351,  0.0157,  0.0844],\n",
       "                      [ 0.0031,  0.0114, -0.0161,  ..., -0.0327, -0.0011,  0.0104],\n",
       "                      ...,\n",
       "                      [-0.0219,  0.0104,  0.0203,  ..., -0.0075,  0.0144,  0.0343],\n",
       "                      [-0.0228,  0.0501, -0.0199,  ..., -0.0073, -0.0545, -0.0216],\n",
       "                      [-0.0182, -0.0124, -0.0393,  ..., -0.0599,  0.0326,  0.0592]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.attention.out_proj.bias',\n",
       "              tensor([-0.0725, -0.1236,  0.1135,  ...,  0.0426, -0.0410,  0.0452],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.layer_norm.weight',\n",
       "              tensor([0.5474, 0.4809, 0.3062,  ..., 0.5259, 0.4282, 0.3175], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.layer_norm.bias',\n",
       "              tensor([ 0.0177,  0.0264, -0.0858,  ...,  0.0271,  0.1316, -0.1326],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.0482, -0.0467, -0.0611,  ..., -0.0329,  0.1178, -0.0982],\n",
       "                      [-0.0293,  0.0719,  0.1294,  ..., -0.1485, -0.0751,  0.1878],\n",
       "                      [ 0.0974,  0.0013, -0.1530,  ...,  0.0308, -0.1082,  0.0662],\n",
       "                      ...,\n",
       "                      [ 0.0173,  0.0722,  0.1093,  ...,  0.0139,  0.1591, -0.0218],\n",
       "                      [ 0.0430, -0.1683,  0.0003,  ..., -0.0541, -0.0379,  0.0068],\n",
       "                      [-0.1390, -0.0316, -0.0273,  ..., -0.0127, -0.0364, -0.0962]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0507, -0.0823, -0.0273,  ..., -0.0598, -0.0096, -0.0730],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0424, -0.0770, -0.2490,  ...,  0.0291, -0.0324,  0.0838],\n",
       "                      [ 0.0690,  0.0602,  0.0212,  ..., -0.1415,  0.0341,  0.1708],\n",
       "                      [ 0.0265, -0.1700, -0.0077,  ...,  0.0305,  0.0366,  0.1117],\n",
       "                      ...,\n",
       "                      [-0.0264, -0.1838, -0.1555,  ...,  0.0360, -0.1153, -0.1434],\n",
       "                      [ 0.0100,  0.1407,  0.0700,  ..., -0.0363, -0.0204,  0.1020],\n",
       "                      [ 0.0044, -0.1254, -0.0620,  ..., -0.1065, -0.0874,  0.0763]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.feed_forward.output_dense.bias',\n",
       "              tensor([-0.1740, -0.2430,  0.0341,  ...,  0.0773,  0.0547,  0.0899],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.final_layer_norm.weight',\n",
       "              tensor([0.7266, 0.9125, 0.4378,  ..., 0.9847, 0.8143, 0.6403], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.21.final_layer_norm.bias',\n",
       "              tensor([-0.0007,  0.1183,  0.0965,  ...,  0.0668, -0.0245, -0.1312],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.k_proj.weight',\n",
       "              tensor([[ 0.0136, -0.1979,  0.0076,  ...,  0.0523, -0.0271, -0.0586],\n",
       "                      [-0.0148, -0.0050,  0.0152,  ..., -0.0102, -0.0249, -0.0349],\n",
       "                      [ 0.0933,  0.0553, -0.0533,  ..., -0.0500, -0.0048,  0.0237],\n",
       "                      ...,\n",
       "                      [ 0.0300,  0.0982,  0.0898,  ..., -0.0147,  0.0845, -0.0379],\n",
       "                      [ 0.0041, -0.1799, -0.0744,  ..., -0.0465,  0.0732, -0.0172],\n",
       "                      [ 0.0306, -0.0679, -0.0364,  ...,  0.0230, -0.1173, -0.0272]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.k_proj.bias',\n",
       "              tensor([-0.0829,  0.1775,  0.1671,  ...,  0.1033, -0.0278,  0.0272],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.v_proj.weight',\n",
       "              tensor([[ 0.0309,  0.1252, -0.0229,  ...,  0.1456, -0.0929,  0.0145],\n",
       "                      [-0.0469,  0.0856, -0.0377,  ..., -0.2061, -0.0818,  0.0950],\n",
       "                      [ 0.0046, -0.0014, -0.0861,  ..., -0.1492, -0.0116, -0.0204],\n",
       "                      ...,\n",
       "                      [-0.0266, -0.1409,  0.0039,  ..., -0.1079,  0.0366,  0.0685],\n",
       "                      [-0.0263,  0.1519, -0.0079,  ..., -0.0105,  0.0455, -0.0144],\n",
       "                      [ 0.0060, -0.1036, -0.1145,  ..., -0.0252, -0.1063,  0.0658]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.v_proj.bias',\n",
       "              tensor([ 0.0844, -0.0475, -0.0558,  ..., -0.0727,  0.0581, -0.0273],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.q_proj.weight',\n",
       "              tensor([[ 3.1087e-02,  1.3779e-05, -1.1939e-01,  ..., -1.6417e-02,\n",
       "                       -4.3188e-03, -1.9199e-02],\n",
       "                      [ 6.3148e-02,  6.3650e-02, -7.4141e-02,  ...,  9.1362e-02,\n",
       "                       -5.6212e-02, -3.7817e-02],\n",
       "                      [ 5.1161e-02,  7.5411e-02,  1.0853e-01,  ..., -6.3126e-02,\n",
       "                        1.1904e-02, -5.7356e-03],\n",
       "                      ...,\n",
       "                      [ 5.6145e-02,  5.0112e-03,  7.2984e-02,  ..., -7.2843e-02,\n",
       "                        2.3563e-03, -1.0487e-01],\n",
       "                      [-4.0226e-03, -8.8760e-02,  4.0457e-03,  ...,  7.2197e-02,\n",
       "                       -2.7172e-02,  1.5163e-02],\n",
       "                      [-9.5654e-02, -9.7420e-04,  1.1048e-02,  ...,  1.7026e-02,\n",
       "                        7.4628e-02,  3.7256e-02]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.q_proj.bias',\n",
       "              tensor([-0.0469, -0.0253,  0.1985,  ..., -0.0682, -0.0293, -0.1351],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.out_proj.weight',\n",
       "              tensor([[-0.0614,  0.0830, -0.2659,  ...,  0.1189, -0.0137,  0.0482],\n",
       "                      [-0.1778,  0.0193, -0.1147,  ...,  0.1466, -0.0712,  0.1222],\n",
       "                      [ 0.0252, -0.0838, -0.0195,  ..., -0.0486, -0.0444, -0.0685],\n",
       "                      ...,\n",
       "                      [-0.1649,  0.1263,  0.0311,  ..., -0.0836,  0.0380, -0.0516],\n",
       "                      [-0.0097,  0.0115,  0.0143,  ...,  0.0433, -0.0180,  0.0949],\n",
       "                      [-0.0218,  0.0125,  0.0613,  ..., -0.0623, -0.0480, -0.1458]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.attention.out_proj.bias',\n",
       "              tensor([-0.1127, -0.1792,  0.0809,  ...,  0.1097,  0.0021,  0.0733],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.layer_norm.weight',\n",
       "              tensor([0.4561, 0.5275, 0.2312,  ..., 0.5893, 0.4331, 0.3593], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.layer_norm.bias',\n",
       "              tensor([ 0.0729, -0.0250, -0.0457,  ..., -0.0262, -0.0447, -0.0536],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[-0.1028,  0.0698,  0.1335,  ...,  0.1710,  0.0488, -0.0980],\n",
       "                      [ 0.0132,  0.0105,  0.0242,  ..., -0.0337, -0.0026,  0.0546],\n",
       "                      [-0.2227,  0.0437, -0.0522,  ..., -0.0687, -0.0296,  0.1942],\n",
       "                      ...,\n",
       "                      [ 0.0008, -0.0201,  0.0148,  ...,  0.0112,  0.0257, -0.0705],\n",
       "                      [-0.1210, -0.0218, -0.0837,  ..., -0.1207, -0.0391, -0.0844],\n",
       "                      [ 0.0805,  0.1489,  0.0721,  ..., -0.1160,  0.0319, -0.1644]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.0369, -0.1199, -0.0818,  ..., -0.0308, -0.1571, -0.0752],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.feed_forward.output_dense.weight',\n",
       "              tensor([[ 0.0215, -0.0185, -0.0173,  ...,  0.0350,  0.0150, -0.0128],\n",
       "                      [ 0.0435, -0.0274,  0.1219,  ..., -0.0442,  0.3414, -0.0263],\n",
       "                      [-0.1005, -0.0914,  0.0306,  ..., -0.0630,  0.1763,  0.0070],\n",
       "                      ...,\n",
       "                      [ 0.0203, -0.0778, -0.0883,  ...,  0.0515, -0.0361,  0.0554],\n",
       "                      [-0.0291,  0.0385, -0.0238,  ..., -0.0122, -0.0612, -0.0042],\n",
       "                      [-0.0974,  0.0313,  0.0910,  ...,  0.0649,  0.0486,  0.0081]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.feed_forward.output_dense.bias',\n",
       "              tensor([-0.1150, -0.1159, -0.0356,  ..., -0.0069,  0.1031,  0.0880],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.final_layer_norm.weight',\n",
       "              tensor([0.5998, 0.8034, 0.4082,  ..., 0.8271, 0.6566, 0.5157], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.22.final_layer_norm.bias',\n",
       "              tensor([ 0.1007,  0.0980,  0.0009,  ..., -0.0521, -0.2230, -0.2087],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.k_proj.weight',\n",
       "              tensor([[-0.1148,  0.0905,  0.1255,  ..., -0.0447, -0.0077,  0.0235],\n",
       "                      [ 0.0227, -0.0388, -0.0751,  ..., -0.1040,  0.0571, -0.1132],\n",
       "                      [-0.0100, -0.0067, -0.0975,  ..., -0.0971,  0.0095, -0.0666],\n",
       "                      ...,\n",
       "                      [-0.0133, -0.0473, -0.0430,  ...,  0.0444, -0.0553, -0.0594],\n",
       "                      [ 0.0859,  0.0782, -0.0141,  ...,  0.1291,  0.0061, -0.0648],\n",
       "                      [ 0.0798,  0.0164,  0.0360,  ...,  0.0089,  0.0366,  0.0336]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.k_proj.bias',\n",
       "              tensor([ 0.8698, -0.5138, -0.7796,  ...,  0.0119, -0.3281, -0.0789],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.v_proj.weight',\n",
       "              tensor([[ 0.1488, -0.1183, -0.0090,  ...,  0.3059, -0.1909, -0.1597],\n",
       "                      [ 0.0210,  0.1277, -0.0060,  ...,  0.1128, -0.0275, -0.1188],\n",
       "                      [ 0.0793, -0.2348,  0.1016,  ...,  0.2104,  0.0439, -0.0489],\n",
       "                      ...,\n",
       "                      [ 0.0053, -0.0250, -0.0481,  ...,  0.0606, -0.0597,  0.0413],\n",
       "                      [ 0.0389,  0.0091, -0.0363,  ...,  0.1967, -0.0971, -0.0097],\n",
       "                      [ 0.0397,  0.0817, -0.0524,  ...,  0.0157,  0.0410, -0.0281]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.v_proj.bias',\n",
       "              tensor([-0.0766,  0.0535,  0.1032,  ..., -0.0426, -0.0827, -0.0039],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.q_proj.weight',\n",
       "              tensor([[ 3.9575e-02, -1.5180e-01,  5.4038e-02,  ...,  3.6284e-03,\n",
       "                        1.5258e-01,  1.5396e-02],\n",
       "                      [-4.3381e-02,  4.9828e-02,  8.5105e-04,  ..., -9.3451e-05,\n",
       "                       -1.0547e-01,  7.5072e-04],\n",
       "                      [-4.2779e-02,  1.2108e-01, -5.0635e-02,  ...,  7.2685e-02,\n",
       "                        1.1608e-01, -5.5684e-02],\n",
       "                      ...,\n",
       "                      [ 1.0107e-01, -8.0742e-02,  7.1307e-02,  ...,  7.3882e-02,\n",
       "                       -7.0298e-03,  4.4404e-02],\n",
       "                      [ 3.4613e-02, -4.5924e-03,  2.2631e-02,  ..., -3.8851e-02,\n",
       "                       -1.9028e-02,  1.5851e-03],\n",
       "                      [-4.9128e-02,  7.3954e-03, -7.5088e-02,  ...,  8.0002e-02,\n",
       "                        1.4112e-01, -1.1694e-01]], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.q_proj.bias',\n",
       "              tensor([-0.1149,  0.0129,  0.0266,  ...,  0.0672,  0.0410,  0.0039],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.out_proj.weight',\n",
       "              tensor([[-0.0873, -0.1309, -0.1500,  ...,  0.0875,  0.0364, -0.0141],\n",
       "                      [ 0.1068, -0.1612, -0.1637,  ..., -0.0737,  0.0630,  0.1133],\n",
       "                      [-0.0015,  0.0786,  0.0177,  ...,  0.1006,  0.0607,  0.1152],\n",
       "                      ...,\n",
       "                      [-0.1001, -0.1455, -0.3283,  ..., -0.0808, -0.1226, -0.0690],\n",
       "                      [ 0.0866,  0.1115, -0.1070,  ...,  0.0747,  0.0599, -0.1113],\n",
       "                      [-0.1053,  0.1374,  0.1053,  ..., -0.1164, -0.0608,  0.0086]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.attention.out_proj.bias',\n",
       "              tensor([-0.1295, -0.0927, -0.0168,  ..., -0.0089,  0.0321,  0.0654],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.layer_norm.weight',\n",
       "              tensor([0.2905, 0.3848, 0.1781,  ..., 0.4464, 0.3485, 0.2402], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.layer_norm.bias',\n",
       "              tensor([ 0.0217,  0.0299, -0.0829,  ...,  0.0100,  0.0031, -0.0101],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.feed_forward.intermediate_dense.weight',\n",
       "              tensor([[ 0.0026, -0.0456, -0.0133,  ..., -0.0046,  0.0019, -0.0075],\n",
       "                      [ 0.0177, -0.0390,  0.0326,  ...,  0.1081, -0.0831,  0.0097],\n",
       "                      [-0.0122, -0.0270,  0.0348,  ..., -0.1447, -0.0542, -0.0683],\n",
       "                      ...,\n",
       "                      [ 0.0288, -0.0703, -0.0562,  ...,  0.1041,  0.1497, -0.0531],\n",
       "                      [-0.0651, -0.1335,  0.0159,  ...,  0.1253, -0.0523, -0.0089],\n",
       "                      [-0.0188, -0.0243, -0.0647,  ..., -0.0262, -0.0414, -0.0242]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.feed_forward.intermediate_dense.bias',\n",
       "              tensor([-0.1316, -0.0887, -0.1332,  ..., -0.1879, -0.0412, -0.0548],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.feed_forward.output_dense.weight',\n",
       "              tensor([[-0.0125, -0.0266,  0.1369,  ..., -0.0047,  0.0738,  0.0317],\n",
       "                      [ 0.0883,  0.0046,  0.0051,  ...,  0.1313,  0.0184,  0.0381],\n",
       "                      [-0.0536,  0.0099, -0.0182,  ..., -0.0358, -0.0322, -0.0075],\n",
       "                      ...,\n",
       "                      [ 0.0536, -0.0006,  0.1760,  ..., -0.0994, -0.0184, -0.0802],\n",
       "                      [ 0.0850,  0.0724, -0.0182,  ..., -0.0956, -0.0076, -0.0221],\n",
       "                      [-0.0217, -0.0157, -0.0165,  ..., -0.0084, -0.0153, -0.0431]],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.feed_forward.output_dense.bias',\n",
       "              tensor([-0.0703,  0.0455, -0.0384,  ..., -0.0872, -0.0717,  0.0024],\n",
       "                     device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.final_layer_norm.weight',\n",
       "              tensor([0.3796, 0.5284, 0.2881,  ..., 0.5173, 0.4062, 0.3457], device='cuda:0')),\n",
       "             ('wav2vec2.encoder.layers.23.final_layer_norm.bias',\n",
       "              tensor([ 0.0523,  0.1230, -0.0670,  ...,  0.0007, -0.1076, -0.1015],\n",
       "                     device='cuda:0')),\n",
       "             ('lm_head.weight',\n",
       "              tensor([[-0.0487,  0.0610,  0.0422,  ...,  0.0483, -0.0477,  0.0610],\n",
       "                      [ 0.1364, -0.0333, -0.0485,  ..., -0.0257, -0.0567, -0.0632],\n",
       "                      [ 0.1516, -0.0479, -0.0620,  ..., -0.0541, -0.0555, -0.0791],\n",
       "                      ...,\n",
       "                      [-0.0002, -0.0873,  0.0644,  ...,  0.0660,  0.0457, -0.0648],\n",
       "                      [-0.0779, -0.0012, -0.1335,  ..., -0.1219,  0.0574,  0.0105],\n",
       "                      [ 0.1241, -0.0091,  0.0344,  ..., -0.0020,  0.0375,  0.0551]],\n",
       "                     device='cuda:0')),\n",
       "             ('lm_head.bias',\n",
       "              tensor([ 5.1623e-02, -6.0152e-01, -6.0362e-01, -7.0756e-02,  4.7726e-04,\n",
       "                      -3.3200e-02, -4.0185e-02,  3.2312e-03, -2.1396e-02, -1.0725e-02,\n",
       "                      -1.2793e-02,  8.5522e-03, -2.7930e-02, -6.8949e-03, -1.2504e-02,\n",
       "                      -2.2481e-03, -4.7258e-02, -1.8597e-02, -1.4799e-02, -2.8081e-02,\n",
       "                      -1.0862e-02, -5.7171e-03, -2.5538e-02, -5.4265e-02, -1.4168e-02,\n",
       "                      -7.9462e-03,  1.0719e-03, -6.4024e-03, -2.5612e-02, -2.0320e-02,\n",
       "                      -5.1308e-02, -1.6362e-02, -4.0602e-02], device='cuda:0'))])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wav2vec2.masked_spec_embed\n",
      "wav2vec2.feature_extractor.conv_layers.0.conv.weight\n",
      "wav2vec2.feature_extractor.conv_layers.0.conv.bias\n",
      "wav2vec2.feature_extractor.conv_layers.0.layer_norm.weight\n",
      "wav2vec2.feature_extractor.conv_layers.0.layer_norm.bias\n",
      "wav2vec2.feature_extractor.conv_layers.1.conv.weight\n",
      "wav2vec2.feature_extractor.conv_layers.1.conv.bias\n",
      "wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight\n",
      "wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias\n",
      "wav2vec2.feature_extractor.conv_layers.2.conv.weight\n",
      "wav2vec2.feature_extractor.conv_layers.2.conv.bias\n",
      "wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight\n",
      "wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias\n",
      "wav2vec2.feature_extractor.conv_layers.3.conv.weight\n",
      "wav2vec2.feature_extractor.conv_layers.3.conv.bias\n",
      "wav2vec2.feature_extractor.conv_layers.3.layer_norm.weight\n",
      "wav2vec2.feature_extractor.conv_layers.3.layer_norm.bias\n",
      "wav2vec2.feature_extractor.conv_layers.4.conv.weight\n",
      "wav2vec2.feature_extractor.conv_layers.4.conv.bias\n",
      "wav2vec2.feature_extractor.conv_layers.4.layer_norm.weight\n",
      "wav2vec2.feature_extractor.conv_layers.4.layer_norm.bias\n",
      "wav2vec2.feature_extractor.conv_layers.5.conv.weight\n",
      "wav2vec2.feature_extractor.conv_layers.5.conv.bias\n",
      "wav2vec2.feature_extractor.conv_layers.5.layer_norm.weight\n",
      "wav2vec2.feature_extractor.conv_layers.5.layer_norm.bias\n",
      "wav2vec2.feature_extractor.conv_layers.6.conv.weight\n",
      "wav2vec2.feature_extractor.conv_layers.6.conv.bias\n",
      "wav2vec2.feature_extractor.conv_layers.6.layer_norm.weight\n",
      "wav2vec2.feature_extractor.conv_layers.6.layer_norm.bias\n",
      "wav2vec2.feature_projection.layer_norm.weight\n",
      "wav2vec2.feature_projection.layer_norm.bias\n",
      "wav2vec2.feature_projection.projection.weight\n",
      "wav2vec2.feature_projection.projection.bias\n",
      "wav2vec2.encoder.pos_conv_embed.conv.bias\n",
      "wav2vec2.encoder.pos_conv_embed.conv.weight_g\n",
      "wav2vec2.encoder.pos_conv_embed.conv.weight_v\n",
      "wav2vec2.encoder.layer_norm.weight\n",
      "wav2vec2.encoder.layer_norm.bias\n",
      "wav2vec2.encoder.layers.0.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.0.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.0.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.0.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.0.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.0.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.0.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.0.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.0.layer_norm.weight\n",
      "wav2vec2.encoder.layers.0.layer_norm.bias\n",
      "wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.0.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.0.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.0.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.0.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.1.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.1.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.1.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.1.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.1.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.1.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.1.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.1.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.1.layer_norm.weight\n",
      "wav2vec2.encoder.layers.1.layer_norm.bias\n",
      "wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.1.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.1.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.1.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.1.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.2.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.2.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.2.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.2.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.2.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.2.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.2.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.2.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.2.layer_norm.weight\n",
      "wav2vec2.encoder.layers.2.layer_norm.bias\n",
      "wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.2.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.2.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.2.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.2.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.3.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.3.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.3.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.3.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.3.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.3.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.3.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.3.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.3.layer_norm.weight\n",
      "wav2vec2.encoder.layers.3.layer_norm.bias\n",
      "wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.3.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.3.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.3.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.3.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.4.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.4.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.4.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.4.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.4.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.4.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.4.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.4.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.4.layer_norm.weight\n",
      "wav2vec2.encoder.layers.4.layer_norm.bias\n",
      "wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.4.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.4.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.4.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.4.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.5.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.5.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.5.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.5.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.5.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.5.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.5.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.5.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.5.layer_norm.weight\n",
      "wav2vec2.encoder.layers.5.layer_norm.bias\n",
      "wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.5.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.5.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.5.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.5.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.6.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.6.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.6.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.6.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.6.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.6.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.6.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.6.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.6.layer_norm.weight\n",
      "wav2vec2.encoder.layers.6.layer_norm.bias\n",
      "wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.6.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.6.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.6.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.6.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.6.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.7.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.7.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.7.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.7.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.7.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.7.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.7.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.7.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.7.layer_norm.weight\n",
      "wav2vec2.encoder.layers.7.layer_norm.bias\n",
      "wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.7.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.7.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.7.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.7.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.7.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.8.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.8.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.8.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.8.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.8.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.8.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.8.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.8.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.8.layer_norm.weight\n",
      "wav2vec2.encoder.layers.8.layer_norm.bias\n",
      "wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.8.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.8.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.8.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.8.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.8.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.9.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.9.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.9.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.9.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.9.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.9.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.9.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.9.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.9.layer_norm.weight\n",
      "wav2vec2.encoder.layers.9.layer_norm.bias\n",
      "wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.9.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.9.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.9.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.9.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.9.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.10.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.10.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.10.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.10.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.10.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.10.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.10.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.10.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.10.layer_norm.weight\n",
      "wav2vec2.encoder.layers.10.layer_norm.bias\n",
      "wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.10.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.10.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.10.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.10.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.10.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.11.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.11.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.11.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.11.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.11.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.11.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.11.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.11.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.11.layer_norm.weight\n",
      "wav2vec2.encoder.layers.11.layer_norm.bias\n",
      "wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.11.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.11.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.11.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.11.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.11.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.12.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.12.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.12.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.12.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.12.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.12.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.12.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.12.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.12.layer_norm.weight\n",
      "wav2vec2.encoder.layers.12.layer_norm.bias\n",
      "wav2vec2.encoder.layers.12.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.12.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.12.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.12.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.12.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.12.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.13.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.13.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.13.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.13.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.13.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.13.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.13.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.13.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.13.layer_norm.weight\n",
      "wav2vec2.encoder.layers.13.layer_norm.bias\n",
      "wav2vec2.encoder.layers.13.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.13.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.13.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.13.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.13.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.13.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.14.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.14.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.14.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.14.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.14.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.14.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.14.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.14.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.14.layer_norm.weight\n",
      "wav2vec2.encoder.layers.14.layer_norm.bias\n",
      "wav2vec2.encoder.layers.14.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.14.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.14.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.14.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.14.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.14.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.15.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.15.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.15.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.15.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.15.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.15.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.15.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.15.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.15.layer_norm.weight\n",
      "wav2vec2.encoder.layers.15.layer_norm.bias\n",
      "wav2vec2.encoder.layers.15.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.15.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.15.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.15.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.15.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.15.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.16.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.16.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.16.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.16.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.16.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.16.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.16.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.16.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.16.layer_norm.weight\n",
      "wav2vec2.encoder.layers.16.layer_norm.bias\n",
      "wav2vec2.encoder.layers.16.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.16.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.16.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.16.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.16.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.16.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.17.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.17.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.17.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.17.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.17.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.17.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.17.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.17.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.17.layer_norm.weight\n",
      "wav2vec2.encoder.layers.17.layer_norm.bias\n",
      "wav2vec2.encoder.layers.17.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.17.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.17.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.17.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.17.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.17.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.18.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.18.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.18.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.18.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.18.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.18.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.18.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.18.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.18.layer_norm.weight\n",
      "wav2vec2.encoder.layers.18.layer_norm.bias\n",
      "wav2vec2.encoder.layers.18.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.18.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.18.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.18.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.18.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.18.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.19.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.19.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.19.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.19.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.19.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.19.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.19.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.19.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.19.layer_norm.weight\n",
      "wav2vec2.encoder.layers.19.layer_norm.bias\n",
      "wav2vec2.encoder.layers.19.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.19.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.19.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.19.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.19.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.19.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.20.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.20.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.20.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.20.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.20.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.20.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.20.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.20.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.20.layer_norm.weight\n",
      "wav2vec2.encoder.layers.20.layer_norm.bias\n",
      "wav2vec2.encoder.layers.20.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.20.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.20.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.20.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.20.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.20.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.21.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.21.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.21.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.21.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.21.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.21.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.21.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.21.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.21.layer_norm.weight\n",
      "wav2vec2.encoder.layers.21.layer_norm.bias\n",
      "wav2vec2.encoder.layers.21.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.21.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.21.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.21.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.21.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.21.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.22.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.22.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.22.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.22.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.22.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.22.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.22.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.22.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.22.layer_norm.weight\n",
      "wav2vec2.encoder.layers.22.layer_norm.bias\n",
      "wav2vec2.encoder.layers.22.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.22.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.22.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.22.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.22.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.22.final_layer_norm.bias\n",
      "wav2vec2.encoder.layers.23.attention.k_proj.weight\n",
      "wav2vec2.encoder.layers.23.attention.k_proj.bias\n",
      "wav2vec2.encoder.layers.23.attention.v_proj.weight\n",
      "wav2vec2.encoder.layers.23.attention.v_proj.bias\n",
      "wav2vec2.encoder.layers.23.attention.q_proj.weight\n",
      "wav2vec2.encoder.layers.23.attention.q_proj.bias\n",
      "wav2vec2.encoder.layers.23.attention.out_proj.weight\n",
      "wav2vec2.encoder.layers.23.attention.out_proj.bias\n",
      "wav2vec2.encoder.layers.23.layer_norm.weight\n",
      "wav2vec2.encoder.layers.23.layer_norm.bias\n",
      "wav2vec2.encoder.layers.23.feed_forward.intermediate_dense.weight\n",
      "wav2vec2.encoder.layers.23.feed_forward.intermediate_dense.bias\n",
      "wav2vec2.encoder.layers.23.feed_forward.output_dense.weight\n",
      "wav2vec2.encoder.layers.23.feed_forward.output_dense.bias\n",
      "wav2vec2.encoder.layers.23.final_layer_norm.weight\n",
      "wav2vec2.encoder.layers.23.final_layer_norm.bias\n",
      "lm_head.weight\n",
      "lm_head.bias\n"
     ]
    }
   ],
   "source": [
    "for k,v in model.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = \"[1024]\"\n",
    "data = test_str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[1024]']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(data[0][1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./block_files/speech_recognition/pickle/4d809a89d92f7d135c207c8647df3b547ab4db2d0c4539fece4b69191e193273.pkl'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.path.join(\"./block_files/speech_recognition/pickle\",\"4d809a89d92f7d135c207c8647df3b547ab4db2d0c4539fece4b69191e193273\"+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1065,  0.3041, -0.2950,  0.2020,  0.2700,  0.1962, -0.1835,  0.0407,\n",
       "         0.2676,  0.0689,  0.0622,  0.1616, -0.2105,  0.1931, -0.1889, -0.2771,\n",
       "        -0.2254, -0.1926, -0.0225, -0.0096])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"block_files/pickle/3dabdefacea5de81bc5ab876106e967ba270c75d73b8d0a1054e890e882033db.pkl\",\"rb\") as file:\n",
    "    data = pickle.load(file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(424, 424)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin = torch.load(\"pytorch_model.bin\")\n",
    "compose = torch.load(\"pytorch_model_compose.pt\")\n",
    "# origin == compose\n",
    "len(origin), len(compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(origin) == len(compose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/jy_zhang/code/test1.ipynb 单元格 55\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jy_zhang/code/test1.ipynb#Y112sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m origin[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "origin[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/jy_zhang/code/test1.ipynb 单元格 56\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jy_zhang/code/test1.ipynb#Y105sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m io,ic \u001b[39min\u001b[39;00m origin\u001b[39m.\u001b[39mitems(),compose\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jy_zhang/code/test1.ipynb#Y105sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(io,ic)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "for io,ic in origin.items(),compose.items():\n",
    "    print(io,ic)\n",
    "origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin.keys() == compose.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin.values() == compose.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_values([tensor([ 0.5507,  0.4647, -0.8060,  ...,  0.5205, -0.4617, -0.2501],\n",
       "       device='cuda:0'), tensor([[[-0.0301,  0.0601, -0.0824,  ...,  0.1017, -0.0714,  0.0293]],\n",
       "\n",
       "        [[-0.0608,  0.2048, -0.3384,  ..., -0.2467,  0.1772, -0.0568]],\n",
       "\n",
       "        [[-0.0760,  0.1932, -0.2710,  ...,  0.1458, -0.0731,  0.0205]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0909, -0.0269, -0.0012,  ...,  0.2131, -0.2964, -0.1357]],\n",
       "\n",
       "        [[-0.0822,  0.0698,  0.0471,  ...,  0.0196, -0.0282,  0.0362]],\n",
       "\n",
       "        [[ 0.0054, -0.0195,  0.0404,  ...,  0.0238,  0.0038, -0.0026]]],\n",
       "       device='cuda:0'), tensor([-0.0115, -0.0240, -0.0242, -0.0116, -0.0116, -0.0115, -0.0226, -0.0405,\n",
       "        -0.0252, -0.0504, -0.0115, -0.0115, -0.0113, -0.0119, -0.0114, -0.0120,\n",
       "        -0.0186, -0.0115, -0.0113, -0.0115, -0.0119, -0.0117, -0.0217, -0.0231,\n",
       "        -0.0044, -0.0116, -0.0122, -0.0116, -0.0117, -0.0440, -0.0117, -0.0105,\n",
       "        -0.0052, -0.0637, -0.0079, -0.0114, -0.0314, -0.0117, -0.0029, -0.0120,\n",
       "        -0.0118, -0.0116, -0.0116, -0.0119, -0.0114, -0.0114, -0.0113, -0.0116,\n",
       "        -0.0654, -0.0123, -0.0384, -0.0128, -0.0457, -0.0120, -0.0408, -0.0116,\n",
       "        -0.0117, -0.0157, -0.0122, -0.0251, -0.0117, -0.0229, -0.0115, -0.0064,\n",
       "        -0.0117, -0.0731, -0.0151, -0.0114, -0.0117, -0.0116, -0.0113, -0.0119,\n",
       "        -0.0115, -0.0114, -0.0732, -0.0381, -0.0010, -0.0113, -0.0117, -0.0114,\n",
       "        -0.0112, -0.0117, -0.0122, -0.0213, -0.0116, -0.0115, -0.0222, -0.0121,\n",
       "        -0.0121, -0.0417, -0.0118, -0.0114, -0.0114, -0.0120, -0.0117, -0.0116,\n",
       "        -0.0114, -0.0116, -0.0122, -0.0117, -0.0120, -0.0388, -0.0117, -0.0119,\n",
       "        -0.0116, -0.0117, -0.0115, -0.0225, -0.0116, -0.0117, -0.0110, -0.0118,\n",
       "        -0.0117, -0.0045, -0.0119, -0.0120, -0.0117, -0.0118, -0.0121, -0.0323,\n",
       "        -0.0233, -0.0123, -0.0063, -0.0117, -0.0116, -0.0114, -0.0116, -0.0124,\n",
       "        -0.0114, -0.0118, -0.0116, -0.0608, -0.0117, -0.0117, -0.0115, -0.0120,\n",
       "        -0.0211, -0.0120, -0.0076, -0.0120, -0.0112, -0.0243, -0.0119, -0.0119,\n",
       "        -0.0120, -0.0120, -0.0116, -0.0118, -0.0118, -0.0241, -0.0117, -0.0120,\n",
       "        -0.0120, -0.0379, -0.0327, -0.0116, -0.0117, -0.0119, -0.0114, -0.0115,\n",
       "        -0.0116, -0.0117, -0.0510, -0.0088, -0.0120, -0.0118, -0.0120, -0.0120,\n",
       "        -0.0117, -0.0070, -0.0151, -0.0117, -0.0116, -0.0074, -0.0704, -0.0118,\n",
       "        -0.0115, -0.0242, -0.0119, -0.0116, -0.0120, -0.0118, -0.0421, -0.0136,\n",
       "        -0.0117, -0.0457, -0.0117, -0.0122, -0.0418, -0.0115, -0.0119, -0.0115,\n",
       "        -0.0120, -0.0118, -0.0248, -0.0121, -0.0448, -0.0598, -0.1759, -0.0115,\n",
       "        -0.0120, -0.0143, -0.0120, -0.0118, -0.0221, -0.0109, -0.0118, -0.0100,\n",
       "        -0.0043, -0.0117, -0.0114, -0.0119, -0.0120, -0.0098, -0.0114, -0.0067,\n",
       "        -0.0164, -0.0116, -0.0371, -0.0118, -0.0117, -0.0240, -0.0120, -0.0121,\n",
       "        -0.0444, -0.0116, -0.0116, -0.0115, -0.0352, -0.0116, -0.0117, -0.0251,\n",
       "        -0.0115, -0.0119, -0.0241, -0.0054, -0.0115, -0.0118, -0.0118, -0.0109,\n",
       "        -0.0119, -0.0157, -0.0115, -0.0654, -0.0117, -0.0102, -0.0116, -0.0119,\n",
       "        -0.0115, -0.0116, -0.0115, -0.0113, -0.0114, -0.0118, -0.0469, -0.0116,\n",
       "        -0.0116, -0.0639, -0.0114, -0.0115, -0.0119, -0.0116, -0.0109, -0.0125,\n",
       "        -0.0119, -0.0545, -0.0116, -0.0516, -0.0122, -0.0119, -0.0117, -0.0332,\n",
       "        -0.0118, -0.0115, -0.0352, -0.0117, -0.0080, -0.0408, -0.0114, -0.0420,\n",
       "        -0.0126, -0.0022, -0.0116, -0.0117, -0.0237, -0.0118, -0.0116, -0.0039,\n",
       "        -0.0116, -0.0053, -0.0115, -0.0121, -0.0564, -0.0117, -0.0466, -0.0117,\n",
       "        -0.0113, -0.0117, -0.0118, -0.0031, -0.0024, -0.0109, -0.0118, -0.0115,\n",
       "        -0.0116, -0.0118, -0.0119, -0.0324, -0.0119, -0.0117, -0.0870, -0.0225,\n",
       "        -0.0120, -0.0477, -0.0115, -0.0116, -0.0115, -0.0118, -0.0122, -0.0984,\n",
       "        -0.0120, -0.0113, -0.0119, -0.0100, -0.0114, -0.0119, -0.0505, -0.0116,\n",
       "        -0.0116, -0.0115, -0.0117, -0.0118, -0.0115, -0.0116, -0.0588, -0.0117,\n",
       "        -0.0428, -0.0131, -0.0409, -0.0440, -0.0122, -0.0118, -0.0115, -0.0119,\n",
       "        -0.0113, -0.0117, -0.0038, -0.0116, -0.0117, -0.0121, -0.0116, -0.0115,\n",
       "        -0.0115, -0.0119, -0.0115, -0.0115, -0.0116, -0.0115, -0.0117, -0.0117,\n",
       "        -0.0117, -0.0121, -0.0116, -0.0762, -0.0118, -0.0116, -0.0120, -0.0284,\n",
       "        -0.0042, -0.0208, -0.0118, -0.0865, -0.0226, -0.0118, -0.0117, -0.0118,\n",
       "        -0.0118, -0.0117, -0.0118, -0.0431, -0.0162, -0.0116, -0.0118, -0.0109,\n",
       "        -0.0381, -0.0116, -0.0116, -0.0410, -0.0447, -0.0116, -0.0264, -0.0254,\n",
       "        -0.0118, -0.0107, -0.0113, -0.0117, -0.0121, -0.0228, -0.0118, -0.0119,\n",
       "        -0.0118, -0.0116, -0.0117, -0.0117, -0.0241, -0.0117, -0.0034, -0.0072,\n",
       "        -0.0081, -0.0113, -0.0117, -0.0673, -0.0067, -0.0117, -0.0336, -0.0120,\n",
       "        -0.0241, -0.0870, -0.0771, -0.0106, -0.0117, -0.0115, -0.0689, -0.0117,\n",
       "        -0.0422, -0.0117, -0.0113, -0.0425, -0.0118, -0.0116, -0.0117, -0.0580,\n",
       "        -0.0118, -0.0120, -0.0042, -0.0116, -0.0962, -0.0114, -0.0117, -0.0115,\n",
       "        -0.0305, -0.0108, -0.0116, -0.0115, -0.0115, -0.0118, -0.0119, -0.0118,\n",
       "        -0.0439, -0.0115, -0.0122, -0.0114, -0.0117, -0.0410, -0.0095, -0.0116,\n",
       "        -0.0113, -0.0121, -0.0122, -0.0068, -0.0178, -0.0387, -0.0117, -0.0116,\n",
       "        -0.0129, -0.0117, -0.0119, -0.0090, -0.0118, -0.0116, -0.0237, -0.0118,\n",
       "        -0.0116, -0.0121, -0.0117, -0.0276, -0.0238, -0.0111, -0.0116, -0.0115,\n",
       "        -0.0117, -0.0117, -0.0126, -0.0116, -0.0110, -0.0115, -0.0115, -0.0119,\n",
       "        -0.0113, -0.0378, -0.0049, -0.0113, -0.0081, -0.0120, -0.0118, -0.0377,\n",
       "        -0.0117, -0.0116, -0.0118, -0.0120, -0.0113, -0.0506, -0.0116, -0.0115,\n",
       "        -0.0118, -0.0115, -0.0118, -0.0118, -0.0080, -0.0046, -0.0119, -0.0119],\n",
       "       device='cuda:0'), tensor([-4.4608e-04,  1.7529e+00,  2.0762e+00,  4.3259e-03,  1.8101e-03,\n",
       "        -1.5535e-03,  9.8730e-01,  8.2861e-01,  6.6992e-01,  1.1426e+00,\n",
       "         1.7433e-03,  4.0855e-03, -2.0230e-04,  6.3934e-03,  4.5280e-03,\n",
       "        -8.2350e-04,  4.3677e-01,  6.6376e-03,  2.5711e-03,  2.8343e-03,\n",
       "         4.0321e-03,  4.5929e-03,  4.9390e-01,  1.5654e+00,  3.6963e-01,\n",
       "         7.6675e-04,  1.8396e-03,  5.2185e-03,  1.3039e-02,  1.0625e+00,\n",
       "         1.3103e-03,  3.7134e-01,  4.3604e-01,  8.9990e-01,  4.0063e-01,\n",
       "         4.2229e-03,  1.2129e+00,  8.8120e-03,  3.7720e-01,  2.3384e-03,\n",
       "         8.1024e-03, -1.0242e-03,  4.4060e-03, -5.3167e-05,  2.0599e-03,\n",
       "         2.4147e-03,  3.1357e-03,  5.1422e-03,  7.7441e-01, -2.0962e-03,\n",
       "         7.6807e-01,  2.9316e-03,  8.8672e-01,  3.6221e-03,  1.0312e+00,\n",
       "         9.0179e-03,  1.9178e-03,  4.9438e-01, -2.2340e-04,  1.4346e+00,\n",
       "         2.5482e-03,  9.8975e-01,  5.9843e-04,  4.0820e-01,  7.4539e-03,\n",
       "         6.6260e-01,  4.3823e-01,  6.1321e-04,  4.6301e-04,  2.9106e-03,\n",
       "         2.1114e-03,  1.5404e-02,  9.4299e-03,  8.8453e-04,  7.7002e-01,\n",
       "         9.9463e-01,  3.3398e-01,  1.7920e-03,  1.9348e-04,  1.8044e-03,\n",
       "         3.3340e-03,  4.4174e-03,  3.3722e-03,  1.4922e+00,  1.4572e-03,\n",
       "         5.1422e-03,  2.7129e+00,  5.6028e-04,  1.1292e-03,  9.7900e-01,\n",
       "         3.5534e-03,  3.5419e-03,  4.2648e-03,  1.5783e-03,  5.3120e-04,\n",
       "         1.3992e-02,  1.0315e-02,  7.8487e-04, -8.6355e-04,  1.3702e-02,\n",
       "         1.4889e-04,  1.1738e+00,  8.2397e-03,  7.8058e-04,  5.3177e-03,\n",
       "        -5.4455e-04,  1.7958e-03,  1.2939e+00,  8.0338e-03,  5.5122e-03,\n",
       "        -4.8804e-04,  1.0166e-03,  1.8654e-03,  3.4546e-01,  6.0616e-03,\n",
       "         1.4508e-04,  5.2299e-03,  4.9095e-03, -6.7329e-04,  1.2041e+00,\n",
       "         1.6396e+00, -2.1458e-04,  4.2676e-01,  1.9684e-03,  1.0147e-02,\n",
       "         3.5229e-03, -9.0790e-04,  1.3566e-04,  1.0315e-02,  3.3455e-03,\n",
       "         3.8700e-03,  5.0928e-01, -9.1400e-03,  2.2054e-04,  1.7441e-02,\n",
       "         1.5850e-03,  1.4424e+00, -1.7014e-03,  2.9224e-01, -3.6573e-04,\n",
       "         2.4204e-03,  6.0596e-01,  4.3030e-03,  1.3483e-04, -1.1120e-03,\n",
       "         7.4625e-04,  1.5535e-03,  1.5831e-03,  1.2894e-03,  7.6074e-01,\n",
       "         1.7929e-03,  3.1490e-03,  4.3106e-03,  1.0098e+00,  5.2490e-01,\n",
       "         3.4924e-03,  2.4376e-03, -7.0095e-04,  1.0729e-03,  7.3612e-05,\n",
       "         1.3412e-02,  3.2558e-03,  8.5645e-01,  2.9932e-01,  4.1914e-04,\n",
       "         5.3482e-03,  3.0041e-03,  2.2564e-03,  4.9973e-03,  4.2432e-01,\n",
       "         8.9600e-02,  3.9177e-03,  2.9812e-03,  3.5645e-01,  7.5830e-01,\n",
       "         7.3013e-03,  9.5901e-03,  1.7451e+00,  5.7373e-03,  2.3556e-03,\n",
       "         1.5211e-03,  1.0672e-03,  9.3652e-01,  3.7500e-01,  5.8508e-04,\n",
       "         1.1553e+00,  3.7994e-03, -4.1461e-04,  1.1387e+00,  1.0612e-02,\n",
       "         2.1362e-03,  2.1000e-03,  4.5729e-04,  5.3062e-03,  9.6484e-01,\n",
       "         1.9627e-03,  1.0410e+00,  9.5068e-01,  6.2451e-01,  1.0815e-03,\n",
       "         5.4884e-04,  2.5220e-01, -3.7360e-04,  5.9242e-03,  1.8242e+00,\n",
       "         2.4462e-04,  3.4771e-03,  4.3042e-01,  4.1162e-01,  1.4847e-02,\n",
       "         1.3952e-03,  4.4899e-03, -2.6512e-04,  2.8711e-01,  1.1559e-03,\n",
       "         2.0850e-01,  2.6489e-01,  8.2779e-03,  1.0244e+00,  7.9727e-03,\n",
       "         7.1466e-05,  9.0771e-01, -8.8120e-04,  2.1324e-03,  9.9414e-01,\n",
       "         1.0414e-02,  3.0351e-04,  4.4098e-03,  8.4717e-01,  7.5042e-05,\n",
       "         8.9798e-03,  1.4463e+00,  7.7133e-03,  3.3817e-03,  1.4248e+00,\n",
       "         3.3765e-01,  3.3627e-03,  6.8359e-03,  8.0061e-04,  1.2197e-03,\n",
       "         2.8312e-05,  3.7549e-01,  2.1496e-03,  7.3193e-01,  3.6087e-03,\n",
       "         3.9062e-01,  4.5443e-04,  8.3685e-05,  5.5504e-04,  4.5509e-03,\n",
       "         7.1220e-03,  1.0939e-03,  5.4359e-03,  3.6449e-03,  6.4209e-01,\n",
       "         1.1168e-03,  1.5282e-02,  3.3887e-01,  2.9907e-03,  3.8700e-03,\n",
       "         4.9210e-03,  1.2712e-03,  2.2812e-03, -2.1160e-05,  1.1616e-03,\n",
       "         9.3945e-01,  1.0628e-02,  1.0420e+00,  6.4850e-04,  1.0214e-03,\n",
       "         6.4774e-03,  6.1230e-01,  2.1248e-03,  4.0550e-03,  1.1787e+00,\n",
       "         7.0038e-03,  4.3164e-01,  1.1016e+00,  8.6670e-03,  1.1074e+00,\n",
       "         1.5440e-03,  4.0210e-01,  2.4567e-03,  9.5062e-03,  2.1914e+00,\n",
       "         7.8869e-04,  2.2373e-03,  4.1479e-01,  5.6686e-03,  3.4131e-01,\n",
       "         1.0557e-03,  2.5883e-03,  1.0059e+00,  7.5989e-03,  9.6973e-01,\n",
       "         2.1219e-05,  1.5602e-03,  1.4000e-02,  4.8752e-03,  4.1504e-01,\n",
       "         3.4131e-01,  9.7656e-04,  7.0343e-03,  9.7427e-03,  1.0193e-02,\n",
       "         4.2496e-03,  8.6212e-03,  6.2207e-01,  7.0190e-03,  1.7853e-03,\n",
       "         5.7861e-01,  1.0430e+00,  2.2278e-03,  1.1025e+00,  2.2316e-03,\n",
       "         6.3591e-03,  5.0697e-03,  7.3195e-04,  5.2185e-03,  4.6826e-01,\n",
       "         2.8667e-03,  3.1986e-03,  1.1263e-03,  3.2373e-01,  2.3594e-03,\n",
       "         5.3825e-03,  1.1582e+00,  1.6769e-02,  7.0953e-03,  1.8940e-03,\n",
       "         9.5654e-04,  4.5776e-03,  6.8817e-03,  1.6220e-02,  5.1318e-01,\n",
       "         5.6114e-03,  8.6426e-01,  9.9219e-01,  9.3506e-01,  9.6875e-01,\n",
       "         4.8375e-04,  2.0332e-03,  2.2373e-03,  1.5869e-03,  2.4462e-04,\n",
       "         1.8711e-03,  3.6670e-01,  7.9918e-04,  7.4768e-04,  1.3695e-03,\n",
       "         3.4399e-01,  1.3256e-03,  4.4670e-03,  4.5052e-03,  1.7214e-03,\n",
       "         3.4218e-03,  2.9812e-03,  3.0155e-03,  5.4836e-04, -3.4833e-04,\n",
       "         7.8278e-03,  1.9608e-03,  5.2528e-03,  4.7559e-01,  3.3998e-04,\n",
       "         1.8738e-02,  1.7405e-03,  5.6738e-01,  3.5767e-01,  1.7988e+00,\n",
       "        -7.5161e-05,  5.9180e-01,  1.8184e+00,  9.7351e-03,  8.7662e-03,\n",
       "         4.2343e-03,  4.5395e-03,  2.5940e-03,  8.0719e-03,  1.0605e+00,\n",
       "         4.8340e-01,  8.2493e-04, -9.0837e-05,  1.4997e-04,  9.9561e-01,\n",
       "         9.8114e-03,  1.8358e-03,  8.6426e-01,  1.1445e+00,  2.8400e-03,\n",
       "         8.0078e-01,  6.5771e-01, -2.8610e-04,  2.2351e-01, -3.4642e-04,\n",
       "        -4.3917e-04,  1.5268e-03,  1.8105e+00,  3.7098e-03,  7.4959e-04,\n",
       "         1.1711e-03,  9.6560e-04,  5.8861e-03,  1.7142e-04,  7.9980e-01,\n",
       "         3.7479e-04,  2.9858e-01,  4.2212e-01,  2.8931e-01,  4.5815e-03,\n",
       "         3.1052e-03,  8.5742e-01,  3.5132e-01,  3.7155e-03,  9.5557e-01,\n",
       "         2.2697e-03,  8.2910e-01,  7.6807e-01,  7.2266e-01,  6.1913e-03,\n",
       "         4.2343e-03,  7.3493e-05,  5.4785e-01,  5.2404e-04,  9.2139e-01,\n",
       "         4.1733e-03, -5.1689e-04,  1.0273e+00,  5.0697e-03,  8.9264e-03,\n",
       "         3.0251e-03,  9.3848e-01,  6.6996e-04,  6.3956e-05,  3.6035e-01,\n",
       "        -3.0470e-04,  4.8926e-01, -7.0953e-04,  3.9825e-03,  5.1212e-04,\n",
       "         1.0713e+00,  3.8208e-01,  8.9931e-04,  1.3016e-02,  3.2215e-03,\n",
       "         3.2082e-03,  9.0456e-04,  6.8207e-03,  1.2061e+00,  1.1597e-03,\n",
       "         1.2016e-03,  7.5388e-04,  5.1270e-03,  9.1064e-01,  7.5244e-01,\n",
       "         2.6932e-03,  5.4312e-04, -5.3740e-04,  2.1591e-03,  3.4106e-01,\n",
       "         4.9048e-01,  9.0918e-01,  3.3150e-03,  4.6349e-03, -1.0002e-04,\n",
       "         2.5177e-03, -2.2078e-04,  3.3936e-01, -3.6621e-04,  1.5114e-02,\n",
       "         3.3105e-01,  3.1204e-03,  1.1568e-03,  9.7132e-04,  9.9869e-03,\n",
       "         3.8696e-01,  1.3369e+00, -4.7565e-05,  1.1162e-02,  1.6623e-03,\n",
       "         3.4237e-03,  1.5450e-03, -8.9931e-04,  1.3313e-03,  5.4741e-04,\n",
       "         1.2474e-02,  2.5902e-03, -2.8014e-04,  4.7565e-04,  5.9082e-01,\n",
       "         4.2334e-01,  2.1706e-03,  2.6318e-01,  3.9148e-04,  4.1604e-04,\n",
       "         1.1338e+00,  3.9940e-03,  9.2239e-03,  8.6823e-03,  2.4652e-04,\n",
       "         1.3771e-03,  9.1797e-01,  8.1100e-03,  9.9659e-04,  9.1553e-03,\n",
       "         1.1091e-03,  3.2120e-03,  2.3174e-03,  3.4741e-01,  2.6318e-01,\n",
       "        -1.0481e-03,  4.5776e-03], device='cuda:0'), tensor([-1.6251e-03,  6.4844e-01,  8.1348e-01,  1.1545e-04, -8.6606e-05,\n",
       "         5.8031e-04,  3.0371e-01, -8.8916e-01,  3.6328e-01, -5.7178e-01,\n",
       "        -1.9522e-03, -3.3784e-04, -1.1816e-03, -2.4548e-03, -9.8419e-04,\n",
       "        -2.1095e-03, -4.3068e-03, -1.4734e-03,  1.2636e-03, -9.4128e-04,\n",
       "         5.3406e-04, -3.4504e-03,  1.3574e-01,  5.4297e-01, -3.2178e-01,\n",
       "        -2.0638e-03, -2.4366e-04, -1.0223e-03, -5.8784e-03, -1.2305e+00,\n",
       "         1.0900e-03, -1.6003e-01, -3.1909e-01,  5.2344e-01, -2.3389e-01,\n",
       "        -1.7929e-03, -1.0967e+00, -2.3918e-03, -3.2397e-01, -1.0271e-03,\n",
       "        -3.1338e-03, -1.6718e-03, -1.3962e-03, -8.5306e-04, -1.7512e-04,\n",
       "        -2.7847e-03, -1.6270e-03, -1.0185e-03,  6.2939e-01, -1.0157e-03,\n",
       "        -1.2412e+00, -1.3475e-03, -1.0957e+00,  2.1152e-03, -1.1709e+00,\n",
       "        -4.1656e-03, -1.3542e-03, -8.3496e-02, -7.5340e-04, -1.0605e+00,\n",
       "        -1.4043e-04,  3.1079e-01, -9.7084e-04, -2.9443e-01, -2.2926e-03,\n",
       "         1.0425e-01, -8.5815e-02, -8.7547e-04, -1.5459e-03, -3.0766e-03,\n",
       "         1.9813e-04, -6.0616e-03, -2.0752e-03, -1.0309e-03,  7.0496e-02,\n",
       "        -1.0293e+00, -3.1909e-01, -1.8358e-05, -4.9067e-04, -2.2850e-03,\n",
       "        -1.3819e-03, -1.4153e-03,  2.5415e-04,  3.0664e-01, -8.4639e-06,\n",
       "        -2.4681e-03,  9.4922e-01,  2.0444e-04,  1.0481e-03, -8.3350e-01,\n",
       "         3.8695e-04, -1.4853e-04,  1.3294e-03,  2.2209e-04, -9.6464e-04,\n",
       "        -3.7022e-03, -1.5621e-03, -1.1501e-03, -1.1263e-03, -6.0692e-03,\n",
       "        -4.2844e-04, -7.8369e-01, -4.1351e-03, -1.0433e-03, -1.0052e-03,\n",
       "        -1.6222e-03,  6.7854e-04,  3.4961e-01, -2.1973e-03, -2.8915e-03,\n",
       "        -1.0910e-03, -1.4162e-03, -9.2125e-04, -2.5171e-01, -9.2173e-04,\n",
       "        -1.1044e-03, -1.8749e-03, -4.2992e-03, -1.5821e-03, -1.0469e+00,\n",
       "         6.0400e-01,  7.9453e-05, -2.8247e-01, -2.3308e-03, -5.0011e-03,\n",
       "         5.9843e-04, -1.8072e-03,  2.8801e-04, -1.6966e-03,  1.0614e-03,\n",
       "        -2.5101e-03,  4.3018e-01,  3.5553e-03,  9.1136e-05, -6.4812e-03,\n",
       "        -4.2844e-04,  2.9492e-01, -1.7729e-03, -1.8555e-01, -2.8801e-04,\n",
       "        -9.3079e-04,  3.9087e-01, -2.5330e-03, -2.5201e-04, -1.5717e-03,\n",
       "         1.5998e-04, -1.0014e-03,  2.6226e-04, -5.5981e-04, -1.1533e+00,\n",
       "         7.4148e-04, -2.0027e-03, -1.4505e-03, -9.4824e-01,  4.7729e-01,\n",
       "        -2.5425e-03, -8.6689e-04, -8.1444e-04, -7.2908e-04, -2.0924e-03,\n",
       "        -3.9368e-03, -1.3075e-03, -6.3184e-01, -1.5491e-01, -1.9627e-03,\n",
       "        -1.6603e-03, -7.7963e-04, -1.3580e-03, -6.2847e-04, -2.9370e-01,\n",
       "        -1.2535e-02, -1.8415e-03,  1.7653e-03, -2.3523e-01, -2.3022e-01,\n",
       "        -2.9087e-03, -4.9095e-03,  6.5869e-01, -1.7414e-03, -2.1591e-03,\n",
       "        -1.2445e-03, -8.8596e-04, -1.2412e+00, -9.9915e-02, -3.7098e-04,\n",
       "        -4.9512e-01, -1.6088e-03, -8.4734e-04, -1.0186e+00, -3.4771e-03,\n",
       "        -1.4277e-03,  4.7708e-04, -1.1148e-03, -2.2793e-03,  3.9429e-01,\n",
       "        -4.4417e-04, -1.1240e+00,  3.3508e-02,  4.7095e-01, -1.4219e-03,\n",
       "        -1.8091e-03, -6.2927e-02, -1.5087e-03, -2.9812e-03,  4.1992e-01,\n",
       "        -2.2089e-04,  1.5488e-03, -2.3792e-01, -3.4009e-01, -5.9738e-03,\n",
       "        -4.4465e-05, -3.1376e-03, -5.6124e-04, -1.4014e-01, -1.6947e-03,\n",
       "        -1.4246e-01, -4.3488e-02, -3.4466e-03, -9.8340e-01, -4.0321e-03,\n",
       "        -8.6689e-04,  3.1274e-01, -5.1022e-04,  1.2283e-03, -1.0117e+00,\n",
       "        -2.6722e-03,  3.8052e-04, -2.5311e-03, -1.2109e+00, -2.8276e-04,\n",
       "        -2.7828e-03, -1.2305e+00, -2.0332e-03, -1.3189e-03,  6.0693e-01,\n",
       "        -2.7588e-01, -1.7052e-03, -3.4351e-03, -2.5673e-03, -2.7370e-03,\n",
       "        -2.7390e-03, -3.9185e-02,  3.6669e-04,  5.4980e-01, -1.6384e-03,\n",
       "        -1.8762e-01, -1.0204e-03, -6.9618e-04, -3.1567e-04,  9.9659e-04,\n",
       "        -2.5330e-03, -2.5120e-03, -2.7943e-03, -1.4048e-03, -1.3457e+00,\n",
       "         1.7762e-04, -6.9618e-03,  2.1301e-01, -1.0242e-03, -8.9550e-04,\n",
       "        -3.5214e-04, -1.1978e-03, -2.0580e-03, -1.5984e-03, -1.4210e-03,\n",
       "        -6.0742e-01, -3.8681e-03, -2.8467e-01, -1.4181e-03,  1.2760e-03,\n",
       "        -1.2932e-03,  5.8789e-01, -1.4324e-03, -2.3022e-03, -1.1699e+00,\n",
       "        -1.1005e-03, -2.5415e-01, -1.1592e+00, -1.6661e-03, -8.9600e-01,\n",
       "         8.6212e-04, -3.5107e-01, -1.3292e-04, -2.7142e-03,  7.8223e-01,\n",
       "        -1.0719e-03, -1.2159e-03, -3.6719e-01, -2.8515e-04, -2.4829e-01,\n",
       "        -1.1402e-04, -8.6164e-04,  3.0060e-02, -1.9217e-03, -7.6953e-01,\n",
       "        -1.8015e-03, -1.2388e-03, -5.3101e-03, -1.6546e-03, -3.8965e-01,\n",
       "        -3.1470e-01, -2.7347e-04, -3.9673e-03, -2.2755e-03, -4.3259e-03,\n",
       "        -2.9202e-03, -4.1237e-03,  5.5322e-01, -2.5101e-03, -7.0477e-04,\n",
       "         3.0957e-01,  2.6221e-01,  1.3173e-05, -4.2700e-01,  6.9237e-04,\n",
       "        -3.8338e-03, -2.5196e-03, -7.0953e-04, -1.7366e-03,  3.4009e-01,\n",
       "        -1.5182e-03, -2.6188e-03, -2.0957e-04, -1.5173e-01,  8.6737e-04,\n",
       "        -3.1090e-03, -2.4890e-01, -7.1220e-03, -1.6890e-03,  2.5511e-04,\n",
       "         1.5998e-04, -2.7866e-03, -1.1120e-03, -7.9422e-03,  4.1718e-02,\n",
       "        -2.0428e-03, -8.9648e-01, -1.1133e+00, -1.1660e+00, -1.1162e+00,\n",
       "        -3.3307e-04, -1.0128e-03, -7.1478e-04,  8.5306e-04, -6.7377e-04,\n",
       "        -5.0545e-05, -2.9517e-01,  4.4799e-04, -1.2541e-03, -1.8587e-03,\n",
       "        -1.3489e-01, -1.6439e-04, -1.6413e-03, -2.7142e-03, -8.9741e-04,\n",
       "        -1.3995e-04, -2.2335e-03, -2.0714e-03, -4.5919e-04, -1.7595e-03,\n",
       "        -3.4046e-03,  9.4271e-04, -3.2120e-03,  5.9521e-01, -6.3944e-04,\n",
       "        -6.1569e-03,  4.5252e-04,  3.7866e-01, -3.0176e-01,  3.2056e-01,\n",
       "        -9.1076e-04,  3.2593e-01,  5.0000e-01, -5.2071e-03, -2.2125e-03,\n",
       "        -8.4734e-04, -2.9640e-03, -2.6798e-04, -2.3842e-03, -7.3340e-01,\n",
       "        -1.0223e-01,  1.0812e-04, -2.2430e-03,  8.6641e-04, -1.0400e+00,\n",
       "        -5.3482e-03,  1.2779e-03, -1.2393e+00, -8.6523e-01,  9.7466e-04,\n",
       "        -1.1260e+00,  3.7842e-01, -1.1654e-03, -8.7585e-02, -1.0926e-04,\n",
       "        -1.8682e-03, -2.7704e-04,  6.5967e-01, -1.8854e-03, -1.5965e-03,\n",
       "        -1.1790e-04, -4.2319e-06, -2.5158e-03, -2.4796e-04,  3.3984e-01,\n",
       "         5.4169e-04, -2.3584e-01, -2.3657e-01, -1.6467e-01, -2.2850e-03,\n",
       "        -2.6703e-03,  2.8760e-01, -2.3633e-01, -1.8072e-03, -1.0977e+00,\n",
       "        -9.9182e-05,  3.4131e-01,  4.2603e-01,  5.8533e-02, -4.1885e-03,\n",
       "        -6.1369e-04, -4.0550e-03,  3.9429e-01, -1.9109e-04, -1.3154e+00,\n",
       "         8.7118e-04, -1.5717e-03, -1.2490e+00, -3.1567e-03, -3.0079e-03,\n",
       "        -8.6880e-04, -6.6016e-01,  7.8201e-05, -3.4618e-04, -2.8101e-01,\n",
       "        -6.1131e-04,  3.3105e-01, -1.1253e-03, -3.9911e-04, -1.3218e-03,\n",
       "        -1.3447e+00, -1.5808e-01, -6.3801e-04, -4.3716e-03,  1.3180e-03,\n",
       "        -9.9564e-04, -4.2558e-04, -5.7840e-04, -6.4600e-01, -6.2609e-04,\n",
       "        -7.1573e-04, -1.1367e-04, -2.8229e-03, -1.2666e+00, -6.9287e-01,\n",
       "         5.5742e-04, -8.8882e-04, -7.8201e-04, -2.2793e-03, -2.2461e-01,\n",
       "         1.5833e-01, -1.3320e+00, -1.1978e-03, -2.8152e-03, -2.7037e-04,\n",
       "        -1.3852e-04, -1.8339e-03, -1.8433e-01, -1.1005e-03, -6.0539e-03,\n",
       "         1.2018e-01, -2.1076e-03, -7.5912e-04,  9.7752e-04, -2.8782e-03,\n",
       "         2.2583e-01,  4.9902e-01, -9.5558e-04, -3.1986e-03, -5.5885e-04,\n",
       "        -1.2093e-03, -4.3511e-04, -1.2360e-03, -7.0095e-05, -5.5730e-05,\n",
       "        -4.2343e-03, -2.3785e-03, -2.3785e-03, -2.9135e-04, -1.1777e+00,\n",
       "        -3.7378e-01, -4.3869e-05, -1.5198e-01, -7.4100e-04, -1.6336e-03,\n",
       "        -9.7656e-01, -3.3779e-03, -3.0785e-03, -2.6054e-03, -6.9189e-04,\n",
       "        -2.5330e-03, -5.8643e-01, -1.3618e-03, -3.2377e-04, -3.2005e-03,\n",
       "         1.5879e-04,  1.0662e-03, -3.6645e-04, -2.3120e-01, -2.1167e-01,\n",
       "        -7.0858e-04, -2.4242e-03], device='cuda:0'), tensor([[[ 9.3765e-03,  2.3621e-02, -8.5373e-03],\n",
       "         [ 1.3745e-01,  1.7627e-01, -3.8770e-01],\n",
       "         [-2.2827e-01,  8.8574e-01, -8.3057e-01],\n",
       "         ...,\n",
       "         [ 6.6284e-02,  1.1725e-01, -5.8929e-02],\n",
       "         [ 7.7744e-03,  4.9591e-03,  2.3899e-03],\n",
       "         [ 1.0345e-02,  2.6112e-03, -2.3537e-03]],\n",
       "\n",
       "        [[ 2.6107e-04,  1.8396e-03,  3.0270e-03],\n",
       "         [ 2.8290e-02,  1.7419e-01, -2.3941e-02],\n",
       "         [ 3.3630e-02, -6.1691e-05,  1.0974e-01],\n",
       "         ...,\n",
       "         [ 8.6304e-02, -9.0881e-02,  1.2817e-01],\n",
       "         [-3.7193e-03, -3.5858e-03, -8.3876e-04],\n",
       "         [ 2.5916e-04,  3.2616e-03,  1.3962e-03]],\n",
       "\n",
       "        [[ 1.3603e-02,  5.0888e-03,  1.1997e-03],\n",
       "         [ 5.4871e-02,  5.1361e-02, -8.4152e-03],\n",
       "         [ 6.4270e-02,  5.2887e-02,  4.8553e-02],\n",
       "         ...,\n",
       "         [ 1.0400e-01,  1.3867e-01, -1.5833e-01],\n",
       "         [ 1.7456e-02,  9.1019e-03,  6.9389e-03],\n",
       "         [ 1.6006e-02, -2.0809e-03, -3.7689e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.4664e-02,  1.1581e-02,  1.4572e-02],\n",
       "         [ 1.9150e-02,  1.5063e-01,  7.0312e-02],\n",
       "         [ 4.3511e-06,  1.0004e-01,  5.0873e-02],\n",
       "         ...,\n",
       "         [ 3.5248e-02, -8.6914e-02,  1.0406e-01],\n",
       "         [ 2.6646e-03,  1.0538e-03,  5.8784e-03],\n",
       "         [ 6.3705e-03, -7.4692e-03, -1.4015e-02]],\n",
       "\n",
       "        [[-3.0518e-03, -1.5306e-03,  2.9969e-04],\n",
       "         [ 5.4382e-02,  7.1716e-02,  3.8025e-02],\n",
       "         [ 6.6772e-02,  5.9143e-02,  4.4189e-02],\n",
       "         ...,\n",
       "         [-2.2839e-01,  1.4917e-01,  2.2812e-02],\n",
       "         [ 9.0332e-03,  2.8419e-03,  6.0158e-03],\n",
       "         [-2.5192e-02, -9.2149e-05,  2.3865e-02]],\n",
       "\n",
       "        [[ 2.8061e-02,  2.1545e-02,  1.6296e-02],\n",
       "         [ 7.1716e-02,  7.9407e-02,  4.4891e-02],\n",
       "         [ 7.8186e-02,  5.4352e-02,  6.3965e-02],\n",
       "         ...,\n",
       "         [-1.3000e-01,  2.9932e-01, -3.2837e-01],\n",
       "         [-2.0905e-02, -2.8229e-02, -2.5208e-02],\n",
       "         [ 3.2898e-02,  2.5116e-02,  8.9550e-04]]], device='cuda:0'), tensor([ 1.2164e-01,  2.0618e-01,  1.1530e-01,  6.6833e-02,  7.7271e-02,\n",
       "         2.3083e-01,  1.4746e-01,  2.5439e-01,  1.3855e-01,  2.1375e-01,\n",
       "         1.1102e-01,  3.1567e-03,  1.3721e-01,  2.5708e-01,  1.3757e-01,\n",
       "        -7.5806e-02,  7.1899e-02,  3.3765e-01,  2.0508e-01,  3.8757e-02,\n",
       "         2.3633e-01,  1.5308e-01,  2.1240e-01,  1.9275e-01,  1.3367e-01,\n",
       "        -3.9520e-02,  1.5857e-01,  1.2646e-01,  1.7310e-01,  3.4253e-01,\n",
       "         2.6514e-01,  2.1570e-01,  1.6223e-01,  2.2388e-01, -1.0992e-01,\n",
       "         1.1047e-01,  1.6003e-01,  1.6370e-01,  2.9053e-01,  2.3083e-01,\n",
       "         6.4392e-02,  1.0590e-01,  2.9126e-01,  1.1053e-01,  1.6821e-01,\n",
       "         2.6660e-01,  1.9885e-01,  1.9312e-01,  1.8823e-01, -1.6333e-01,\n",
       "         2.5635e-01,  1.1603e-01,  1.5210e-01,  1.7338e-03,  9.9304e-02,\n",
       "         1.1090e-01,  1.3208e-01,  2.4451e-01,  1.6614e-01, -5.6793e-02,\n",
       "         1.8835e-01,  2.6581e-02,  1.8591e-01,  1.9617e-01, -1.2146e-01,\n",
       "         1.7017e-01, -1.1047e-01,  2.8003e-01,  1.2231e-01, -7.4585e-02,\n",
       "        -5.4779e-02,  1.7407e-01,  2.4121e-01,  2.1826e-01,  2.5659e-01,\n",
       "         2.3950e-01,  2.6270e-01,  3.3398e-01,  2.3242e-01,  2.7710e-01,\n",
       "         2.4036e-01, -2.5781e-01,  1.6101e-01,  1.5063e-01,  2.5806e-01,\n",
       "         2.5195e-01,  1.5022e-02, -3.6072e-02, -3.3276e-01,  1.6968e-01,\n",
       "        -7.8552e-02,  1.3098e-01,  1.0217e-01,  2.4683e-01,  1.7505e-01,\n",
       "         2.0837e-01,  1.0651e-01,  1.5576e-01,  1.7993e-01,  1.3831e-01,\n",
       "        -2.6047e-02,  3.2617e-01,  2.4304e-01,  1.9690e-01,  2.5171e-01,\n",
       "         1.7151e-01,  2.2449e-01,  1.8188e-01,  1.0651e-01,  3.7201e-02,\n",
       "         2.4780e-01,  4.7729e-02,  6.7261e-02,  1.6296e-01,  1.2842e-01,\n",
       "         2.1228e-01,  7.5134e-02,  1.2488e-01,  1.9507e-01,  4.1772e-01,\n",
       "         1.8799e-01,  2.5513e-01,  1.8896e-01,  1.5540e-01,  1.5430e-01,\n",
       "         1.6040e-01,  8.2214e-02,  3.3008e-01,  1.5295e-01, -1.1383e-02,\n",
       "         1.7908e-01,  1.4124e-01,  2.0166e-01,  2.6367e-01,  2.0581e-01,\n",
       "         1.1407e-01,  1.2512e-01,  1.4502e-01,  1.2756e-01,  2.0630e-01,\n",
       "         1.7249e-01,  7.6172e-02,  2.0764e-01,  2.7832e-01,  1.5906e-01,\n",
       "         1.5955e-01,  2.0972e-01,  1.1102e-01,  1.5601e-01,  2.2064e-02,\n",
       "         1.1554e-01,  2.5366e-01,  2.3450e-01, -4.5471e-02,  1.3281e-01,\n",
       "         1.3782e-01,  5.1208e-02,  2.3914e-01,  2.1948e-01,  3.2886e-01,\n",
       "         1.8274e-01,  2.6855e-01,  1.8628e-01,  2.0215e-01,  2.6147e-01,\n",
       "         1.2354e-01,  1.3806e-01,  1.6998e-02,  2.6318e-01, -3.0176e-01,\n",
       "         2.3596e-01,  1.5161e-01,  1.8408e-01,  1.4258e-01,  1.2903e-01,\n",
       "         1.1945e-01,  2.8381e-03, -2.5513e-01,  1.9214e-01,  1.9824e-01,\n",
       "         1.9116e-01,  1.4392e-01,  1.4636e-01,  2.1204e-01,  2.4219e-01,\n",
       "         6.6772e-02,  2.3120e-01,  2.5781e-01,  2.0837e-01,  2.5000e-01,\n",
       "         1.3550e-01,  6.5857e-02,  1.5405e-01,  2.4353e-01,  1.1438e-01,\n",
       "         1.2634e-01,  1.0529e-01,  1.4380e-01,  2.3962e-01,  2.2205e-01,\n",
       "        -7.1960e-02,  2.8687e-01,  2.3486e-01,  2.0776e-01, -6.5308e-02,\n",
       "         2.3413e-01,  6.3782e-02,  1.9067e-01,  1.9128e-01,  2.3071e-01,\n",
       "         2.0239e-01, -3.2749e-03,  3.6084e-01,  2.6099e-01,  2.0813e-01,\n",
       "         2.5317e-01,  9.7229e-02,  2.0764e-01,  1.6992e-01,  1.1176e-01,\n",
       "         1.7249e-01,  3.6743e-01,  1.3977e-01,  2.0984e-01,  1.4392e-01,\n",
       "         2.7417e-01,  7.0374e-02, -2.1899e-01,  2.3425e-01,  2.0251e-01,\n",
       "         8.6670e-02,  2.4683e-01,  1.7651e-01,  1.2352e-02,  1.8591e-01,\n",
       "         2.7490e-01,  1.9995e-01,  1.6797e-01,  1.9287e-01,  6.0463e-03,\n",
       "         2.7808e-01,  1.5088e-01,  4.4441e-03,  1.1945e-01,  2.0520e-01,\n",
       "         4.3915e-02, -1.4722e-01,  1.9653e-01,  1.9690e-01,  2.0911e-01,\n",
       "         2.5131e-02,  2.1912e-01,  1.9092e-01,  2.6709e-01,  1.9409e-01,\n",
       "         2.4841e-01, -1.9324e-01,  2.4866e-01,  2.1045e-01,  1.6846e-01,\n",
       "         2.4109e-01,  1.5515e-01, -7.0984e-02,  2.0947e-01, -1.7151e-01,\n",
       "         1.4893e-01,  2.0886e-01,  2.5879e-01,  1.5442e-01, -1.4014e-01,\n",
       "         2.8760e-01,  7.4341e-02,  2.0178e-01,  2.3584e-01,  2.6099e-01,\n",
       "        -1.9196e-02,  1.9006e-01,  2.0984e-01,  1.8896e-01,  7.2937e-02,\n",
       "        -3.5858e-02,  2.5684e-01, -7.4707e-02,  1.4551e-01,  1.8103e-01,\n",
       "         4.0466e-02,  1.0565e-01,  3.1372e-01,  1.3940e-01,  1.4233e-01,\n",
       "         2.1777e-01,  1.9629e-01,  1.4539e-01,  2.5659e-01,  1.6211e-01,\n",
       "         2.0654e-01,  2.0007e-01, -6.9031e-02,  1.9409e-01,  1.2610e-01,\n",
       "        -5.0293e-02,  1.3501e-01,  3.1372e-01,  9.4788e-02,  2.1057e-01,\n",
       "         1.9775e-01,  2.1802e-01,  8.8928e-02,  1.9495e-01,  1.1307e-02,\n",
       "         1.4636e-01,  1.8262e-01,  2.7930e-01,  1.8030e-01,  1.6418e-01,\n",
       "         2.0581e-01,  1.3745e-01, -3.1952e-02,  1.5405e-01,  5.2100e-01,\n",
       "         2.4976e-01,  2.6416e-01,  1.2189e-01,  2.2961e-01,  6.1768e-02,\n",
       "         3.0200e-01,  1.6919e-01,  1.2115e-01, -1.3831e-01,  2.1973e-01,\n",
       "         2.0520e-01,  1.1707e-01,  2.4390e-01,  2.1191e-01,  1.1505e-01,\n",
       "         2.1948e-01,  8.0109e-03, -1.8884e-01,  1.8225e-01,  2.1484e-01,\n",
       "         1.0712e-01,  1.7432e-01,  1.5430e-01, -5.6725e-03,  3.0869e-02,\n",
       "         1.4526e-01, -4.2334e-01,  1.1835e-01,  2.0068e-01,  2.0276e-01,\n",
       "         1.4331e-01,  3.0640e-01,  9.7534e-02,  2.3779e-01, -6.9214e-02,\n",
       "         3.2178e-01,  1.3708e-01,  1.5430e-01,  2.2034e-01,  1.9946e-01,\n",
       "         6.1523e-02,  1.3245e-01,  1.9727e-01,  2.7441e-01,  2.7075e-01,\n",
       "         2.1133e-02,  1.5820e-01,  1.7542e-01,  2.7173e-01,  3.1445e-01,\n",
       "        -3.4973e-02,  1.5723e-01,  6.9214e-02,  1.2158e-01,  2.3657e-01,\n",
       "         2.9883e-01,  2.5781e-01,  2.0642e-01,  2.1948e-01,  1.3464e-01,\n",
       "         2.7734e-01,  3.1787e-01,  2.9761e-01,  1.4539e-01,  1.2445e-01,\n",
       "         1.7566e-01,  1.8518e-01,  2.1094e-01,  4.0576e-01,  4.5410e-02,\n",
       "         4.4189e-01,  2.2083e-01,  2.9663e-01,  3.9795e-02,  3.3325e-01,\n",
       "         2.3425e-01,  1.2036e-01,  1.3818e-01,  2.7246e-01,  2.1985e-01,\n",
       "         2.1716e-01,  1.2268e-01,  1.0394e-01,  2.8247e-01, -4.0474e-03,\n",
       "         2.9370e-01,  1.6248e-01,  1.4514e-01,  4.7424e-02,  1.4429e-01,\n",
       "         2.0593e-01,  1.5271e-01,  1.1798e-01,  2.5513e-01,  1.0333e-01,\n",
       "         3.2544e-01, -2.1423e-01,  1.0223e-01,  2.0435e-01,  3.5156e-02,\n",
       "         7.7393e-02, -6.2805e-02,  1.6846e-01,  2.0251e-01,  1.7480e-01,\n",
       "         2.0850e-01,  2.7734e-01,  1.6528e-01,  3.7445e-02,  2.5024e-01,\n",
       "         1.4966e-01,  1.2207e-01,  2.8223e-01,  1.6443e-01,  1.6699e-01,\n",
       "         6.4468e-03,  2.6660e-01,  2.9883e-01,  2.5659e-01,  1.2817e-01,\n",
       "         3.2007e-01,  1.8909e-01,  1.5308e-01,  1.8481e-01,  7.5439e-02,\n",
       "         1.7261e-01,  1.5869e-01,  1.0559e-01, -2.8229e-02,  5.8136e-03,\n",
       "        -2.4854e-01,  1.2024e-01,  2.0032e-01,  9.9915e-02,  1.6821e-01,\n",
       "         2.2229e-01,  2.4231e-01, -1.4807e-01,  1.0516e-01,  3.4404e-04,\n",
       "        -2.9053e-01,  1.0590e-02,  1.4990e-01,  2.0569e-01,  1.7896e-01,\n",
       "         3.6011e-01,  2.9272e-01, -1.3281e-01,  2.3718e-01,  1.5576e-01,\n",
       "        -6.9214e-02,  9.6436e-02, -1.9385e-01,  1.5564e-01,  2.6733e-01,\n",
       "         3.3545e-01,  2.8198e-01,  2.8662e-01,  1.6052e-01,  4.3427e-02,\n",
       "         8.0078e-02,  2.2839e-01,  1.2262e-01,  1.3879e-01,  9.9976e-02,\n",
       "         1.6614e-01, -9.6008e-02,  2.0691e-01,  1.9116e-01,  2.2314e-01,\n",
       "         1.6577e-01,  1.9653e-01,  5.5542e-03,  2.4426e-01,  1.3782e-01,\n",
       "         7.8369e-02, -8.2932e-03,  3.4497e-01,  8.5571e-02,  1.0144e-01,\n",
       "         1.3329e-02,  1.7786e-01,  2.7515e-01,  2.2559e-01,  3.6353e-01,\n",
       "         1.5222e-01,  1.3611e-01,  2.1313e-01, -2.0618e-01,  2.4646e-01,\n",
       "         1.8066e-01,  1.7871e-01], device='cuda:0'), tensor([ 2.0078e+00,  7.6485e-03,  9.4434e-01,  1.0400e+00,  1.0879e+00,\n",
       "         6.8604e-01,  7.9346e-01,  1.3242e+00,  8.1445e-01,  6.7676e-01,\n",
       "         1.2988e+00,  9.0430e-01,  8.0762e-01,  7.0996e-01,  9.3799e-01,\n",
       "         1.2881e+00,  9.8926e-01,  1.2637e+00,  1.1250e+00,  7.4609e-01,\n",
       "         6.0352e-01,  1.3525e+00,  2.1035e+00,  9.8096e-01,  7.5195e-01,\n",
       "         6.2012e-01,  7.0381e-03,  2.0256e-03,  6.4404e-01,  9.5557e-01,\n",
       "         1.3828e+00,  1.3475e-03,  5.7666e-01,  5.1074e-01,  5.0635e-01,\n",
       "         1.9502e+00,  6.2598e-01,  1.1221e+00,  9.3848e-01,  6.8359e-01,\n",
       "         8.6823e-03,  8.8562e-02,  5.7910e-01,  8.1711e-03,  7.5830e-01,\n",
       "         2.6270e+00,  9.4482e-01,  7.6123e-01,  6.8213e-01,  5.4199e-01,\n",
       "         1.2852e+00,  7.5635e-01,  6.8799e-01,  4.2554e-01,  5.6006e-01,\n",
       "         6.7383e-01,  6.3281e-01,  6.7261e-02,  1.2119e+00,  1.0107e+00,\n",
       "         8.1201e-01,  7.0801e-01,  7.9102e-01,  9.2773e-02,  9.6973e-01,\n",
       "         7.1826e-01,  9.6289e-01,  9.6094e-01,  8.1238e-02,  1.0244e+00,\n",
       "         1.1045e+00,  2.5225e-04,  6.2598e-01,  8.5791e-01,  5.2032e-02,\n",
       "         9.4287e-01,  5.1611e-01,  1.4287e+00,  9.4238e-01,  1.3711e+00,\n",
       "         5.8301e-01,  2.6309e+00,  9.2041e-01,  3.4981e-03,  6.7920e-01,\n",
       "         6.4502e-01,  1.1895e+00,  4.0991e-01,  4.3506e-01,  9.9564e-04,\n",
       "         1.0615e+00,  9.3506e-01,  6.5283e-01,  1.3994e+00,  6.7139e-01,\n",
       "         7.0557e-01,  6.6846e-01,  7.7588e-01,  1.2090e+00,  9.4092e-01,\n",
       "         1.2002e+00,  7.7246e-01,  9.8047e-01,  1.1357e+00,  1.0414e-02,\n",
       "         6.4160e-01,  7.4072e-01,  1.0371e+00,  7.3047e-01,  4.3457e-01,\n",
       "         1.3047e+00,  6.1865e-01,  1.4229e+00,  5.1123e-01,  4.2700e-01,\n",
       "         1.4612e-01,  5.6396e-01,  9.4629e-01,  2.3770e+00,  2.4883e+00,\n",
       "         4.6045e-01,  1.0186e+00,  8.1787e-01,  8.8086e-01,  8.6865e-01,\n",
       "         8.6084e-01,  5.7764e-01,  4.3457e-01,  1.0431e-01,  1.1260e+00,\n",
       "         7.5244e-01,  8.9600e-01,  9.6045e-01,  9.9951e-01,  1.0272e-01,\n",
       "         4.9683e-01,  1.0400e+00,  5.7568e-01,  9.7266e-01,  7.5293e-01,\n",
       "         7.5342e-01,  1.9658e+00,  3.0322e-01,  6.4160e-01,  1.6621e+00,\n",
       "         6.4941e-01,  8.6523e-01,  4.4629e-01,  7.1680e-01,  6.5625e-01,\n",
       "         1.5195e+00,  1.1835e-01,  9.1016e-01,  7.2656e-01,  1.4395e+00,\n",
       "         1.0283e+00,  9.6289e-01,  8.7402e-01,  7.5195e-01,  8.4814e-01,\n",
       "         7.9443e-01,  5.3558e-03,  2.5234e+00,  1.9153e-01,  6.6064e-01,\n",
       "         4.4653e-01,  5.9326e-01,  1.2588e+00,  7.3340e-01,  4.8169e-01,\n",
       "         1.9551e+00,  4.7363e-02,  3.6182e-01,  7.6660e-01,  6.8164e-01,\n",
       "         1.4893e-01,  8.7939e-01,  4.5239e-01,  1.7538e-03,  1.1123e+00,\n",
       "         7.0654e-01,  5.8105e-01,  9.0039e-01,  1.9727e-01,  5.9668e-01,\n",
       "         2.3572e-01,  7.9785e-01,  1.7188e+00,  7.2607e-01,  8.4277e-01,\n",
       "         1.1787e-02,  4.4482e-01,  8.8525e-01,  8.4814e-01,  9.0332e-02,\n",
       "         1.8591e-01,  5.9937e-02,  6.9153e-02,  1.2969e+00,  5.4199e-01,\n",
       "         1.0508e+00,  1.1543e+00,  8.4619e-01,  5.2979e-01,  6.9336e-01,\n",
       "         5.8447e-01,  1.0312e+00,  6.4795e-01,  8.8501e-02,  8.0078e-01,\n",
       "         5.5664e-01,  7.6318e-01,  1.1533e+00,  9.0551e-04,  8.9844e-01,\n",
       "         5.6494e-01,  6.7529e-01,  7.1143e-01,  3.3752e-02,  1.1885e+00,\n",
       "         5.7471e-01,  1.0615e+00,  4.4946e-01,  1.2158e+00,  7.6709e-01,\n",
       "         5.0391e-01,  1.3145e+00,  5.1416e-01,  5.5762e-01,  1.4639e+00,\n",
       "         8.0664e-01,  2.5508e+00,  8.3936e-01,  3.4155e-01,  6.5918e-01,\n",
       "         7.8320e-01,  8.2471e-01,  8.3350e-01,  1.2510e+00,  5.5078e-01,\n",
       "         4.4434e-01,  7.5684e-01,  8.0762e-01,  1.7734e+00,  9.4678e-01,\n",
       "         4.6313e-01,  9.6680e-01,  6.4453e-01,  7.8857e-02,  9.9121e-01,\n",
       "         7.5049e-01,  1.1045e+00,  6.0107e-01,  5.5322e-01,  6.8506e-01,\n",
       "         6.3818e-01,  4.0527e-01,  5.7568e-01,  7.5342e-01,  1.0297e-01,\n",
       "         8.2825e-02,  1.3203e+00,  9.8340e-01,  7.2656e-01,  8.3008e-01,\n",
       "         3.9209e-01,  7.8003e-02,  8.9258e-01,  7.9004e-01,  1.1934e+00,\n",
       "         1.5732e+00,  5.5908e-01,  7.0850e-01,  5.9814e-01,  2.0187e-02,\n",
       "         8.7939e-01,  8.9258e-01,  8.5498e-01,  6.9043e-01,  5.1660e-01,\n",
       "         9.7656e-01,  8.6328e-01,  4.2749e-01,  4.9756e-01,  1.1426e+00,\n",
       "         1.0596e+00,  7.0215e-01,  9.8022e-02,  7.2998e-01,  6.8311e-01,\n",
       "         5.9277e-01,  9.7266e-01,  4.5703e-01,  7.0508e-01,  2.5273e+00,\n",
       "         7.4902e-01,  6.3379e-01,  1.8594e+00,  6.3379e-01,  1.3652e+00,\n",
       "         1.0674e+00,  6.4600e-01,  2.7661e-01,  7.3059e-02,  6.5430e-01,\n",
       "         5.3711e-01,  8.4961e-01,  1.1953e+00,  6.2646e-01,  1.2861e+00,\n",
       "         8.8525e-01,  8.7842e-01,  4.0503e-01,  1.0248e-01,  6.5869e-01,\n",
       "         5.5322e-01,  9.1992e-01,  1.4951e+00,  1.0898e+00,  1.3701e+00,\n",
       "         9.2383e-01,  8.9404e-01,  4.0137e-01,  5.9375e-01,  5.0391e-01,\n",
       "         1.1006e+00,  8.3057e-01,  1.3857e+00,  4.4238e-01,  7.4829e-02,\n",
       "         9.3018e-01,  1.1660e+00,  7.1240e-01,  1.2373e+00,  3.7598e-02,\n",
       "         7.6611e-01,  1.0605e+00,  1.6455e+00,  5.5420e-01,  6.3037e-01,\n",
       "         6.1279e-01,  7.6904e-01,  1.2549e-01,  9.6973e-01,  3.9819e-01,\n",
       "         1.1006e+00,  5.3223e-01,  5.0635e-01,  7.3096e-01,  2.5425e-03,\n",
       "         2.0984e-01,  7.2168e-01,  9.3628e-02,  1.8994e+00,  6.5674e-01,\n",
       "         1.2793e+00,  1.3586e-01,  7.6953e-01,  2.2681e-01,  6.7725e-01,\n",
       "         7.8027e-01,  1.8044e-03,  7.9395e-01,  7.8662e-01, -1.9121e-03,\n",
       "         8.7012e-01,  9.2236e-01,  7.7539e-01,  7.5623e-02,  5.7715e-01,\n",
       "         6.5283e-01,  6.8701e-01,  4.2432e-01,  1.7114e-01,  9.0430e-01,\n",
       "         1.0498e+00,  7.4756e-01,  5.4834e-01,  1.2236e+00,  9.0625e-01,\n",
       "         4.6501e-03,  8.3643e-01,  6.6699e-01,  4.1211e-01,  7.4023e-01,\n",
       "         8.2471e-01,  6.8213e-01,  7.0654e-01,  5.1165e-04,  7.8418e-01,\n",
       "         1.4121e+00,  5.3802e-02,  6.1963e-01,  1.4600e+00,  5.2100e-01,\n",
       "         6.3232e-01,  4.4775e-01,  8.3691e-01,  6.6504e-01,  8.6328e-01,\n",
       "         4.8364e-01,  7.5732e-01,  1.0342e+00,  1.0625e+00,  1.0537e+00,\n",
       "         1.2344e+00,  3.0396e-01,  4.6704e-01,  6.9531e-01,  6.7578e-01,\n",
       "         1.1172e+00,  9.0869e-01,  7.3193e-01,  6.7480e-01,  1.0272e-01,\n",
       "         5.7422e-01,  8.8281e-01,  7.3535e-01,  1.0859e+00,  2.1924e-01,\n",
       "         1.1602e+00,  1.0322e+00,  1.1871e-01,  5.2765e-02,  6.8213e-01,\n",
       "         9.2627e-01,  4.4263e-01,  7.9053e-01,  1.0391e+00,  2.1836e+00,\n",
       "         4.1089e-01,  3.2153e-01,  7.6233e-02,  1.0889e-01,  9.0576e-01,\n",
       "         1.4150e+00,  7.6514e-01,  1.0342e+00,  1.1064e+00,  1.6738e+00,\n",
       "         1.0537e+00,  1.1133e+00,  2.7173e-01,  2.5269e-01,  1.1445e+00,\n",
       "         7.6514e-01,  1.6284e-01,  6.7932e-02,  3.7378e-01,  1.1639e-01,\n",
       "         1.6973e+00,  1.5198e-01,  6.4014e-01,  4.9219e-01,  7.3779e-01,\n",
       "         7.5488e-01,  6.9189e-01,  4.8779e-01,  6.1188e-02,  6.5479e-01,\n",
       "         4.4067e-01,  9.4775e-01,  5.0684e-01,  4.5967e-04,  2.9712e-01,\n",
       "         9.5215e-01,  6.1719e-01,  4.2896e-01,  2.3474e-01,  5.9180e-01,\n",
       "         1.0762e+00,  9.5459e-01,  8.6328e-01,  4.7583e-01,  7.4365e-01,\n",
       "         8.3057e-01,  5.6152e-01,  1.1475e+00,  7.9688e-01,  4.5654e-01,\n",
       "         6.9775e-01,  6.4795e-01,  7.8906e-01,  7.2876e-02,  8.1445e-01,\n",
       "         8.8232e-01,  9.2480e-01,  5.7861e-01,  6.5576e-01,  1.2539e+00,\n",
       "         7.6611e-01,  2.2119e-01,  8.7207e-01,  2.2109e+00,  6.1035e-01,\n",
       "         5.1514e-01,  7.5977e-01,  4.6753e-01,  6.2561e-02,  7.8174e-01,\n",
       "         5.6738e-01,  6.4258e-01,  1.4512e+00,  9.2334e-01,  1.1895e+00,\n",
       "         5.2832e-01,  7.8418e-01, -1.3313e-03,  1.7988e+00,  8.1592e-01,\n",
       "         1.1309e+00,  9.2285e-01], device='cuda:0'), tensor([-2.7637e-01, -4.8637e-03, -4.6021e-01, -6.6797e-01, -6.6650e-01,\n",
       "        -6.9380e-04,  1.3354e-01, -3.5669e-01, -3.4058e-01, -3.0200e-01,\n",
       "         6.4026e-02, -1.0828e-01, -2.4194e-01, -3.2056e-01, -3.3228e-01,\n",
       "        -5.0621e-03, -6.9824e-01, -3.0518e-01, -1.4209e-01, -2.1875e-01,\n",
       "        -3.9575e-01, -8.3203e-01, -2.6367e-01, -7.3926e-01, -3.4943e-02,\n",
       "        -5.2887e-02, -9.2773e-03, -2.1515e-03, -4.1284e-01, -4.1748e-01,\n",
       "        -3.9746e-01, -1.7357e-04, -3.2227e-01, -2.1851e-01,  1.3379e-01,\n",
       "        -1.0269e-02, -8.3801e-02, -8.7207e-01, -3.6719e-01, -1.7188e-01,\n",
       "        -1.0109e-02, -9.2346e-02, -4.8438e-01, -7.8354e-03,  3.5950e-02,\n",
       "        -2.4036e-01, -6.9287e-01, -4.0161e-01, -2.0496e-01,  2.0885e-03,\n",
       "        -2.7051e-01, -1.7676e-01, -2.2913e-01,  2.1774e-02, -3.6530e-02,\n",
       "        -5.0354e-02, -5.8556e-03, -5.6549e-02, -1.9812e-01, -4.3335e-01,\n",
       "        -1.2622e-01, -1.8896e-01, -3.4106e-01, -8.0627e-02,  6.5735e-02,\n",
       "        -6.6711e-02, -4.4458e-01, -4.1699e-01, -7.6233e-02,  7.4158e-02,\n",
       "        -1.6736e-01, -4.6158e-04, -4.8120e-01, -1.6248e-01, -4.7180e-02,\n",
       "        -2.7173e-01, -3.8428e-01, -2.3633e-01, -2.3718e-01,  1.8567e-01,\n",
       "        -5.1367e-01, -1.0187e-01,  1.0059e-01, -3.1433e-03, -3.4521e-01,\n",
       "        -3.2812e-01, -4.6509e-01, -1.2878e-02,  6.1066e-02, -6.6137e-04,\n",
       "        -3.9014e-01,  1.1304e-01, -1.5442e-01,  3.2074e-02, -2.8271e-01,\n",
       "        -1.7188e-01, -3.6768e-01,  4.4830e-02, -2.7661e-01,  1.2372e-01,\n",
       "         8.3740e-02, -1.9104e-01, -5.2148e-01, -1.6040e-01, -8.6212e-03,\n",
       "        -4.5703e-01, -2.6196e-01, -1.3062e-01, -1.7175e-01, -2.0966e-02,\n",
       "        -6.3184e-01, -6.0205e-01, -4.8193e-01, -3.0151e-01, -1.0205e-01,\n",
       "        -1.2646e-01, -6.1760e-03,  6.0760e-02, -2.3083e-01, -6.6605e-03,\n",
       "        -3.7134e-01, -8.0029e-01, -2.4854e-01, -1.3672e-01, -6.6406e-02,\n",
       "        -1.4917e-01, -1.7102e-01, -3.1641e-01, -1.0028e-01, -4.8755e-01,\n",
       "        -3.2959e-02,  7.4280e-02, -8.3740e-01, -4.3018e-01, -9.2590e-02,\n",
       "        -7.9712e-02, -1.0535e-01, -2.7661e-01, -4.6356e-02, -2.4463e-01,\n",
       "         3.3264e-02, -1.2964e-01, -2.6538e-01, -4.3140e-01, -7.6855e-01,\n",
       "         3.4912e-02, -7.9224e-02, -5.7739e-02, -2.7148e-01, -2.7466e-01,\n",
       "        -4.8492e-02, -1.1829e-01, -5.8936e-01, -2.6367e-01, -2.3889e-01,\n",
       "        -7.6660e-01,  1.1371e-01, -5.8643e-01, -2.0667e-01, -6.7090e-01,\n",
       "        -3.2324e-01, -5.5885e-03, -2.3108e-01, -1.6357e-01, -3.8232e-01,\n",
       "        -7.9590e-02, -1.9897e-01, -7.0435e-02, -6.0693e-01,  1.0791e-01,\n",
       "        -2.8735e-01, -4.6265e-02, -2.1558e-01, -2.6147e-01,  2.1229e-03,\n",
       "        -1.2585e-01, -3.4473e-01, -7.9102e-02, -2.0332e-03, -4.7852e-01,\n",
       "        -4.2627e-01, -3.3008e-01, -7.5195e-02, -1.4453e-01, -4.2554e-01,\n",
       "        -1.8933e-01, -3.9526e-01, -2.1774e-02, -4.5337e-01, -1.8616e-01,\n",
       "        -1.1948e-02, -1.2073e-01, -4.0967e-01, -3.1030e-01, -9.6375e-02,\n",
       "        -1.3599e-01, -6.4148e-02, -6.5430e-02, -4.9487e-01, -2.9614e-01,\n",
       "         1.7407e-01, -1.3623e-01, -3.6060e-01, -1.0034e-01, -5.5351e-03,\n",
       "        -4.3872e-01,  2.0721e-02, -2.0081e-01, -9.3323e-02, -3.4985e-01,\n",
       "        -1.5344e-01, -2.5586e-01, -4.3262e-01, -2.3823e-03, -2.1826e-01,\n",
       "        -4.5801e-01, -6.0883e-02, -3.4961e-01, -3.1372e-02, -8.1665e-02,\n",
       "        -1.7725e-01, -5.3271e-01, -1.5088e-01, -1.9861e-01, -8.2764e-02,\n",
       "        -4.1016e-01, -4.3896e-01,  2.3926e-02, -4.0332e-01, -7.6562e-01,\n",
       "        -9.2957e-02, -2.1228e-01, -4.0039e-01, -2.5620e-02, -2.2327e-01,\n",
       "         2.6703e-03, -4.1290e-02, -9.2957e-02, -2.5732e-01, -4.5868e-02,\n",
       "        -7.4158e-02, -2.9614e-01, -3.9380e-01, -2.4207e-01, -4.0039e-01,\n",
       "        -3.6377e-02, -3.5474e-01, -3.5425e-01, -7.4707e-02,  1.2268e-02,\n",
       "        -2.6953e-01, -1.7334e-01, -4.2969e-01, -3.6426e-01, -2.5299e-02,\n",
       "        -2.8564e-01,  4.0192e-02, -5.0781e-01, -3.5767e-01, -1.0547e-01,\n",
       "        -7.1838e-02, -1.3464e-01, -1.1871e-01, -1.6516e-01, -3.4521e-01,\n",
       "        -1.2781e-01, -7.7026e-02, -1.2830e-01, -2.3547e-01, -1.1438e-01,\n",
       "         2.7954e-01, -5.1697e-02, -1.9238e-01, -4.4800e-01, -1.6769e-02,\n",
       "        -1.3550e-01,  2.8763e-02, -2.2620e-01, -3.9697e-01, -1.1731e-01,\n",
       "        -6.2109e-01, -6.5381e-01,  7.7438e-03, -1.4111e-01, -2.0020e-01,\n",
       "        -7.1143e-01, -1.0809e-01, -8.1055e-02, -4.2236e-01, -2.5952e-01,\n",
       "        -4.8169e-01, -4.7949e-01, -1.9800e-01,  1.4221e-02, -2.5195e-01,\n",
       "        -4.3433e-01, -2.8711e-01,  1.2444e-02, -4.3091e-01, -2.4670e-01,\n",
       "        -5.1855e-01, -6.5857e-02,  1.8433e-02, -7.6355e-02, -2.4963e-01,\n",
       "        -4.0625e-01, -4.4141e-01, -2.3486e-01, -3.5278e-01, -3.5718e-01,\n",
       "        -3.4082e-01, -2.1729e-01, -1.6272e-01, -1.0028e-01, -2.5586e-01,\n",
       "        -3.9062e-01, -4.3945e-01, -1.9873e-01, -7.4316e-01, -4.2456e-01,\n",
       "        -3.4668e-01, -3.9941e-01, -1.6895e-01, -5.0684e-01, -3.2898e-02,\n",
       "        -6.8848e-02,  4.3060e-02, -3.4351e-01,  4.7455e-02, -7.9956e-02,\n",
       "        -5.1465e-01, -2.7808e-01, -4.8218e-01, -8.4375e-01, -4.0131e-02,\n",
       "        -2.5574e-02, -6.5857e-02, -3.7085e-01, -3.0371e-01, -4.1357e-01,\n",
       "        -4.2175e-02, -3.1396e-01, -1.0425e-01, -4.1113e-01,  2.4048e-02,\n",
       "        -2.4487e-01,  1.0840e-01, -8.7402e-02, -3.0347e-01, -7.7581e-04,\n",
       "        -2.0203e-01, -7.0410e-01, -8.5693e-02, -3.3569e-01, -5.7220e-02,\n",
       "        -2.4658e-01, -1.1884e-01, -2.3828e-01, -1.8481e-01, -2.4792e-01,\n",
       "         3.0838e-02, -4.4775e-04, -3.5059e-01, -4.3018e-01,  1.3626e-04,\n",
       "        -1.5015e-01,  1.1145e-01, -2.3889e-01, -6.8054e-02, -4.1943e-01,\n",
       "        -1.5625e-01,  2.0981e-02, -1.3623e-01, -2.0923e-01, -9.7046e-02,\n",
       "        -3.5938e-01, -4.6973e-01, -3.6938e-01, -1.7419e-01, -2.2559e-01,\n",
       "        -5.0735e-03, -4.3555e-01, -3.6230e-01, -1.6895e-01, -2.0752e-01,\n",
       "        -3.8208e-01, -3.2690e-01, -4.3701e-01,  5.6648e-04,  8.8989e-02,\n",
       "        -1.0229e-01, -4.9652e-02, -4.9927e-01, -1.0127e+00, -2.7417e-01,\n",
       "        -3.4326e-01, -2.1045e-01,  1.0248e-01, -5.4639e-01, -5.9668e-01,\n",
       "        -4.6826e-01, -3.1445e-01, -1.4050e-01, -3.0200e-01, -2.7515e-01,\n",
       "        -2.2949e-01, -1.8054e-01, -1.8506e-01,  2.4780e-02, -6.1707e-02,\n",
       "        -1.0098e+00,  7.6172e-02,  4.9896e-02, -5.4932e-01, -9.8206e-02,\n",
       "        -3.9307e-01, -3.0957e-01, -9.9487e-02, -4.3018e-01, -1.7676e-01,\n",
       "        -6.4404e-01, -3.9380e-01, -1.1755e-01, -4.8920e-02, -8.2715e-01,\n",
       "        -1.8396e-01, -1.1237e-01, -2.4121e-01, -1.0925e-01, -2.1069e-01,\n",
       "        -1.7188e-01, -1.2781e-01, -6.6101e-02, -1.0303e-01, -3.2471e-01,\n",
       "         7.2327e-02, -2.9297e-01, -3.9331e-01, -7.7344e-01, -6.7383e-02,\n",
       "        -3.7134e-01, -1.1676e-01, -2.0239e-01, -2.0532e-01, -2.9199e-01,\n",
       "        -3.6572e-01, -1.3367e-01, -6.9519e-02,  1.5869e-02, -1.3110e-01,\n",
       "        -5.1367e-01, -1.3818e-01, -4.5850e-01, -4.0894e-02,  5.8350e-02,\n",
       "        -4.4238e-01, -1.2335e-01, -1.4941e-01, -5.9906e-02, -3.1250e-02,\n",
       "         4.4495e-02,  1.0535e-01, -3.6450e-01, -2.1458e-03, -2.5220e-01,\n",
       "        -2.8760e-01, -4.8608e-01,  2.4796e-02,  9.0515e-02, -4.1260e-01,\n",
       "        -4.2651e-01,  5.9814e-02, -1.4746e-01, -1.2341e-01, -5.6592e-01,\n",
       "        -4.7144e-01, -9.8083e-02, -3.1152e-01, -3.4619e-01, -2.3755e-01,\n",
       "        -3.7476e-02, -3.7646e-01, -2.9175e-01, -6.7566e-02, -8.6212e-03,\n",
       "         7.7271e-02, -2.1643e-01, -4.3579e-01, -3.7231e-01, -3.3838e-01,\n",
       "         6.2561e-02, -1.7883e-01,  3.4149e-02, -2.4280e-01, -1.5149e-01,\n",
       "        -6.6772e-02, -2.9907e-01, -3.2495e-01, -6.9214e-02, -3.0225e-01,\n",
       "        -2.8857e-01, -7.2876e-02, -1.1243e-01, -3.5864e-01, -5.2002e-01,\n",
       "        -5.0171e-02, -3.5327e-01, -2.2469e-03, -4.4043e-01, -5.5267e-02,\n",
       "        -2.0325e-01, -4.3213e-01], device='cuda:0'), tensor([[[ 0.1364,  0.0595,  0.0558],\n",
       "         [-0.0027,  0.0036,  0.0130],\n",
       "         [ 0.0205, -0.1594, -0.1644],\n",
       "         ...,\n",
       "         [-0.0302, -0.1316, -0.3022],\n",
       "         [-0.1302,  0.0339,  0.1055],\n",
       "         [-0.0198,  0.0161, -0.2947]],\n",
       "\n",
       "        [[ 0.0486,  0.0623,  0.0605],\n",
       "         [ 0.0252,  0.0129,  0.0387],\n",
       "         [-0.0129,  0.0412,  0.1558],\n",
       "         ...,\n",
       "         [-0.0335,  0.0049,  0.1519],\n",
       "         [ 0.0413,  0.0309, -0.1652],\n",
       "         [ 0.2773,  0.1691, -0.1676]],\n",
       "\n",
       "        [[ 0.0562,  0.0528,  0.0679],\n",
       "         [ 0.0068, -0.0200,  0.0214],\n",
       "         [-0.2084,  0.0162, -0.0525],\n",
       "         ...,\n",
       "         [-0.0758,  0.0801,  0.1887],\n",
       "         [ 0.3630,  0.2178,  0.0604],\n",
       "         [ 0.0210,  0.2490,  0.1212]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.1433,  0.0085, -0.3591],\n",
       "         [-0.0114,  0.0310,  0.0248],\n",
       "         [ 0.0788,  0.0666,  0.0865],\n",
       "         ...,\n",
       "         [ 0.2230, -0.0247,  0.0292],\n",
       "         [-0.0077, -0.0243,  0.0525],\n",
       "         [ 0.0226, -0.0550,  0.0958]],\n",
       "\n",
       "        [[ 0.0714,  0.0373,  0.0659],\n",
       "         [-0.0265,  0.0334,  0.0310],\n",
       "         [ 0.1245, -0.2297, -0.0685],\n",
       "         ...,\n",
       "         [ 0.2686, -0.2405, -0.3040],\n",
       "         [ 0.0626,  0.0735,  0.0289],\n",
       "         [-0.0309, -0.0014,  0.0367]],\n",
       "\n",
       "        [[ 0.0657,  0.0394,  0.0715],\n",
       "         [ 0.0172,  0.0166,  0.0124],\n",
       "         [-0.0970, -0.1090, -0.0036],\n",
       "         ...,\n",
       "         [-0.1805, -0.0740,  0.1110],\n",
       "         [ 0.2578,  0.2362,  0.1311],\n",
       "         [-0.0439, -0.1266,  0.0463]]], device='cuda:0'), tensor([ 3.4570e-01,  8.5083e-02,  2.1570e-01,  2.8979e-01, -3.1464e-02,\n",
       "         1.9470e-02,  1.6190e-02,  6.3574e-01,  2.1347e-02,  2.4231e-01,\n",
       "         1.3232e-01,  1.1780e-01,  1.5381e-01, -2.2537e-02,  2.5952e-01,\n",
       "         2.1106e-01,  3.3423e-01,  2.9102e-01,  4.6844e-02,  2.4402e-01,\n",
       "         1.0406e-01,  6.7993e-02,  1.7041e-01,  2.4622e-01,  4.4525e-02,\n",
       "         7.9346e-02,  1.5234e-01,  7.7637e-02, -2.0111e-02,  2.0923e-01,\n",
       "         6.9885e-02,  1.5442e-01,  1.8054e-01, -3.5980e-02,  1.5991e-01,\n",
       "        -4.7394e-02, -4.0161e-02,  2.9150e-01,  3.9948e-02,  2.5122e-01,\n",
       "        -9.4666e-02,  2.1118e-01,  7.8979e-02, -1.0370e-01,  4.4830e-02,\n",
       "         4.2676e-01,  8.6060e-02, -2.7985e-02, -7.1655e-02,  2.0105e-01,\n",
       "        -6.3705e-03,  1.2622e-01,  1.1682e-01, -8.5938e-02,  4.7882e-02,\n",
       "         7.7759e-02,  1.3741e-02,  3.6011e-02, -1.0687e-01, -8.4961e-02,\n",
       "         2.5220e-01,  5.5084e-02,  1.4290e-02,  1.0590e-02,  9.3323e-02,\n",
       "         3.9160e-01,  2.1472e-01,  9.9060e-02,  9.9731e-02,  3.6328e-01,\n",
       "        -2.2202e-02,  7.3364e-02,  4.5093e-01,  7.7515e-02,  4.6875e-01,\n",
       "         8.8623e-02,  8.8959e-03,  1.8970e-01,  1.7676e-01, -1.5601e-01,\n",
       "        -3.0197e-02, -2.1103e-02,  8.2825e-02,  4.5502e-02, -1.6117e-03,\n",
       "         1.2952e-01,  2.3389e-01,  1.3403e-01, -1.5820e-01,  7.3181e-02,\n",
       "         9.1614e-02,  2.4988e-01,  4.5850e-01,  2.0349e-01, -3.1067e-02,\n",
       "         6.4758e-02,  3.5181e-01, -8.8196e-02,  2.2571e-01,  1.0852e-01,\n",
       "        -1.2354e-01,  5.8105e-02,  2.5098e-01,  4.8291e-01,  1.4905e-01,\n",
       "         6.9153e-02, -1.8811e-01,  2.6465e-01,  1.1053e-01,  4.7455e-02,\n",
       "        -3.6102e-02,  1.2549e-01,  3.8062e-01,  2.0203e-01,  3.1567e-01,\n",
       "         2.5317e-01,  7.0740e-02,  2.0667e-01, -2.1768e-04, -1.7078e-01,\n",
       "         2.7051e-01,  4.1333e-01,  1.6565e-01,  1.1823e-01,  2.5317e-01,\n",
       "         8.8013e-02, -3.1769e-02,  2.0117e-01,  5.0244e-01,  1.8164e-01,\n",
       "         5.3711e-02, -2.4445e-02, -1.0727e-02,  7.6355e-02,  7.6355e-02,\n",
       "         1.8481e-01,  1.8604e-01,  3.9624e-01, -9.0256e-03,  2.2363e-01,\n",
       "         2.3962e-01,  2.0471e-01,  1.0126e-01,  1.7993e-01, -6.5575e-03,\n",
       "         1.0413e-01, -2.1858e-03,  5.9479e-02,  2.4365e-01,  3.6841e-01,\n",
       "         3.7329e-01,  5.9204e-02,  2.5708e-01,  3.0859e-01,  2.4109e-01,\n",
       "         1.9373e-01,  5.0244e-01,  4.4946e-01,  5.0262e-02, -9.9670e-02,\n",
       "         1.3049e-01,  1.8628e-01, -4.4708e-02,  2.3315e-01,  6.8481e-02,\n",
       "        -3.1250e-02,  8.9478e-02,  2.8955e-01, -9.2346e-02,  1.6144e-02,\n",
       "         2.2705e-01, -2.9419e-02, -6.1615e-02,  1.3257e-01, -1.7075e-02,\n",
       "        -1.9257e-02, -3.8055e-02,  2.6416e-01,  1.0767e-01,  6.7993e-02,\n",
       "         5.1953e-01,  9.1492e-02,  5.0995e-02,  1.0199e-01,  6.0455e-02,\n",
       "        -1.0297e-01,  1.4880e-01,  1.9409e-01,  1.8628e-01,  1.5479e-01,\n",
       "         1.3649e-02,  2.8223e-01,  1.7114e-01,  2.3206e-01,  1.2976e-01,\n",
       "         5.0995e-02, -1.3074e-01,  1.9641e-01,  9.8572e-02, -3.2990e-02,\n",
       "         9.9426e-02,  6.1310e-02,  7.6180e-03,  3.4229e-01, -1.4679e-02,\n",
       "         1.6565e-01,  1.0608e-01,  1.0663e-01,  2.9395e-01,  1.9006e-01,\n",
       "         2.9395e-01,  3.2104e-01,  1.6565e-01,  1.6113e-01,  1.5894e-01,\n",
       "        -1.9608e-02,  3.4937e-01,  3.2715e-02,  3.8159e-01,  4.4849e-01,\n",
       "         2.6294e-01,  5.2414e-03,  6.9824e-02,  4.9591e-02,  4.8340e-02,\n",
       "         4.9744e-02,  6.5063e-02,  2.9221e-02,  3.2056e-01,  2.1411e-01,\n",
       "         2.6733e-01,  1.5918e-01,  1.9275e-01,  4.4312e-01,  4.5190e-01,\n",
       "         2.1875e-01,  5.0537e-01,  1.7188e-01,  3.7573e-01,  2.2363e-01,\n",
       "         1.0254e-02,  4.0955e-02, -4.6814e-02,  2.1387e-01, -2.0416e-02,\n",
       "         1.8420e-01, -8.7158e-02,  2.9053e-02,  2.3987e-01,  1.4636e-01,\n",
       "         2.1082e-01,  2.3438e-01,  1.3318e-01,  8.1482e-02,  1.0406e-01,\n",
       "         3.0615e-01,  8.2214e-02,  2.5854e-01,  6.3538e-02,  5.1575e-03,\n",
       "        -3.1494e-02,  2.2766e-01, -1.8555e-02,  2.2522e-01,  1.1316e-01,\n",
       "         1.5039e-01,  1.5137e-01,  2.3145e-01, -1.9252e-04,  7.7881e-02,\n",
       "         5.6824e-02,  2.6440e-01,  5.2881e-01,  3.3984e-01,  9.6436e-02,\n",
       "        -3.7140e-02,  2.2998e-01,  3.1738e-01,  2.6343e-01,  2.1277e-01,\n",
       "        -6.9824e-02,  1.5210e-01,  1.4941e-01,  2.0825e-01,  3.6133e-01,\n",
       "         9.8267e-02,  3.6938e-01,  3.3691e-02,  2.8369e-01,  2.9028e-01,\n",
       "        -9.8816e-02,  1.4160e-02,  1.8860e-02,  1.1279e-01,  1.3184e-01,\n",
       "         1.3489e-02,  2.1448e-01,  1.0353e-02,  1.5308e-01,  3.7354e-01,\n",
       "         1.8091e-01,  1.5295e-01,  1.9189e-01,  2.6050e-01, -1.1725e-01,\n",
       "         1.6748e-01,  3.2178e-01,  1.9543e-01,  5.6885e-02,  1.8384e-01,\n",
       "        -1.2012e-01,  1.3623e-01,  2.5195e-01,  1.4490e-01,  1.6504e-01,\n",
       "         2.3816e-01,  2.3486e-01, -5.3375e-02,  2.3462e-01,  2.8101e-01,\n",
       "         2.2141e-02,  1.9263e-01,  2.6840e-02,  6.0150e-02,  3.3447e-01,\n",
       "         9.4360e-02,  6.7078e-02,  1.7480e-01,  2.4060e-01,  4.9622e-02,\n",
       "         2.3560e-01,  5.5481e-02,  4.0527e-02, -3.9124e-02,  3.5919e-02,\n",
       "        -6.3965e-02, -2.5928e-01, -1.4551e-01,  2.6831e-01, -4.4495e-02,\n",
       "         1.9666e-01, -6.3416e-02, -4.7119e-02,  2.1509e-01,  1.1029e-01,\n",
       "         2.1631e-01,  6.7139e-02,  2.0218e-02, -3.0945e-02,  4.4824e-01,\n",
       "        -1.8921e-02,  5.7487e-03,  1.3696e-01,  2.3169e-01,  2.6465e-01,\n",
       "         3.9185e-02,  4.0137e-01,  7.9163e-02, -2.7756e-02,  4.1687e-02,\n",
       "         3.1519e-01,  5.7404e-02,  9.0698e-02,  1.0516e-01, -6.8115e-02,\n",
       "         2.3792e-01, -3.3783e-02, -1.6384e-03,  1.1823e-01,  1.9287e-01,\n",
       "         1.5881e-01, -3.7018e-02,  3.5286e-03, -1.4061e-02,  3.3276e-01,\n",
       "         1.9250e-01, -2.5055e-02,  4.4312e-02,  4.7150e-02,  8.5510e-02,\n",
       "         5.7953e-02, -7.5928e-02,  1.4099e-01,  1.1835e-01,  2.4536e-01,\n",
       "         4.4263e-01,  2.4866e-01,  3.2935e-01, -3.0396e-02,  8.8867e-02,\n",
       "        -1.0345e-01, -1.3596e-02,  1.2720e-01,  3.2837e-01,  3.1934e-01,\n",
       "         1.5747e-01,  4.0649e-01,  1.0706e-01,  1.6760e-01,  4.6655e-01,\n",
       "        -6.8970e-02, -2.5589e-02, -3.3661e-02,  3.9233e-01,  4.3213e-01,\n",
       "         1.2854e-01,  3.3789e-01,  1.7847e-01, -1.6434e-02,  9.8999e-02,\n",
       "         4.0710e-02,  1.4417e-01,  4.5227e-02,  2.7539e-01, -8.6853e-02,\n",
       "         2.9932e-01,  1.9263e-01,  1.5900e-02,  4.9469e-02,  1.2329e-01,\n",
       "         2.4158e-01, -1.7914e-02, -2.0703e-01,  9.7412e-02,  1.4844e-01,\n",
       "         2.1875e-01, -1.9617e-01, -4.6936e-02,  1.2189e-01,  5.5029e-01,\n",
       "         3.5181e-01,  9.1492e-02,  5.5115e-02,  1.2457e-01,  1.0602e-01,\n",
       "         4.2908e-02,  2.0117e-01, -1.5762e-02,  2.0544e-01,  1.0785e-01,\n",
       "         2.7026e-01,  1.4783e-01, -1.2413e-02,  1.9702e-01,  2.0337e-01,\n",
       "        -8.4229e-02, -5.1422e-02,  4.5654e-01,  1.2891e-01, -5.6686e-03,\n",
       "         2.0349e-01, -1.0658e-02,  4.0918e-01,  1.4246e-01,  7.5562e-02,\n",
       "        -8.2275e-02,  1.6800e-02,  3.9697e-01, -2.6886e-02,  2.0654e-01,\n",
       "         2.8345e-01,  1.5674e-01,  7.6355e-02,  1.6382e-01,  2.3328e-01,\n",
       "         3.3417e-02, -3.6194e-02, -3.2990e-02,  1.9055e-01,  1.3542e-02,\n",
       "         1.3281e-01,  3.8159e-01,  2.8979e-01,  2.0642e-01,  4.7668e-02,\n",
       "         1.8262e-01,  1.6809e-01, -3.2623e-02,  1.0516e-01,  3.2013e-02,\n",
       "         1.6101e-01, -7.5378e-02,  2.9395e-01,  2.9590e-01,  2.2742e-01,\n",
       "         2.1338e-01,  1.3745e-01,  3.3301e-01, -3.7689e-02,  1.5637e-01,\n",
       "        -2.6875e-03,  7.3792e-02,  2.5684e-01,  1.3757e-01,  3.6035e-01,\n",
       "         8.7952e-02,  2.6392e-01,  5.4492e-01,  2.0349e-01, -2.8900e-02,\n",
       "         2.2046e-01,  1.9516e-02, -1.0724e-01,  1.3025e-01,  1.0138e-01,\n",
       "         3.3105e-01, -7.3303e-02,  1.9299e-01,  1.5259e-01,  1.3147e-01,\n",
       "         4.9622e-02,  8.6670e-03], device='cuda:0'), tensor([ 4.8193e-01,  8.4131e-01,  9.0967e-01,  6.5967e-01,  8.3789e-01,\n",
       "         8.5986e-01,  8.3398e-01,  8.3923e-02,  1.1318e+00,  4.4165e-01,\n",
       "         7.4316e-01,  1.1221e+00,  5.7031e-01,  1.2305e+00,  4.4214e-01,\n",
       "         1.0059e+00,  4.0405e-01,  9.0039e-01,  8.5840e-01,  8.5205e-01,\n",
       "         8.1250e-01,  1.1641e+00,  6.4551e-01,  1.0381e+00,  1.1748e+00,\n",
       "         9.3750e-01,  8.9160e-01,  7.5000e-01,  1.2598e+00,  2.9297e-01,\n",
       "         1.3486e+00,  1.3408e+00,  6.9385e-01,  6.0059e-01,  6.8506e-01,\n",
       "         1.2109e+00,  1.0742e+00,  4.2871e-01,  6.6260e-01,  5.8887e-01,\n",
       "         5.5811e-01,  2.5732e-01,  8.7207e-01,  6.0254e-01,  9.5166e-01,\n",
       "         5.8350e-01,  9.5068e-01,  1.1914e+00,  9.4678e-01,  5.4590e-01,\n",
       "         2.0176e+00,  1.2812e+00,  7.3584e-01,  7.9639e-01,  6.2207e-01,\n",
       "         8.4277e-01,  4.4482e-01,  7.0996e-01,  5.1611e-01,  7.2266e-01,\n",
       "         6.1621e-01,  4.3750e-01,  7.9248e-01,  1.1982e+00,  1.0830e+00,\n",
       "         4.2480e-01,  5.4395e-01,  7.6904e-01,  7.1387e-01,  4.0723e-01,\n",
       "         9.5898e-01,  1.1104e+00,  3.6670e-01,  6.5088e-01,  2.8320e-01,\n",
       "         8.2422e-01,  9.6387e-01,  7.9297e-01,  4.6558e-01,  1.4102e+00,\n",
       "         1.6416e+00,  9.0918e-01,  7.5635e-01,  1.3359e+00,  5.6250e-01,\n",
       "         4.6582e-01,  1.3110e-01,  1.0879e+00,  1.2666e+00,  6.5186e-01,\n",
       "         4.7363e-01,  1.1484e+00,  2.5488e-01,  1.0928e+00,  1.2344e+00,\n",
       "         1.2119e+00,  3.3154e-01,  1.1504e+00,  8.0176e-01,  2.2839e-01,\n",
       "         1.4580e+00,  9.0479e-01,  2.9785e-01,  2.6587e-01,  8.1689e-01,\n",
       "         9.1016e-01,  5.2979e-01,  6.5625e-01,  6.4941e-01,  5.2539e-01,\n",
       "         6.8604e-01,  1.2236e+00,  3.0493e-01,  7.3584e-01,  3.2935e-01,\n",
       "         7.6123e-01,  6.4062e-01,  6.2012e-01,  1.1836e+00,  4.6655e-01,\n",
       "         5.3076e-01,  3.3887e-01,  8.7744e-01,  7.7783e-01,  7.6611e-01,\n",
       "         1.4277e+00,  8.4570e-01,  1.1191e+00,  3.2080e-01,  9.3408e-01,\n",
       "         1.5635e+00,  8.4863e-01,  4.6875e-01,  8.7061e-01,  9.0088e-01,\n",
       "         7.4072e-01,  5.9131e-01,  3.7671e-01,  8.5596e-01,  7.5049e-01,\n",
       "         4.5654e-01,  6.9727e-01,  1.2148e+00,  3.4033e-01,  5.8936e-01,\n",
       "         1.0244e+00,  5.7080e-01,  8.8135e-01,  8.2617e-01,  3.1201e-01,\n",
       "         4.4360e-01,  6.1670e-01,  4.1528e-01,  1.6089e-01,  1.2666e+00,\n",
       "         7.3975e-01,  1.0217e-01,  2.5879e-01,  1.0967e+00,  1.2070e+00,\n",
       "         8.3447e-01,  7.3877e-01,  1.0420e+00,  5.4785e-01,  6.0889e-01,\n",
       "         8.4229e-01,  1.0322e+00,  5.1758e-01,  1.3418e+00,  1.6797e+00,\n",
       "         5.2588e-01,  8.5596e-01,  9.3750e-01,  8.2959e-01,  3.4985e-01,\n",
       "         8.6816e-01,  8.5352e-01,  5.6055e-01,  7.8369e-01,  6.6260e-01,\n",
       "         2.2607e-01,  6.0010e-01,  7.6416e-01,  5.4102e-01,  1.4004e+00,\n",
       "         1.9258e+00,  1.1201e+00,  8.2861e-01,  1.5000e+00,  8.2617e-01,\n",
       "         1.1221e+00,  7.7002e-01,  3.9648e-01,  5.9912e-01,  6.7773e-01,\n",
       "         9.7217e-01,  1.0283e+00,  9.9609e-01,  4.4336e-01,  1.4639e+00,\n",
       "         9.0869e-01,  8.6328e-01,  8.5645e-01,  3.1104e-01,  8.8037e-01,\n",
       "         7.0605e-01,  1.2393e+00,  8.6621e-01,  3.3521e-01,  1.1182e+00,\n",
       "         4.1455e-01,  3.2715e-01,  6.9727e-01,  7.8320e-01,  3.5425e-01,\n",
       "         8.1055e-01,  8.6279e-01,  8.5449e-01,  5.8252e-01,  2.0581e-01,\n",
       "         4.8804e-01,  1.2207e+00,  1.0010e+00,  1.4121e+00,  1.0312e+00,\n",
       "         1.0361e+00,  1.6250e+00,  7.6270e-01,  5.4297e-01,  4.5728e-01,\n",
       "         2.5024e-01,  8.8721e-01,  6.5479e-01,  3.2056e-01,  1.4673e-01,\n",
       "         6.4307e-01,  3.3911e-01,  7.7148e-01,  3.1421e-01,  4.5923e-01,\n",
       "         8.5303e-01,  8.8574e-01,  1.4668e+00,  8.3887e-01,  1.1719e+00,\n",
       "         6.1621e-01,  1.1641e+00,  8.4961e-01,  7.1094e-01,  1.2715e+00,\n",
       "         9.7363e-01,  9.7119e-01,  1.0215e+00,  1.2588e+00,  6.0938e-01,\n",
       "         8.0615e-01,  9.1064e-01,  3.6035e-01,  7.0850e-01,  1.3770e+00,\n",
       "         9.6094e-01,  5.1025e-01,  8.5840e-01,  6.9727e-01,  5.7471e-01,\n",
       "         7.0410e-01,  2.7832e-01,  5.5371e-01,  8.5107e-01,  5.0098e-01,\n",
       "         6.1035e-01,  5.9326e-01,  2.6050e-01,  6.6406e-01,  8.9404e-01,\n",
       "         9.2969e-01,  3.7402e-01,  3.4985e-01,  5.3711e-01,  5.2588e-01,\n",
       "         8.7646e-01,  9.0430e-01,  7.5146e-01,  6.6211e-01,  1.1641e+00,\n",
       "         9.8682e-01,  3.3301e-01,  9.6338e-01,  4.4336e-01,  2.8052e-01,\n",
       "         8.7061e-01,  1.3916e+00,  8.8330e-01,  9.4775e-01,  1.4316e+00,\n",
       "         8.0176e-01,  8.6963e-01,  7.5732e-01,  1.3291e+00,  3.6743e-01,\n",
       "         6.9629e-01,  4.0698e-01,  4.5459e-01,  6.7578e-01,  4.8779e-01,\n",
       "         1.4248e+00,  3.0640e-01,  5.7910e-01,  7.9785e-01,  1.2803e+00,\n",
       "         6.8945e-01,  7.1826e-01,  9.4629e-01,  6.8799e-01,  7.5342e-01,\n",
       "         1.1162e+00,  4.2188e-01,  9.3359e-01,  6.3232e-01,  7.7637e-01,\n",
       "         1.6162e+00,  7.6611e-01,  8.3350e-01,  1.0234e+00,  6.5771e-01,\n",
       "         8.4814e-01,  5.9717e-01,  8.8037e-01,  8.4326e-01,  8.3301e-01,\n",
       "         6.8164e-01,  1.3555e+00,  9.1162e-01,  1.3389e+00,  9.0283e-01,\n",
       "         1.1162e+00,  1.4688e+00,  1.0078e+00,  5.5127e-01,  1.0176e+00,\n",
       "         8.7842e-01,  4.2456e-01,  1.2578e+00,  7.1484e-01,  5.6592e-01,\n",
       "         6.0547e-01,  1.0400e+00,  1.1338e+00,  6.7871e-01,  1.5564e-01,\n",
       "         7.5928e-01,  8.1982e-01,  8.3594e-01,  7.1924e-01,  3.3447e-01,\n",
       "         6.0449e-01,  2.0105e-01,  6.4111e-01,  9.0820e-01,  9.8340e-01,\n",
       "         3.8208e-01,  1.0420e+00,  8.1787e-01,  8.1006e-01,  9.6826e-01,\n",
       "         4.0234e-01,  9.5312e-01,  8.3594e-01,  6.3428e-01,  6.8994e-01,\n",
       "         4.7144e-01,  9.4141e-01,  1.3018e+00,  7.9004e-01,  1.0430e+00,\n",
       "         3.8721e-01,  8.6035e-01,  1.2666e+00,  9.2920e-01,  8.9893e-01,\n",
       "         1.3896e+00,  1.0898e+00,  4.9731e-01,  6.5137e-01,  5.2783e-01,\n",
       "         2.4854e-01,  3.1592e-01,  4.7266e-01,  8.1348e-01,  9.7852e-01,\n",
       "         7.0166e-01,  8.6572e-01,  1.0156e+00,  3.5840e-01,  4.2578e-01,\n",
       "         9.3652e-01,  3.3936e-01,  7.7295e-01,  6.5723e-01, -6.8426e-05,\n",
       "         6.9531e-01,  7.7930e-01,  6.5088e-01,  3.7939e-01,  2.6343e-01,\n",
       "         5.6592e-01,  2.8516e-01,  6.6846e-01,  9.6094e-01,  1.5049e+00,\n",
       "         1.0449e+00,  3.7036e-01,  7.4951e-01,  4.4849e-01,  1.3135e+00,\n",
       "         8.9990e-01,  7.6855e-01,  9.0088e-01,  4.0918e-01,  7.0801e-01,\n",
       "         9.1895e-01,  1.0781e+00,  5.0586e-01,  6.5576e-01,  9.3262e-01,\n",
       "         6.2549e-01,  4.8755e-01,  1.1885e+00,  5.5176e-01,  1.9128e-01,\n",
       "         3.8672e-01,  8.3105e-01,  3.9380e-01,  4.7900e-01,  6.9141e-01,\n",
       "         5.0537e-01,  4.3750e-01,  8.6768e-01,  5.6299e-01,  3.9893e-01,\n",
       "         3.7085e-01,  8.9697e-01,  1.1094e+00,  4.0454e-01,  5.7129e-01,\n",
       "         1.1992e+00,  1.6953e+00,  2.8613e-01,  8.9160e-01,  8.3789e-01,\n",
       "         8.2764e-01,  7.9590e-01,  4.7461e-01,  7.8760e-01,  1.4961e+00,\n",
       "         1.0957e+00,  9.0576e-01,  4.2627e-01,  8.9697e-01,  9.0137e-01,\n",
       "         4.7705e-01,  5.9424e-01,  7.7344e-01,  6.5771e-01,  7.6416e-01,\n",
       "         1.1484e+00,  9.3604e-01,  1.3340e+00,  5.5811e-01,  7.3291e-01,\n",
       "         7.7783e-01,  3.5107e-01,  8.7891e-01,  6.2354e-01,  9.7900e-01,\n",
       "         8.4180e-01,  5.1855e-01,  8.4277e-01,  8.1396e-01,  4.2847e-01,\n",
       "         8.9258e-01,  5.9863e-01,  4.9756e-01,  5.9961e-01,  3.8477e-01,\n",
       "         2.9614e-01,  8.2520e-01,  3.0078e-01,  8.3545e-01,  1.2715e+00,\n",
       "         1.0010e+00,  1.0684e+00,  7.6025e-01,  6.6211e-01,  3.7061e-01,\n",
       "         8.8525e-01,  6.4795e-01,  1.6296e-01,  4.7681e-01,  7.7686e-01,\n",
       "         2.8760e-01,  6.6797e-01,  9.5117e-01,  7.8320e-01,  8.4619e-01,\n",
       "         1.1328e+00,  4.2749e-01,  4.2603e-01,  3.5010e-01,  4.1089e-01,\n",
       "         1.1836e+00,  9.5166e-01], device='cuda:0'), tensor([-2.6562e-01, -2.7124e-01, -3.3350e-01, -3.3838e-01, -3.3618e-01,\n",
       "        -1.5894e-01, -2.5928e-01, -1.1865e-01, -2.5732e-01, -1.1298e-01,\n",
       "        -3.1641e-01, -2.8442e-01, -1.7395e-01, -4.5264e-01, -2.5391e-01,\n",
       "        -3.1592e-01, -2.7319e-01, -4.4385e-01, -3.8916e-01, -4.2529e-01,\n",
       "        -1.4856e-01, -8.1299e-01, -4.0039e-01, -3.2544e-01, -9.8535e-01,\n",
       "        -3.2349e-01, -3.6108e-01, -4.6338e-01, -1.8542e-01, -1.6980e-01,\n",
       "        -2.9688e-01, -3.0054e-01, -2.7954e-01, -1.6040e-01, -3.6255e-01,\n",
       "        -1.8604e-01, -4.8248e-02, -2.7441e-01, -2.0544e-01, -3.9111e-01,\n",
       "        -1.2756e-01,  5.5634e-02, -2.9150e-01, -1.0706e-01, -4.4458e-01,\n",
       "        -3.9087e-01, -7.9150e-01, -3.7720e-01, -3.3008e-01, -1.7578e-01,\n",
       "        -6.5479e-01, -2.0044e-01, -4.0088e-01,  1.9547e-02, -2.4902e-01,\n",
       "        -2.5879e-01,  1.8518e-01, -1.1340e-01, -1.2170e-01, -1.2817e-01,\n",
       "        -4.5801e-01, -2.0801e-01, -2.0654e-01, -4.4116e-01, -4.1992e-01,\n",
       "        -2.1118e-01, -2.2717e-01, -3.8330e-01, -3.9038e-01, -1.8518e-01,\n",
       "        -5.0146e-01, -3.6108e-01, -2.4915e-01, -2.5732e-01, -2.2803e-01,\n",
       "        -4.0015e-01, -3.3691e-02, -3.4302e-01, -1.2793e-01, -8.4619e-01,\n",
       "        -6.6260e-01, -2.9077e-01, -3.3789e-01, -8.4033e-01, -1.6309e-01,\n",
       "        -1.3794e-01, -1.3298e-02, -3.9233e-01, -3.4204e-01, -2.3474e-01,\n",
       "        -3.1067e-02, -7.8418e-01, -2.2327e-01, -6.0498e-01, -6.3184e-01,\n",
       "        -7.6074e-01, -1.7615e-01, -3.5156e-01, -2.7148e-01,  9.3994e-03,\n",
       "        -4.2267e-02, -3.0249e-01, -2.3608e-01, -2.4841e-01, -3.4766e-01,\n",
       "        -1.6882e-01,  5.1178e-02, -2.6001e-01, -1.5002e-01, -2.4536e-02,\n",
       "        -2.4475e-01, -1.8591e-01, -2.2070e-01, -2.4890e-01, -2.0593e-01,\n",
       "        -3.7915e-01, -1.2805e-01, -2.8101e-01, -3.7305e-01, -1.5515e-01,\n",
       "        -3.5498e-01, -2.3743e-01, -3.8892e-01, -1.8726e-01, -3.2422e-01,\n",
       "        -1.4941e-01, -1.5796e-01, -3.2861e-01, -2.5757e-01, -4.9756e-01,\n",
       "        -2.8125e-01, -2.6367e-01, -2.0544e-01, -3.6523e-01, -4.1187e-01,\n",
       "        -3.0542e-01, -2.0459e-01, -2.1875e-01, -2.4463e-01, -3.4497e-01,\n",
       "        -1.3062e-01, -4.0430e-01, -2.3877e-01, -1.1792e-01, -2.9248e-01,\n",
       "        -7.5781e-01, -6.8481e-02, -2.0093e-01, -3.0493e-01, -2.2412e-01,\n",
       "        -1.2695e-01, -2.3828e-01, -2.5244e-01, -1.3184e-01, -3.1421e-01,\n",
       "        -3.0835e-01, -2.1777e-01, -2.2400e-01, -2.8223e-01, -2.4683e-01,\n",
       "         3.4058e-02, -6.8994e-01, -2.1106e-01, -2.1863e-01, -2.0129e-01,\n",
       "        -3.1885e-01, -2.1851e-01, -1.6968e-01, -5.5713e-01, -3.0542e-01,\n",
       "        -2.8979e-01, -2.3999e-01, -1.1316e-01, -3.6743e-01, -1.2036e-01,\n",
       "        -2.9858e-01, -2.5146e-01, -2.0776e-01, -3.1836e-01, -1.8384e-01,\n",
       "        -2.2498e-01, -2.5854e-01, -2.7979e-01, -1.6895e-01, -6.7188e-01,\n",
       "         3.4424e-02, -8.7793e-01, -3.0518e-01, -2.7222e-01, -2.5586e-01,\n",
       "        -5.5078e-01, -2.1277e-01, -1.4880e-01, -2.1667e-01, -1.8457e-01,\n",
       "        -4.4214e-01, -1.6199e-01, -3.5889e-01, -2.3267e-01, -5.5469e-01,\n",
       "        -1.6272e-01, -3.2715e-01, -2.2278e-01, -2.2229e-01, -3.4399e-01,\n",
       "        -3.5718e-01, -3.3252e-01, -4.4629e-01, -2.3010e-01, -3.4814e-01,\n",
       "        -2.6538e-01, -1.0559e-01, -2.7222e-01, -3.3447e-01, -5.4047e-02,\n",
       "        -2.0581e-01, -3.5327e-01, -3.3008e-01, -5.7129e-01, -1.0590e-01,\n",
       "        -2.5903e-01, -2.1655e-01, -3.2812e-01, -1.1270e+00, -3.8159e-01,\n",
       "        -5.2441e-01, -3.2056e-01, -3.5669e-01, -4.8431e-02, -1.3318e-01,\n",
       "        -2.6709e-01, -5.7324e-01, -4.7290e-01, -1.7651e-01, -1.3757e-01,\n",
       "        -4.0674e-01, -2.3181e-01, -4.1772e-01, -2.0984e-01, -2.0447e-01,\n",
       "        -1.0687e-01, -4.1089e-01,  9.2840e-04, -3.3936e-01, -6.3232e-01,\n",
       "        -1.8250e-01, -5.3271e-01, -1.4465e-01, -1.6492e-01, -9.7363e-01,\n",
       "        -5.6836e-01, -3.7427e-01, -3.8916e-01, -8.9697e-01, -2.3145e-01,\n",
       "        -2.7197e-01, -5.7178e-01, -1.7871e-01, -1.3342e-01, -8.0322e-01,\n",
       "        -1.7273e-01, -1.8958e-01, -1.4233e-01, -4.4678e-01, -1.1951e-01,\n",
       "        -3.5229e-01, -1.3452e-01, -2.5171e-01, -2.8784e-01, -2.1643e-01,\n",
       "        -1.1334e-01, -2.8149e-01, -2.1082e-01, -3.1958e-01, -3.0835e-01,\n",
       "        -3.7720e-01, -1.9458e-01, -2.6660e-01, -1.6687e-01, -3.3911e-01,\n",
       "        -1.5540e-01, -5.9912e-01, -2.8076e-01, -3.1030e-01, -4.4116e-01,\n",
       "        -2.5391e-01, -1.8042e-01, -3.5620e-01, -1.8359e-01, -1.7493e-01,\n",
       "        -2.7710e-01, -6.4014e-01, -1.8311e-01, -3.1250e-01, -2.6074e-01,\n",
       "        -8.0505e-02, -3.1152e-01, -2.2852e-01, -8.9990e-01, -2.3938e-01,\n",
       "        -2.1436e-01, -1.4539e-01, -1.4429e-01, -3.1055e-01,  6.2164e-02,\n",
       "        -8.4570e-01, -2.2717e-01, -3.6841e-01, -4.4556e-02, -9.0869e-01,\n",
       "        -1.4661e-01, -2.8540e-01, -7.0020e-01, -3.1763e-01, -3.0151e-01,\n",
       "        -5.1318e-01, -2.5659e-01, -2.3743e-01, -2.9810e-01, -3.7012e-01,\n",
       "        -2.9126e-01, -3.5986e-01, -1.7261e-01, -3.0078e-01, -4.3140e-01,\n",
       "        -3.8525e-01, -2.4377e-01, -4.5044e-01, -4.5947e-01, -2.9663e-01,\n",
       "        -4.3750e-01, -4.5337e-01, -2.8491e-01, -2.9199e-01, -1.5295e-01,\n",
       "        -2.7734e-01, -9.5276e-02, -5.3925e-02, -2.4731e-01, -3.5254e-01,\n",
       "        -2.2754e-01, -9.0149e-02, -8.4180e-01, -3.9038e-01, -2.0447e-01,\n",
       "        -2.3718e-01, -4.5142e-01, -7.3389e-01, -2.2534e-01, -1.3220e-01,\n",
       "        -2.4463e-01, -2.0129e-01, -4.7021e-01, -3.1323e-01, -1.5955e-01,\n",
       "        -1.7505e-01, -1.0895e-01, -2.7246e-01, -2.6611e-01, -6.7480e-01,\n",
       "        -4.8004e-02, -6.3477e-01, -3.7476e-01, -3.6719e-01, -3.5034e-01,\n",
       "        -1.9592e-01, -3.2275e-01, -2.5610e-01, -3.1079e-01, -4.1504e-01,\n",
       "        -1.5454e-01, -4.4995e-01, -6.5576e-01, -1.5442e-01, -1.6357e-01,\n",
       "        -1.7664e-01, -1.7554e-01, -7.9785e-01, -6.6455e-01, -3.8062e-01,\n",
       "        -3.4888e-01, -3.2178e-01, -1.8677e-01, -4.5068e-01, -4.2383e-01,\n",
       "        -2.3596e-01, -2.4368e-02, -3.4961e-01, -1.5808e-01, -4.2529e-01,\n",
       "        -2.5665e-02, -2.5024e-01, -4.2700e-01, -2.3853e-01, -1.8689e-01,\n",
       "        -4.8511e-01, -2.2864e-01, -4.3042e-01, -3.5400e-01, -2.1458e-03,\n",
       "        -1.3647e-01, -3.4717e-01, -8.1970e-02, -2.4585e-01, -2.2070e-01,\n",
       "        -2.6050e-01, -4.6326e-02, -4.6289e-01, -2.5439e-01, -2.5171e-01,\n",
       "         9.3750e-02, -4.1321e-02, -2.7881e-01, -2.8979e-01, -9.9414e-01,\n",
       "        -4.8584e-01, -3.1665e-01, -3.2812e-01, -1.8677e-01, -3.8794e-01,\n",
       "        -3.2568e-01, -1.2000e-01,  7.8918e-02, -2.4170e-01, -3.9771e-01,\n",
       "        -2.5049e-01,  3.2440e-02, -1.3879e-01, -2.6538e-01, -1.8542e-01,\n",
       "        -2.4268e-01, -4.2285e-01, -1.0834e-01, -9.9487e-02, -3.0762e-01,\n",
       "        -1.7041e-01, -4.9072e-02, -1.4075e-01, -3.0103e-01, -8.0627e-02,\n",
       "        -2.1326e-01, -4.2603e-01, -8.1482e-02, -6.6711e-02, -2.2424e-01,\n",
       "        -4.6094e-01, -7.6904e-01, -2.6074e-01, -1.9397e-01, -2.6050e-01,\n",
       "        -4.7803e-01, -2.2375e-01, -2.2400e-01, -3.8892e-01, -5.7666e-01,\n",
       "        -3.1079e-01, -1.3843e-01, -6.9580e-02, -1.8713e-01, -4.0527e-01,\n",
       "        -1.6821e-01, -2.7075e-01, -3.1567e-01, -1.8286e-01, -2.8418e-01,\n",
       "        -7.0020e-01, -3.1738e-01, -1.1650e+00, -8.8196e-02, -9.2041e-02,\n",
       "        -3.8916e-01,  1.2469e-01, -1.7615e-01, -2.3157e-01, -2.8101e-01,\n",
       "        -4.4287e-01, -2.0227e-01, -1.9080e-01, -3.4497e-01, -1.4514e-01,\n",
       "        -3.4473e-01, -8.5938e-02, -2.2229e-01, -3.1470e-01, -1.6260e-01,\n",
       "        -1.7725e-01, -3.5474e-01, -2.1545e-01, -1.5918e-01, -3.0176e-01,\n",
       "        -3.2788e-01, -7.0215e-01, -4.5142e-01, -1.1298e-01, -1.8091e-01,\n",
       "        -1.8225e-01, -3.1738e-01, -1.8396e-01, -1.1273e-01, -2.8442e-01,\n",
       "        -4.0100e-02, -2.4512e-01, -1.9629e-01, -3.5742e-01, -1.9080e-01,\n",
       "        -5.1709e-01,  5.2704e-02, -3.1567e-01, -2.4597e-01, -1.5210e-01,\n",
       "        -4.0747e-01, -3.5107e-01], device='cuda:0'), tensor([[[ 1.3147e-01,  8.8440e-02,  9.1858e-02],\n",
       "         [ 1.1438e-01,  1.2207e-01,  8.4351e-02],\n",
       "         [-2.8516e-01, -3.0151e-01, -1.5527e-01],\n",
       "         ...,\n",
       "         [-2.2986e-01, -2.8491e-01, -2.9541e-01],\n",
       "         [-3.6835e-02, -3.0914e-02, -6.7627e-02],\n",
       "         [-1.4771e-01, -4.5746e-02, -2.8198e-01]],\n",
       "\n",
       "        [[-2.1765e-01, -4.1595e-02, -2.7847e-02],\n",
       "         [-3.6285e-02, -4.1504e-02,  7.3486e-02],\n",
       "         [-1.1700e-01, -1.4313e-02,  2.1271e-02],\n",
       "         ...,\n",
       "         [ 1.9730e-02, -3.6835e-02,  9.3994e-02],\n",
       "         [ 1.4587e-01,  1.5381e-01,  5.5756e-02],\n",
       "         [-6.3843e-02, -1.6296e-02, -3.1860e-02]],\n",
       "\n",
       "        [[-1.2329e-01, -1.3171e-01, -1.1395e-01],\n",
       "         [-5.0110e-02, -8.8257e-02,  1.0889e-01],\n",
       "         [-5.3833e-02, -1.0233e-03,  5.3345e-02],\n",
       "         ...,\n",
       "         [ 1.1711e-02,  1.0193e-01,  2.4829e-01],\n",
       "         [ 4.0955e-02,  1.1017e-02, -3.4580e-03],\n",
       "         [-1.3757e-01, -5.0079e-02,  4.1901e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.1102e-01, -1.1682e-01, -1.5369e-01],\n",
       "         [ 1.3684e-01,  7.1716e-02,  1.2537e-01],\n",
       "         [ 5.0110e-02,  1.6077e-01,  1.1353e-01],\n",
       "         ...,\n",
       "         [ 3.9111e-01,  3.4692e-01,  4.0405e-01],\n",
       "         [ 3.3402e-04, -5.2948e-02, -2.8458e-02],\n",
       "         [ 1.8896e-01,  1.8604e-01,  9.4116e-02]],\n",
       "\n",
       "        [[ 2.6196e-01,  3.1464e-02, -3.8910e-02],\n",
       "         [ 1.1267e-01,  2.8540e-01,  2.9126e-01],\n",
       "         [-1.4038e-01, -1.8811e-01, -2.4158e-01],\n",
       "         ...,\n",
       "         [-1.0223e-02, -8.7891e-03, -3.6106e-03],\n",
       "         [ 3.1616e-02, -4.0283e-03,  4.8767e-02],\n",
       "         [-3.3936e-01, -2.8491e-01, -2.8711e-01]],\n",
       "\n",
       "        [[-1.9385e-01, -2.0142e-01, -2.0520e-01],\n",
       "         [-1.2891e-01, -5.8289e-02,  5.2856e-02],\n",
       "         [ 4.9286e-02,  3.2837e-02, -2.8778e-02],\n",
       "         ...,\n",
       "         [-1.2189e-01,  2.1094e-01,  5.6641e-01],\n",
       "         [ 8.0994e-02,  6.3599e-02,  6.2683e-02],\n",
       "         [ 1.5457e-02, -5.0262e-02, -7.3242e-02]]], device='cuda:0'), tensor([ 7.0374e-02,  2.4078e-02,  1.2292e-01, -1.2794e-02,  2.8564e-01,\n",
       "         9.5154e-02,  8.1055e-02, -3.9062e-02,  2.5040e-02,  1.3110e-01,\n",
       "        -2.0251e-01, -5.4321e-02,  1.4514e-01,  3.6426e-01, -1.1975e-01,\n",
       "         5.0110e-02, -7.2784e-03, -5.4504e-02,  1.5576e-01, -1.6937e-02,\n",
       "         1.3293e-01,  6.9458e-02,  4.4342e-02,  6.0913e-02,  1.3757e-01,\n",
       "        -5.9662e-02,  1.2415e-01,  2.5696e-02, -7.2021e-03,  7.1838e-02,\n",
       "         2.5284e-02,  2.2485e-01,  1.3989e-01,  7.4341e-02, -1.6882e-01,\n",
       "        -9.7900e-02,  4.3396e-02,  1.0315e-01,  3.2501e-02,  9.5581e-02,\n",
       "         9.1492e-02, -1.6541e-01,  1.5100e-01,  1.2024e-01,  2.6245e-01,\n",
       "         7.1106e-02,  4.3018e-01, -9.2468e-02,  1.2646e-01,  2.4182e-01,\n",
       "         1.4026e-01,  8.7097e-02,  1.0089e-01,  4.4342e-02,  2.0557e-01,\n",
       "         1.8237e-01, -5.6610e-02,  1.8066e-01,  1.9562e-02, -9.6069e-02,\n",
       "         4.4604e-01,  3.1201e-01,  3.0981e-01, -2.8442e-02,  2.8854e-02,\n",
       "        -2.9434e-02,  7.7881e-02, -5.5939e-02, -1.2671e-01,  4.4220e-02,\n",
       "        -1.5137e-01,  2.7832e-01,  2.4231e-01,  1.9885e-01, -5.2765e-02,\n",
       "        -9.3750e-02,  1.5698e-01,  1.7053e-01,  7.9956e-02,  7.5439e-02,\n",
       "        -4.6234e-02,  3.4058e-01,  3.3057e-01,  9.5154e-02,  1.1078e-01,\n",
       "        -1.8753e-02,  5.8685e-02,  5.1788e-02,  7.2937e-02,  1.0046e-01,\n",
       "         9.0515e-02, -3.5187e-02,  2.1619e-01, -2.1667e-02,  9.5825e-02,\n",
       "        -3.2444e-03,  2.0752e-01,  6.3477e-03,  1.4664e-02,  2.9434e-02,\n",
       "         8.2947e-02,  5.6854e-02,  2.2919e-02, -7.1777e-02,  1.2469e-01,\n",
       "         1.0077e-01,  7.0992e-03, -2.3071e-02,  2.4988e-01,  6.7787e-03,\n",
       "         6.9336e-02,  1.7249e-01,  2.7661e-01,  2.1851e-01,  1.8738e-01,\n",
       "         9.8999e-02, -1.1548e-01,  1.6895e-01, -1.5186e-01, -6.3721e-02,\n",
       "         8.1238e-02,  1.5808e-01,  8.7585e-02,  1.3489e-01,  5.4504e-02,\n",
       "        -3.0746e-02, -6.1249e-02,  2.3364e-01, -3.3951e-03,  3.6865e-01,\n",
       "        -1.3596e-02,  4.2053e-02,  3.7506e-02, -3.2768e-03,  7.1289e-02,\n",
       "         2.6782e-01,  6.4697e-02, -1.3696e-01,  1.7493e-01, -1.6125e-01,\n",
       "         1.9019e-01, -6.4331e-02, -3.5065e-02,  3.2397e-01,  1.9312e-01,\n",
       "         1.1978e-02,  1.0217e-01,  5.4779e-03, -1.9119e-02,  7.4463e-02,\n",
       "         1.7603e-01,  4.0436e-02, -5.5389e-02,  8.9645e-04,  1.0858e-01,\n",
       "         2.6947e-02,  1.4114e-02,  9.9548e-02,  1.9531e-02, -3.9215e-03,\n",
       "         9.7168e-02,  1.7456e-01,  1.7261e-01,  3.4637e-02, -3.1174e-02,\n",
       "         1.3977e-01,  3.1177e-01,  3.4302e-02,  4.2206e-02,  7.4158e-02,\n",
       "         4.5319e-02,  1.4771e-01, -1.3123e-02,  1.8884e-01,  1.1053e-01,\n",
       "         2.5610e-01,  3.8116e-02, -6.1432e-02,  3.0835e-01,  8.9233e-02,\n",
       "         6.3232e-02,  1.5466e-01,  2.3193e-01, -3.9124e-02, -4.3945e-02,\n",
       "         1.5625e-02,  1.1951e-01,  2.0142e-02,  9.8572e-02,  6.3538e-02,\n",
       "         1.0913e-01,  4.8218e-02,  2.5708e-01,  2.9395e-01,  2.6465e-01,\n",
       "        -4.9561e-02,  1.3745e-01,  1.3794e-01,  2.0325e-01, -9.6130e-03,\n",
       "         1.0376e-01,  1.0595e-03,  1.9519e-01,  1.1157e-01,  2.8442e-01,\n",
       "         5.8563e-02,  1.8115e-01,  5.4474e-02, -2.0020e-02,  8.6487e-02,\n",
       "         9.8083e-02,  2.1729e-01,  1.7529e-01,  3.0563e-02, -4.0070e-02,\n",
       "         6.0699e-02,  1.4641e-02,  1.1456e-01,  1.1646e-01,  7.6355e-02,\n",
       "        -6.4575e-02,  9.5520e-02,  2.8149e-01,  1.6748e-01,  3.0078e-01,\n",
       "         1.0931e-01,  4.7089e-02,  6.4758e-02, -1.4502e-01,  4.8999e-01,\n",
       "        -1.1133e-01, -3.9864e-03,  1.2268e-02,  3.9612e-02,  1.7773e-01,\n",
       "         2.4048e-02,  1.9861e-01, -5.8136e-02, -1.8640e-01,  1.2680e-02,\n",
       "         3.3569e-01, -1.5430e-01,  2.6260e-02, -1.0449e-01,  1.8298e-01,\n",
       "        -5.3497e-02,  2.0300e-01,  1.6516e-01,  1.5186e-01,  9.0393e-02,\n",
       "         2.5781e-01,  1.8066e-02, -8.3069e-02, -7.9880e-03, -5.2429e-02,\n",
       "         2.5659e-01,  4.8248e-02,  2.5732e-01,  4.8431e-02,  8.1482e-02,\n",
       "        -3.8025e-02,  3.6450e-01, -1.5967e-01, -6.0120e-02,  2.3376e-01,\n",
       "         1.0553e-01,  1.5976e-02,  1.9531e-02,  1.2018e-01,  2.6465e-01,\n",
       "         6.4880e-02,  1.0382e-01,  2.5909e-02,  3.5919e-02,  2.5537e-01,\n",
       "         1.3220e-01,  2.4597e-01,  1.8567e-01,  2.1985e-01, -5.3009e-02,\n",
       "         3.7109e-02,  2.9602e-02,  7.6355e-02,  1.7883e-01,  3.8513e-02,\n",
       "        -4.3701e-02,  1.7542e-01,  2.8247e-01,  1.0370e-01,  3.1396e-01,\n",
       "         6.1890e-02,  8.9661e-02, -2.4548e-01,  9.1553e-03,  2.1558e-01,\n",
       "         1.9385e-01,  2.1790e-02,  2.6392e-01, -1.4136e-01, -1.5373e-02,\n",
       "         1.4124e-01, -3.2776e-02,  7.0572e-03,  3.6774e-02, -4.8065e-02,\n",
       "         2.0862e-01,  1.2927e-01,  4.8218e-02, -2.9968e-02,  3.1372e-01,\n",
       "         1.2177e-01,  8.5449e-02, -1.1749e-03,  1.0852e-01,  4.2480e-02,\n",
       "        -1.6882e-01,  7.0524e-04,  1.5045e-02,  4.0308e-01,  1.9788e-01,\n",
       "         1.3208e-01,  1.0483e-02,  1.0333e-01,  1.5686e-01,  1.9751e-01,\n",
       "         9.1614e-02,  2.7542e-02,  2.1591e-03, -8.5999e-02,  5.6702e-02,\n",
       "         9.1553e-02,  2.0740e-01,  1.1322e-01,  1.2646e-01,  1.7407e-01,\n",
       "         6.3049e-02,  2.1130e-01,  3.0420e-01,  3.5010e-01,  1.8396e-01,\n",
       "         8.5876e-02, -1.3184e-01,  1.8445e-01,  2.3083e-01,  7.3608e-02,\n",
       "         1.9531e-01,  8.0933e-02, -1.4282e-02,  2.0850e-01,  1.8958e-01,\n",
       "         4.6387e-02,  1.5100e-01,  1.1487e-01,  6.8245e-03,  2.2308e-02,\n",
       "        -1.4941e-01,  8.1970e-02,  2.9694e-02,  2.6343e-01,  4.2847e-01,\n",
       "        -3.7262e-02, -1.5793e-02,  2.0801e-01,  2.5864e-02,  4.2572e-02,\n",
       "        -4.8462e-02, -3.0853e-02,  1.4954e-01, -3.8055e-02,  2.8271e-01,\n",
       "         4.4495e-02, -7.2021e-02, -2.3392e-02,  3.2406e-03,  2.4207e-01,\n",
       "        -4.7638e-02,  1.7285e-01, -3.4912e-02, -8.9741e-04, -5.2094e-02,\n",
       "         3.0737e-01,  2.0093e-01,  3.1681e-03,  1.8787e-01,  2.5665e-02,\n",
       "         1.2016e-02,  1.6138e-01,  5.5145e-02,  6.9153e-02, -1.1377e-01,\n",
       "         4.0253e-02,  8.4076e-03,  6.3184e-01,  2.0544e-01,  1.0345e-01,\n",
       "         2.7441e-01, -4.5258e-02, -4.7943e-02, -4.9805e-02, -6.8115e-02,\n",
       "         9.4299e-02,  2.3376e-01,  2.0096e-02,  9.6069e-02, -8.9478e-02,\n",
       "         1.4978e-01, -2.0416e-02, -3.2007e-01, -1.3269e-01,  3.1738e-02,\n",
       "         7.7698e-02, -1.5259e-01,  1.7688e-01, -1.7383e-01, -1.1383e-01,\n",
       "        -8.6243e-02, -3.7537e-02,  3.0347e-01,  1.7065e-01,  6.6284e-02,\n",
       "         1.4258e-01,  5.5603e-02,  1.3684e-01, -6.4148e-02,  1.7444e-01,\n",
       "         1.9934e-01, -3.3630e-02, -1.1505e-02,  1.3257e-01,  1.4453e-01,\n",
       "         3.2318e-02,  2.9541e-02, -9.1675e-02,  1.2329e-01, -5.6305e-02,\n",
       "         3.0566e-01,  3.0884e-01,  3.4888e-01,  3.5076e-03,  3.2593e-02,\n",
       "         8.4656e-02, -2.4933e-02,  5.1880e-02,  3.7842e-02, -3.7155e-03,\n",
       "         1.1743e-01,  7.2205e-02,  3.5498e-01,  2.5366e-01,  3.9795e-02,\n",
       "         2.0435e-01,  2.3689e-03,  3.8971e-02, -5.2399e-02,  4.2389e-02,\n",
       "         2.0898e-01, -4.4891e-02,  2.4304e-01,  1.0333e-01,  2.4902e-01,\n",
       "         1.8396e-01,  3.6670e-01,  1.3831e-01, -2.1576e-02, -3.6678e-03,\n",
       "        -3.1219e-02,  8.3740e-02,  1.8250e-01,  6.9031e-02,  4.7638e-02,\n",
       "        -1.3879e-01,  8.0627e-02,  1.9006e-01,  2.4438e-01,  1.0413e-01,\n",
       "        -1.0124e-02,  1.1230e-01, -9.2590e-02, -3.9001e-02,  2.1204e-01,\n",
       "         2.6270e-01,  1.3208e-01,  2.7930e-01, -1.1124e-02,  2.4841e-01,\n",
       "         2.7496e-02,  9.5947e-02,  3.5980e-02,  3.8940e-02, -2.7664e-02,\n",
       "         7.7026e-02,  1.6309e-01,  7.0850e-01, -1.6272e-01,  8.3679e-02,\n",
       "         2.0801e-01,  1.9169e-03,  1.5100e-01, -9.3140e-02,  4.0710e-02,\n",
       "        -2.8595e-02,  9.2285e-02,  1.6022e-02, -2.3041e-02,  1.8933e-01,\n",
       "        -1.2262e-01,  3.1555e-02,  6.8779e-03, -6.9641e-02,  1.5271e-01,\n",
       "         1.0175e-01, -5.4382e-02], device='cuda:0'), tensor([0.5840, 0.6870, 0.3955, 0.4963, 0.3679, 0.4226, 0.7583, 0.7490, 0.7183,\n",
       "        0.3723, 0.8921, 0.5752, 0.6587, 0.1932, 1.2275, 1.0859, 1.0449, 0.5601,\n",
       "        0.6436, 0.9844, 0.7983, 0.7290, 0.7715, 0.7388, 0.3953, 0.6973, 0.5786,\n",
       "        0.9551, 0.6045, 0.5928, 0.9507, 0.7388, 0.3503, 1.0381, 1.3926, 0.6323,\n",
       "        1.2432, 0.4658, 0.4822, 0.9897, 0.6592, 0.8774, 0.5073, 0.5312, 0.3376,\n",
       "        0.5308, 0.3208, 0.9653, 0.6187, 0.4172, 0.7388, 0.7554, 0.7925, 0.8755,\n",
       "        0.3831, 0.4829, 0.8657, 0.4675, 0.5317, 0.7510, 0.1938, 0.2720, 0.3096,\n",
       "        1.4473, 0.7910, 0.7261, 0.3965, 0.9956, 0.7490, 0.4490, 1.2666, 0.3162,\n",
       "        0.2937, 0.6689, 0.8574, 0.7627, 0.3423, 0.4961, 0.7529, 0.4216, 1.0156,\n",
       "        0.2455, 0.3521, 0.7407, 0.8154, 0.7520, 0.5625, 0.6084, 0.7432, 0.3457,\n",
       "        0.7544, 1.3389, 0.4458, 0.9238, 0.8696, 0.5425, 0.3398, 0.6919, 0.7988,\n",
       "        0.9663, 1.4072, 0.7544, 0.7314, 0.6104, 0.6924, 1.2334, 0.9565, 0.8774,\n",
       "        0.4172, 0.8945, 0.4180, 0.4861, 0.4375, 0.5049, 0.4128, 0.8462, 1.3906,\n",
       "        0.4014, 0.9551, 0.6694, 0.9370, 0.5889, 0.6519, 0.4246, 0.8203, 0.8677,\n",
       "        0.7295, 0.6641, 0.8633, 0.2727, 0.9741, 0.6914, 0.6792, 0.5112, 0.4546,\n",
       "        0.3345, 1.0693, 1.2822, 0.4456, 0.7905, 0.4023, 0.6504, 0.6772, 0.2473,\n",
       "        0.3938, 0.6885, 0.4846, 0.6099, 0.4912, 0.9883, 0.7065, 0.8887, 1.1152,\n",
       "        0.9424, 0.9526, 0.5425, 0.8262, 0.6284, 0.7681, 0.5674, 0.6338, 0.8418,\n",
       "        0.3945, 1.0615, 0.9355, 0.7139, 0.2534, 0.5649, 0.4573, 0.5020, 0.7114,\n",
       "        0.6831, 0.6138, 0.8325, 0.7144, 0.8081, 0.7178, 0.4866, 0.4653, 0.5229,\n",
       "        0.5884, 0.6816, 1.0303, 0.5127, 0.5903, 0.9336, 0.4866, 0.7993, 0.5479,\n",
       "        0.5859, 0.6738, 0.9985, 0.4119, 0.3657, 0.2937, 1.0684, 0.6167, 0.5664,\n",
       "        0.4734, 0.6362, 0.7671, 0.4993, 0.3286, 0.4341, 0.5962, 0.5752, 0.4429,\n",
       "        0.6436, 0.9458, 0.5024, 0.7905, 0.4563, 0.4583, 0.5918, 1.0342, 0.8901,\n",
       "        0.8818, 0.7993, 0.6899, 0.4421, 0.7852, 0.6548, 0.2656, 0.7271, 0.2893,\n",
       "        0.8442, 0.8960, 0.5752, 1.5439, 0.1890, 1.1201, 0.7178, 0.7637, 0.6123,\n",
       "        0.7393, 0.4790, 0.5488, 0.4412, 0.9443, 0.5269, 0.3342, 1.0908, 0.4841,\n",
       "        0.8589, 0.4465, 1.3564, 0.6030, 0.3486, 0.4307, 0.4910, 0.3679, 0.6758,\n",
       "        0.7061, 0.5767, 0.9463, 0.3701, 0.9429, 0.3979, 0.8105, 0.7192, 0.8052,\n",
       "        0.2362, 1.1855, 0.6104, 0.4902, 0.3752, 0.4790, 0.8296, 0.6353, 0.3928,\n",
       "        0.7603, 0.3503, 0.6284, 0.7808, 0.3350, 0.3843, 0.3213, 0.7837, 0.4241,\n",
       "        0.8633, 0.7837, 0.8096, 1.0127, 0.4165, 0.9902, 1.0312, 0.5581, 0.3262,\n",
       "        0.4851, 0.2554, 0.7451, 0.5801, 0.9907, 0.5322, 0.3394, 0.3435, 1.0342,\n",
       "        0.3601, 0.7622, 0.9336, 0.4468, 0.7422, 0.6963, 0.5459, 0.7324, 0.6064,\n",
       "        0.3804, 0.8213, 0.9785, 0.2920, 0.7661, 0.7173, 0.5850, 0.7202, 0.5713,\n",
       "        1.3623, 0.8047, 0.8496, 0.2908, 0.7407, 0.4111, 0.7900, 0.7324, 0.5264,\n",
       "        0.3254, 0.4744, 0.7051, 0.6045, 1.0127, 0.4502, 0.7329, 0.4312, 0.5972,\n",
       "        0.4226, 0.4275, 0.5337, 0.5923, 0.2722, 0.3420, 0.2732, 0.8237, 0.6714,\n",
       "        0.3784, 0.3621, 0.8013, 1.0225, 0.9517, 0.6333, 0.3242, 0.5435, 0.5786,\n",
       "        0.3455, 0.6367, 0.7783, 0.7412, 1.2988, 0.6187, 1.0254, 0.3481, 0.2260,\n",
       "        1.0762, 0.6719, 0.7378, 0.5298, 0.4521, 0.8560, 0.6797, 0.5771, 1.0625,\n",
       "        0.3542, 0.6064, 0.8608, 0.8901, 0.6743, 0.8047, 0.6724, 0.5957, 0.8076,\n",
       "        0.5815, 0.8091, 0.2534, 0.4712, 0.7656, 0.4570, 0.9043, 0.8110, 0.7510,\n",
       "        0.4922, 0.4636, 0.7256, 0.4365, 0.5078, 0.2039, 0.3101, 0.8242, 0.2561,\n",
       "        1.2236, 1.0332, 0.8423, 0.5835, 0.5532, 0.3516, 0.8267, 0.5698, 0.7686,\n",
       "        0.4663, 0.9751, 1.7324, 1.2041, 0.9116, 0.5317, 1.3447, 0.5249, 1.4199,\n",
       "        1.0273, 1.0605, 0.8257, 0.2871, 0.7861, 0.6812, 0.3977, 0.8750, 0.7979,\n",
       "        1.0068, 0.4067, 0.3691, 0.7969, 0.4585, 0.6626, 0.4724, 1.0947, 1.0918,\n",
       "        1.4062, 0.3447, 0.8438, 0.2368, 0.3899, 0.3379, 0.9028, 0.7881, 0.7769,\n",
       "        0.8823, 0.6938, 0.6611, 0.9395, 0.7290, 0.9985, 0.3142, 0.4224, 0.6304,\n",
       "        0.4031, 0.8428, 0.4539, 0.6050, 0.4917, 0.3699, 1.0195, 0.3438, 0.4158,\n",
       "        0.3391, 0.3701, 0.2688, 0.3613, 0.7617, 0.5991, 0.8975, 0.4270, 0.7212,\n",
       "        0.5239, 0.4612, 0.8037, 0.4436, 0.8491, 0.3892, 0.5015, 0.5391, 0.7935,\n",
       "        1.0117, 0.8545, 0.3940, 0.3875, 0.4001, 0.2571, 0.5996, 0.3228, 0.5854,\n",
       "        0.3970, 0.4050, 0.6660, 1.1172, 0.5962, 0.6729, 0.0446, 1.4639, 0.7539,\n",
       "        0.4844, 0.7520, 0.6982, 0.5469, 0.7925, 0.7349, 0.7207, 0.9365, 1.0645,\n",
       "        0.6558, 0.9409, 0.8325, 0.8530, 1.0732, 0.5205, 0.6558, 0.5859],\n",
       "       device='cuda:0'), tensor([-0.2830, -0.1367, -0.0483, -0.0236, -0.3149,  0.1836, -0.2632, -0.2539,\n",
       "        -0.2031, -0.1543, -0.2126,  0.1937, -0.3237, -0.2001, -0.4934, -0.4631,\n",
       "        -0.1652, -0.2450, -0.3770, -0.7144, -0.4551, -0.3337, -0.2155, -0.3418,\n",
       "        -0.1466, -0.3235, -0.3159, -0.6274, -0.1809, -0.3962, -0.1449, -0.2178,\n",
       "        -0.1631, -0.3489, -0.3140, -0.0219, -0.6919, -0.0220, -0.2103, -0.3418,\n",
       "        -0.2852, -0.2242, -0.2460, -0.1305, -0.1729, -0.0943, -0.2551, -0.3113,\n",
       "        -0.2915, -0.1915, -0.5366, -0.3223, -0.1495, -0.3569,  0.0863, -0.1982,\n",
       "        -0.2666, -0.0959,  0.0778, -0.1433, -0.3232, -0.2402, -0.2213,  0.0129,\n",
       "        -0.0948, -0.0791, -0.1544, -0.6592, -0.1284,  0.1179, -0.8896,  0.1153,\n",
       "        -0.2590, -0.2271, -0.1670, -0.1709, -0.1782, -0.2269, -0.1428, -0.2153,\n",
       "        -0.4475, -0.2419, -0.2646, -0.1964, -0.2749, -0.3218, -0.2771, -0.2832,\n",
       "        -0.1372, -0.1133, -0.2289, -0.6035, -0.2023, -0.4045, -0.3923, -0.1986,\n",
       "        -0.2373, -0.1071, -0.2327, -0.3865, -0.8389, -0.3547, -0.3091, -0.3450,\n",
       "        -0.1150, -0.4792, -0.4822, -0.4744, -0.1641, -0.5483, -0.2498, -0.2281,\n",
       "        -0.2305, -0.2539, -0.0940, -0.3425, -0.2339, -0.2629, -0.2355, -0.1509,\n",
       "        -0.4272, -0.2375, -0.3354, -0.1744, -0.1691, -0.3787, -0.2012, -0.3701,\n",
       "        -0.4988, -0.2778, -0.3784, -0.3049, -0.2489, -0.6807, -0.0584, -0.2749,\n",
       "        -0.4155, -0.0983, -0.2510, -0.1242, -0.0951, -0.2236, -0.2102, -0.1746,\n",
       "        -0.2812, -0.1429, -0.2168, -0.2128, -0.1044, -0.3999, -0.1886, -0.7603,\n",
       "        -0.4741, -0.4976, -0.4417, -0.1299, -0.4778, -0.3379, -0.3391, -0.2715,\n",
       "        -0.5015, -0.3037, -0.1891, -0.3186, -0.7070, -0.4041, -0.2510, -0.2460,\n",
       "         0.0453, -0.1152, -0.1749, -0.4036, -0.1213, -0.1026, -0.1841, -0.0757,\n",
       "        -0.1580,  0.0852, -0.3616, -0.2510, -0.3628, -0.3718, -0.4260, -0.2617,\n",
       "        -0.0709, -0.5205, -0.1891, -0.3276, -0.0827, -0.2014, -0.1588, -0.4443,\n",
       "        -0.2499, -0.0774, -0.0745, -0.7573, -0.1129, -0.4702, -0.3025, -0.2861,\n",
       "        -0.1898, -0.1290, -0.2346, -0.0850, -0.4004, -0.1064, -0.3330, -0.1340,\n",
       "        -0.5361, -0.2094, -0.3323, -0.2849, -0.2571, -0.4878, -0.2566, -0.3579,\n",
       "        -0.3413, -0.1921, -0.3384, -0.1284, -0.2585, -0.1755, -0.2527, -0.2028,\n",
       "         0.0087, -0.4102, -0.2421, -0.1902, -0.0521, -0.3274, -0.1223, -0.2832,\n",
       "        -0.2125, -0.1415, -0.5137, -0.1562, -0.3088, -0.1368, -0.1377, -0.0909,\n",
       "        -0.2201, -0.5278, -0.0267, -0.3083, -0.2191, -0.9136, -0.2275, -0.0911,\n",
       "         0.0248, -0.1973, -0.0926, -0.1261, -0.1573, -0.4509, -0.4412, -0.2148,\n",
       "        -0.7319, -0.2532, -0.4150, -0.2017, -0.3640, -0.3291, -0.4131, -0.2338,\n",
       "         0.2317, -0.1813, -0.4414, -0.2805, -0.3000, -0.1929, -0.1912, -0.1665,\n",
       "        -0.2622, -0.2561, -0.2253, -0.2695, -0.1096, -0.1049, -0.2046, -0.4756,\n",
       "        -0.4229, -0.5767, -0.3225, -0.1660, -0.6372, -0.4875, -0.0718, -0.1713,\n",
       "        -0.1952, -0.2361, -0.0850, -0.4714, -0.2130, -0.3728, -0.1304, -0.1112,\n",
       "        -0.6362, -0.0205, -0.3516, -0.5391, -0.2291, -0.1121, -0.4424, -0.1157,\n",
       "        -0.1058, -0.1168, -0.1733, -0.3374, -0.4148, -0.2766, -0.3501, -0.3110,\n",
       "        -0.0858, -0.2952, -0.0602, -0.3201, -0.1510, -0.3491, -0.2312, -0.3164,\n",
       "         0.1650, -0.6455, -0.3567,  0.2299, -0.2654, -0.2632, -0.1803, -0.2032,\n",
       "        -0.6084, -0.3098, -0.5039, -0.0310, -0.2751, -0.1851,  0.0099, -0.4971,\n",
       "        -0.1819, -0.1376, -0.1926, -0.2106, -0.3652, -0.0184, -0.1681, -0.0328,\n",
       "        -0.3044, -0.4014, -0.7339, -0.5688, -0.2006, -0.1105, -0.1815, -0.0402,\n",
       "        -0.2456, -0.4438, -0.2496, -0.2573, -0.2445, -0.4492, -0.1947, -0.2372,\n",
       "        -0.3367, -0.0971, -0.3279,  0.0053, -0.0217, -0.3096, -0.3818, -0.1328,\n",
       "        -0.3782, -0.3044, -0.2197, -0.4707, -0.2732, -0.2710, -0.2367, -0.4346,\n",
       "        -0.3044, -0.2466, -0.1925, -0.4971, -0.2092, -0.2568, -0.3403, -0.0589,\n",
       "        -0.5264, -0.4175, -0.3003, -0.1819, -0.2568, -0.2150, -0.2236, -0.1260,\n",
       "        -0.3271, -0.0204, -0.3171, -0.0597, -0.6421, -0.5449, -0.2966,  0.2539,\n",
       "         0.0626, -0.2637, -0.4578, -0.2654, -0.2678, -0.1703, -0.4722, -0.2438,\n",
       "        -0.6826, -0.3464, -0.1317, -0.1405, -0.2964, -0.2256, -0.4060, -0.4277,\n",
       "        -0.3005, -0.2274, -0.1608, -0.2015, -0.0103, -0.1221, -0.3232, -0.5234,\n",
       "        -0.0767, -0.1442, -0.2954,  0.1067, -0.3254,  0.0494, -0.3762, -0.7402,\n",
       "        -0.0235, -0.2397, -0.4170, -0.0543, -0.2651, -0.1820, -0.1312, -0.4065,\n",
       "        -0.2637, -0.2844, -0.3748, -0.3779, -0.6304, -0.3572, -0.4448, -0.1595,\n",
       "        -0.2883, -0.0759, -0.2717, -0.5938, -0.2119, -0.2273,  0.1831, -0.2522,\n",
       "        -0.6138, -0.1958, -0.1083, -0.2412, -0.2177, -0.1777, -0.2223, -0.1696,\n",
       "        -0.1498, -0.3730, -0.0793, -0.0722, -0.1377, -0.1760, -0.1819,  0.0608,\n",
       "        -0.4285, -0.1188, -0.2419, -0.1220, -0.2856, -0.3203, -0.3228, -0.2233,\n",
       "        -0.2966, -0.1573, -0.2566, -0.1096, -0.1902, -0.3706, -0.2190, -0.1172,\n",
       "        -0.1930, -0.4214, -0.2261, -0.5674, -0.0739, -0.2117, -0.4124, -0.1531,\n",
       "        -0.0011, -0.3083, -0.2087,  0.0653, -0.2800, -0.1637, -0.5234, -0.5371,\n",
       "        -0.3384, -0.5483, -0.5068, -0.2734, -0.5947, -0.3701, -0.3901, -0.1571],\n",
       "       device='cuda:0'), tensor([[[-3.0613e-04,  6.1188e-02,  1.1597e-02],\n",
       "         [ 2.2668e-01,  3.2471e-01,  2.1521e-01],\n",
       "         [ 8.7402e-02,  5.1147e-02,  1.7227e-02],\n",
       "         ...,\n",
       "         [-3.4882e-02, -3.6469e-02, -5.4054e-03],\n",
       "         [ 8.2886e-02,  8.3374e-02,  2.1042e-02],\n",
       "         [-1.6492e-01, -1.1841e-01, -7.7087e-02]],\n",
       "\n",
       "        [[-6.2469e-02, -1.2939e-02,  1.7609e-02],\n",
       "         [ 1.1298e-01,  4.7363e-02,  1.1414e-01],\n",
       "         [-9.1003e-02, -4.6906e-02, -1.0553e-01],\n",
       "         ...,\n",
       "         [-1.0870e-01, -5.9723e-02, -8.8806e-02],\n",
       "         [-8.6182e-02, -1.3623e-01, -1.4197e-01],\n",
       "         [ 7.2823e-03, -2.2461e-02, -8.0750e-02]],\n",
       "\n",
       "        [[ 4.5776e-02,  1.7444e-01,  5.6152e-02],\n",
       "         [ 2.3758e-02,  7.3547e-02,  3.4332e-02],\n",
       "         [-1.0339e-01, -1.3318e-01, -1.3574e-01],\n",
       "         ...,\n",
       "         [-1.9272e-02, -1.0193e-01, -4.6112e-02],\n",
       "         [-6.7383e-02, -1.0236e-01, -1.8616e-01],\n",
       "         [ 3.3142e-02,  3.1372e-02,  4.6356e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-1.8604e-01, -2.5986e-02,  7.5684e-02],\n",
       "         [ 2.7466e-02,  3.0945e-02, -4.3457e-02],\n",
       "         [ 2.1460e-01, -1.1438e-01,  1.2878e-01],\n",
       "         ...,\n",
       "         [ 6.1157e-02, -8.6487e-02, -2.8442e-02],\n",
       "         [-1.3403e-01, -3.5004e-02,  4.4800e-02],\n",
       "         [-3.7140e-02,  5.4718e-02, -1.4519e-02]],\n",
       "\n",
       "        [[-1.0319e-03, -5.5023e-02, -7.8003e-02],\n",
       "         [-9.1629e-03,  2.6352e-02,  6.7505e-02],\n",
       "         [-5.9113e-02, -5.6190e-03,  2.3590e-02],\n",
       "         ...,\n",
       "         [ 8.5938e-02, -8.0444e-02, -3.3142e-02],\n",
       "         [ 1.0236e-01,  5.4108e-02, -2.2998e-01],\n",
       "         [ 6.9824e-02,  3.6192e-04,  1.2561e-01]],\n",
       "\n",
       "        [[ 3.3894e-03, -1.0284e-01,  4.7760e-02],\n",
       "         [ 6.1859e-02,  6.0616e-03,  8.2520e-02],\n",
       "         [-3.6896e-02, -5.9601e-02, -2.6215e-02],\n",
       "         ...,\n",
       "         [ 2.2491e-02, -2.1194e-02,  1.3098e-01],\n",
       "         [-1.6907e-01,  6.0547e-02,  3.1948e-03],\n",
       "         [-1.3379e-01, -4.1718e-02, -1.7358e-01]]], device='cuda:0'), tensor([-0.1340, -0.0091,  0.4941,  0.3403, -0.0213,  0.2808, -0.0520,  0.0105,\n",
       "         0.0013,  0.1985,  0.2081, -0.1735,  0.1625,  0.1764,  0.0205,  0.0959,\n",
       "         0.0999,  0.0406,  0.1581, -0.1064,  0.0087,  0.0104,  0.1299,  0.0046,\n",
       "         0.1393,  0.0028, -0.0375,  0.0262,  0.3215, -0.0703,  0.0671,  0.1037,\n",
       "         0.1516,  0.0424,  0.0439, -0.0105, -0.0830,  0.1127,  0.0130,  0.1243,\n",
       "         0.1111,  0.2434,  0.1140, -0.1803, -0.0831, -0.0830, -0.0360, -0.0396,\n",
       "         0.0846,  0.1687,  0.0927,  0.0661,  0.1088,  0.2996, -0.0057, -0.1132,\n",
       "        -0.0398,  0.2739,  0.0487,  0.0546,  0.2776,  0.0444,  0.0584, -0.0899,\n",
       "         0.0801,  0.1183,  0.1138, -0.0115, -0.1282,  0.0684,  0.2107, -0.0696,\n",
       "        -0.0294, -0.0583, -0.0159,  0.0458,  0.1799, -0.0568,  0.1470,  0.0377,\n",
       "        -0.1366,  0.1357, -0.0342,  0.3293,  0.1047,  0.0452,  0.0155,  0.0288,\n",
       "         0.1246,  0.2517,  0.1150,  0.1039, -0.0458,  0.0890, -0.0867,  0.1471,\n",
       "         0.0428,  0.3857,  0.1301,  0.0300,  0.0675,  0.1626,  0.0380,  0.0221,\n",
       "         0.0597,  0.1888,  0.1233,  0.0401,  0.1355, -0.0052,  0.0764,  0.3323,\n",
       "        -0.0041,  0.2788, -0.1586, -0.0373, -0.0956,  0.0418, -0.0819, -0.1134,\n",
       "         0.0110,  0.2556, -0.0184, -0.0637,  0.0498, -0.0999,  0.0337,  0.0920,\n",
       "         0.0196,  0.0389,  0.1609, -0.0154, -0.1780,  0.1703, -0.0433, -0.0812,\n",
       "         0.0143,  0.0784, -0.1089,  0.1697, -0.1288, -0.0385,  0.0870,  0.0372,\n",
       "        -0.0059,  0.1144, -0.2026, -0.0983,  0.1274,  0.0016,  0.1710, -0.0409,\n",
       "         0.0483,  0.0021, -0.0255,  0.0193, -0.0443,  0.2208, -0.0389,  0.2861,\n",
       "         0.0007,  0.1530, -0.0817,  0.0578, -0.0788,  0.1804,  0.0059,  0.1758,\n",
       "         0.0900,  0.1024, -0.0388,  0.2395,  0.0215,  0.1237,  0.2402, -0.0172,\n",
       "         0.1765,  0.1295,  0.1793, -0.0095,  0.1271,  0.0178,  0.3113, -0.0053,\n",
       "        -0.0865,  0.0704, -0.1077, -0.1575,  0.0016,  0.0825, -0.0111, -0.0429,\n",
       "        -0.0379,  0.3049,  0.1242,  0.1714,  0.1654,  0.0396, -0.0273,  0.0162,\n",
       "         0.0139,  0.0544, -0.0186,  0.0020,  0.2021,  0.0023, -0.0223, -0.0582,\n",
       "        -0.0416, -0.1682,  0.1299,  0.0920,  0.2578, -0.1062, -0.0925,  0.1749,\n",
       "         0.0687,  0.0222,  0.1355,  0.2148, -0.1302,  0.0793,  0.0577,  0.1146,\n",
       "        -0.0020,  0.0903, -0.2561,  0.0469, -0.0391,  0.1058,  0.2510,  0.0410,\n",
       "         0.0024,  0.1793, -0.0132,  0.0380, -0.0428,  0.1561, -0.0581,  0.1144,\n",
       "        -0.0318,  0.0471, -0.0790,  0.0452, -0.1099, -0.0138,  0.1477, -0.0103,\n",
       "         0.0196, -0.1057,  0.4651, -0.0372,  0.1660,  0.1433, -0.0673, -0.2610,\n",
       "        -0.0655,  0.0576,  0.1921,  0.0197,  0.1459,  0.0173,  0.0198, -0.0134,\n",
       "         0.0919, -0.1043, -0.0187,  0.1277,  0.0643,  0.0151,  0.0303,  0.0524,\n",
       "         0.0800,  0.3035,  0.0436,  0.0216, -0.0827,  0.2861,  0.1311, -0.0511,\n",
       "        -0.0345,  0.1962,  0.1700,  0.3354,  0.1575,  0.2617,  0.0878,  0.0970,\n",
       "         0.0853, -0.0515,  0.0265,  0.2318,  0.0423,  0.0287,  0.1014,  0.2891,\n",
       "        -0.0536, -0.0220,  0.1556,  0.0148,  0.0738,  0.0192,  0.0172, -0.0131,\n",
       "         0.2305,  0.1231,  0.0249, -0.1043,  0.3569, -0.0454,  0.1432,  0.1013,\n",
       "         0.1514,  0.2639,  0.3816,  0.0107, -0.0449, -0.0434,  0.1162, -0.1438,\n",
       "        -0.0365, -0.1399,  0.0045, -0.0757,  0.0556,  0.2983,  0.1731, -0.2345,\n",
       "        -0.0582,  0.2321,  0.1459,  0.0512,  0.2141,  0.1082,  0.3704,  0.0331,\n",
       "         0.0287,  0.1975,  0.1214,  0.0704,  0.0524, -0.1486, -0.0884,  0.0116,\n",
       "         0.1519,  0.0510, -0.0069,  0.0359, -0.2059, -0.2051,  0.0995, -0.0900,\n",
       "         0.0692,  0.0682,  0.1033,  0.2148, -0.0374,  0.3396, -0.0833,  0.1149,\n",
       "        -0.0200,  0.0543,  0.1797,  0.0439,  0.1564, -0.0782,  0.1943,  0.0785,\n",
       "         0.2130, -0.0509,  0.3752,  0.3076, -0.0498,  0.1351,  0.0483,  0.0269,\n",
       "        -0.0916, -0.0534,  0.2178,  0.0466, -0.1583,  0.1489, -0.1312,  0.1749,\n",
       "        -0.1023,  0.1125, -0.0226,  0.1809,  0.2430,  0.1042, -0.0354,  0.3101,\n",
       "        -0.0094,  0.0799,  0.0929, -0.0863, -0.0313,  0.0033,  0.0597,  0.2294,\n",
       "         0.1401,  0.2664, -0.0568, -0.0588, -0.0941, -0.1537,  0.1066,  0.1675,\n",
       "         0.0679, -0.0728, -0.0846,  0.1260, -0.0140, -0.0079,  0.2238, -0.0035,\n",
       "         0.1310,  0.1681, -0.0011,  0.2539,  0.0127, -0.0932,  0.0591,  0.1172,\n",
       "        -0.0223, -0.0289,  0.3618,  0.1760, -0.0375,  0.0093, -0.0989, -0.0285,\n",
       "         0.0820,  0.0176, -0.0611,  0.3530,  0.0391,  0.1946, -0.0855,  0.0845,\n",
       "         0.0661, -0.0729, -0.0526,  0.1195,  0.0565,  0.0380, -0.0925, -0.2393,\n",
       "         0.0736,  0.2174, -0.0656,  0.1709,  0.0443,  0.2754,  0.1981,  0.2188,\n",
       "        -0.0488, -0.0566,  0.1292, -0.1487,  0.1959, -0.0485,  0.1271,  0.1461,\n",
       "         0.2358,  0.1565, -0.0595,  0.0253,  0.2583, -0.0058,  0.0583,  0.1067,\n",
       "         0.2441,  0.3999, -0.0985,  0.3110, -0.0759,  0.0475,  0.0098,  0.3147,\n",
       "        -0.2908, -0.0667,  0.0322,  0.0744,  0.1272, -0.0215, -0.0033, -0.0477,\n",
       "         0.0426,  0.2344,  0.1219,  0.0851,  0.0271, -0.0881, -0.0158, -0.0751,\n",
       "         0.0784,  0.1636, -0.0450, -0.1937, -0.1608,  0.0969,  0.1631, -0.1230,\n",
       "         0.2922,  0.0601,  0.1444,  0.0681,  0.0331,  0.1203,  0.0252,  0.1920],\n",
       "       device='cuda:0'), tensor([0.6206, 0.4829, 0.3208, 0.3118, 0.7598, 0.3921, 0.7993, 0.9736, 0.8447,\n",
       "        0.4077, 0.3198, 0.8950, 0.4556, 0.3291, 0.5913, 0.4299, 0.3787, 0.5552,\n",
       "        0.4858, 1.1934, 0.6729, 0.5151, 0.4490, 0.6035, 0.3186, 0.3179, 0.6606,\n",
       "        0.6299, 0.2961, 0.3374, 0.5215, 0.6318, 0.3748, 0.5728, 0.3972, 0.6816,\n",
       "        0.6362, 0.3699, 0.7134, 0.5366, 0.4705, 0.3816, 0.5469, 0.6504, 0.6406,\n",
       "        0.7749, 0.7793, 0.7588, 0.4568, 0.4407, 0.3821, 0.6694, 0.4316, 0.2976,\n",
       "        0.7188, 0.7222, 0.9209, 0.2776, 0.6064, 0.5132, 0.4082, 0.4888, 0.4260,\n",
       "        0.5889, 0.6016, 0.5137, 0.3477, 0.9312, 1.0293, 0.4873, 0.3538, 0.4819,\n",
       "        0.5562, 0.9741, 0.5444, 0.3877, 0.3291, 0.7114, 0.3245, 0.8276, 0.6504,\n",
       "        0.5220, 0.4070, 0.3162, 0.5366, 1.1445, 0.5103, 0.5654, 0.6240, 0.3704,\n",
       "        0.7144, 0.4087, 0.8110, 0.5967, 0.5718, 0.4333, 0.4775, 0.2568, 0.5205,\n",
       "        0.4944, 0.7319, 0.4377, 0.4226, 0.8481, 0.7910, 0.4192, 0.4658, 0.7979,\n",
       "        0.3645, 0.5708, 0.4099, 0.2925, 0.6245, 0.2993, 1.1631, 0.5605, 0.9536,\n",
       "        0.7339, 0.7417, 0.6714, 0.7207, 0.3489, 1.2939, 0.9697, 1.0771, 0.6890,\n",
       "        0.4214, 0.5137, 0.6489, 0.5176, 0.5049, 0.8281, 0.9678, 0.4556, 0.6416,\n",
       "        0.7373, 0.7471, 0.5967, 0.6680, 0.4446, 0.6602, 0.9819, 0.4609, 0.3508,\n",
       "        0.4934, 0.6670, 0.6865, 0.6763, 0.4458, 0.5571, 0.4526, 0.6943, 0.6890,\n",
       "        0.8428, 0.8301, 0.6753, 0.9883, 0.3745, 0.6567, 0.2988, 0.7642, 0.4187,\n",
       "        0.6201, 0.7271, 0.9844, 0.4097, 0.7124, 0.3091, 0.5645, 0.4915, 0.7144,\n",
       "        0.2842, 0.5244, 0.6929, 0.2976, 0.6421, 0.4343, 0.4475, 0.4539, 0.7202,\n",
       "        0.4749, 0.6929, 0.6187, 0.5718, 0.9297, 0.5225, 0.7163, 1.3984, 0.5762,\n",
       "        0.4253, 0.5703, 0.7261, 0.5596, 0.3113, 0.5640, 0.3647, 0.3213, 0.7710,\n",
       "        0.4790, 0.7344, 0.5757, 0.3455, 0.7197, 0.4568, 0.3032, 0.5601, 0.7988,\n",
       "        0.5659, 0.6792, 0.5576, 0.3379, 0.3955, 0.3472, 0.9868, 0.8091, 0.3335,\n",
       "        0.5669, 0.5952, 0.3376, 0.5264, 0.8306, 0.4988, 0.6753, 0.4280, 0.6802,\n",
       "        0.7881, 1.7217, 0.5254, 0.7930, 0.3958, 0.3105, 0.5146, 0.8198, 0.3613,\n",
       "        0.5972, 0.4866, 0.7002, 0.4231, 0.7744, 0.3096, 0.5352, 0.6885, 0.7129,\n",
       "        0.7075, 0.5010, 0.5957, 0.3638, 1.1973, 0.6670, 0.6919, 0.2188, 0.7925,\n",
       "        0.5249, 0.3276, 0.9058, 1.0938, 0.7598, 0.6094, 0.2905, 0.5488, 0.5122,\n",
       "        0.4214, 0.4485, 0.6670, 0.4045, 0.8838, 0.4358, 0.6558, 0.4805, 0.6758,\n",
       "        0.6582, 0.6450, 0.5181, 0.3345, 0.6104, 0.6343, 0.7729, 0.2769, 0.3887,\n",
       "        0.9893, 0.5947, 0.3447, 0.3303, 0.3250, 0.4019, 0.3142, 0.5493, 0.5767,\n",
       "        0.4536, 0.5195, 0.4580, 0.3655, 0.5015, 0.3613, 0.6958, 0.3853, 0.6719,\n",
       "        0.5806, 0.3115, 0.6533, 0.4329, 0.5425, 0.7212, 0.7700, 0.3462, 0.3333,\n",
       "        0.5034, 0.9341, 0.3274, 0.6118, 0.5283, 0.6079, 0.4878, 0.4338, 0.2615,\n",
       "        0.5044, 0.6680, 1.1865, 0.5903, 0.8252, 0.6470, 0.8984, 0.4385, 0.5078,\n",
       "        0.3391, 0.3757, 0.4275, 0.8208, 0.8110, 0.3591, 0.5396, 0.9248, 0.3333,\n",
       "        0.5474, 0.2524, 0.7632, 0.6445, 0.3267, 0.6377, 0.5889, 0.4141, 0.6392,\n",
       "        0.6245, 0.6880, 0.3958, 0.5103, 0.7407, 0.6182, 0.7188, 0.7847, 0.7432,\n",
       "        0.6924, 0.7588, 0.6680, 0.6494, 0.3228, 0.5229, 0.2617, 1.0029, 0.3589,\n",
       "        0.4851, 0.5259, 0.4187, 0.5483, 0.4387, 0.4919, 0.3569, 0.7910, 0.3528,\n",
       "        0.7700, 0.2676, 0.3987, 0.6284, 0.4587, 0.5439, 0.4460, 0.7568, 0.5991,\n",
       "        0.3892, 0.5615, 1.1582, 0.5283, 1.0088, 0.3438, 0.8257, 0.5640, 0.5708,\n",
       "        0.5391, 0.4636, 0.5449, 0.8208, 0.3154, 0.4817, 0.5449, 0.8193, 0.5093,\n",
       "        0.5347, 0.7812, 0.7769, 0.3945, 0.3118, 0.4106, 0.7549, 0.7612, 0.8154,\n",
       "        0.6064, 0.4604, 0.3667, 0.6094, 0.9048, 0.9399, 0.3711, 0.9067, 0.4917,\n",
       "        0.4438, 0.5688, 0.3308, 0.3184, 0.5708, 0.2668, 0.6182, 1.0059, 0.9209,\n",
       "        0.4468, 0.6089, 1.1836, 0.2830, 0.3167, 0.5010, 0.6274, 0.8896, 0.6880,\n",
       "        0.4272, 0.6543, 0.7729, 0.2332, 0.5537, 0.5400, 0.8799, 0.4875, 0.3547,\n",
       "        0.5298, 0.5439, 0.3901, 0.5161, 0.5859, 0.7876, 0.8188, 0.5625, 0.3516,\n",
       "        1.7842, 0.3611, 0.4153, 0.2798, 0.3958, 0.5674, 0.5430, 0.5718, 0.5537,\n",
       "        1.1104, 0.4155, 0.6646, 0.4248, 0.6475, 0.4141, 0.3867, 0.3845, 0.5312,\n",
       "        0.3215, 0.6245, 0.7139, 0.3853, 0.4392, 0.2957, 0.6924, 0.3845, 0.7632,\n",
       "        0.4858, 0.7456, 0.2920, 0.7998, 0.6523, 0.7466, 0.6895, 0.4934, 0.5142,\n",
       "        0.5728, 0.7295, 1.0381, 0.4077, 0.5620, 0.6177, 0.6182, 0.7178, 0.8042,\n",
       "        0.9346, 0.5737, 0.4561, 0.5254, 1.3438, 1.0605, 0.4319, 0.2849, 0.6074,\n",
       "        0.2932, 0.7827, 0.3472, 0.4939, 1.1357, 0.5547, 0.4827, 0.3274],\n",
       "       device='cuda:0'), tensor([-3.3032e-01,  4.9866e-02,  4.3701e-02, -9.6497e-02, -6.7529e-01,\n",
       "        -4.0222e-02, -3.9624e-01, -2.2400e-01, -1.2720e-01, -2.2546e-01,\n",
       "        -1.9006e-01, -1.2415e-01, -2.2717e-01, -1.6663e-01, -3.1519e-01,\n",
       "        -2.2095e-01, -1.5149e-01, -2.6855e-01, -2.2375e-01, -2.0483e-01,\n",
       "        -3.4692e-01, -4.4946e-01, -3.8745e-01, -3.2300e-01, -8.1970e-02,\n",
       "         3.5980e-02, -3.7061e-01, -2.4402e-01, -1.1267e-01, -3.0273e-01,\n",
       "        -3.0542e-01, -1.3037e-01, -1.0876e-01, -3.6865e-01, -1.6577e-01,\n",
       "        -5.6104e-01, -3.7451e-01, -1.4282e-01, -3.9746e-01, -2.3254e-01,\n",
       "        -3.2837e-01, -1.8433e-01, -1.3367e-01, -5.4541e-01, -3.5254e-01,\n",
       "        -4.1162e-01, -5.8838e-01, -8.4863e-01, -4.9146e-01, -1.6052e-01,\n",
       "        -2.3755e-01, -2.7905e-01, -1.2494e-01, -9.0088e-02, -5.2588e-01,\n",
       "        -4.3335e-01, -6.7871e-01,  1.1829e-01, -1.0162e-01, -2.0569e-01,\n",
       "        -3.1464e-02, -2.2998e-01, -2.5415e-01, -6.0352e-01,  1.6916e-04,\n",
       "        -3.6572e-01, -2.0959e-01, -4.5801e-01, -8.4961e-01, -1.9678e-01,\n",
       "        -7.3059e-02, -3.4473e-01, -2.9004e-01, -6.6748e-01, -1.4600e-01,\n",
       "        -1.4746e-01, -1.7249e-01, -2.4487e-01, -1.5405e-01,  6.9458e-02,\n",
       "        -6.8799e-01, -1.0388e-01,  7.0648e-03, -6.6956e-02, -2.3254e-01,\n",
       "         5.9021e-02, -1.7664e-01, -1.1462e-01, -2.8369e-01, -1.3904e-01,\n",
       "        -1.8555e-01, -4.3732e-02, -4.2798e-01, -1.9250e-01, -1.0309e-01,\n",
       "        -1.2830e-01, -1.7932e-01,  5.2490e-02, -2.0325e-01, -8.2397e-02,\n",
       "        -1.3196e-01, -1.6919e-01, -2.1973e-01, -3.8696e-01, -4.3530e-01,\n",
       "        -1.0907e-01, -9.8694e-02, -3.4497e-01,  4.8492e-02, -2.6416e-01,\n",
       "        -1.0773e-01, -9.2102e-02, -3.9526e-01,  3.5797e-02, -6.5625e-01,\n",
       "        -1.8359e-01, -7.2900e-01, -3.6285e-02, -6.5088e-01, -4.2798e-01,\n",
       "        -4.3774e-01, -3.9856e-02, -7.3389e-01, -9.1602e-01, -3.4692e-01,\n",
       "        -6.9629e-01, -6.5674e-02, -1.6077e-01, -1.3318e-01, -2.7197e-01,\n",
       "        -1.8982e-01, -1.4551e-01, -7.6855e-01, -3.9819e-01, -5.5078e-01,\n",
       "        -3.9429e-01, -2.2949e-01,  1.1841e-02, -2.4670e-01, -1.5613e-01,\n",
       "        -4.7412e-01, -4.3506e-01, -1.6943e-01,  2.6566e-02, -4.8804e-01,\n",
       "        -2.9858e-01, -1.5955e-01, -2.7148e-01, -1.6821e-01, -2.2852e-01,\n",
       "        -1.4319e-01, -3.6743e-01, -4.2969e-01, -6.5967e-01, -3.1006e-01,\n",
       "        -2.8125e-01, -6.8066e-01, -2.6245e-01, -4.7461e-01, -2.0471e-01,\n",
       "        -4.7241e-01, -1.9788e-01, -4.2236e-01, -2.6172e-01, -2.9077e-01,\n",
       "        -1.3940e-01, -3.4521e-01, -6.8298e-02, -4.5239e-01, -1.9836e-01,\n",
       "        -6.0059e-01, -1.2457e-01, -2.9517e-01, -3.3545e-01, -5.1971e-02,\n",
       "        -5.0684e-01, -6.2805e-02, -3.2983e-01, -9.8267e-02, -3.4399e-01,\n",
       "        -4.6265e-02, -6.8909e-02, -1.7822e-01, -1.6760e-01, -7.3975e-01,\n",
       "        -4.0466e-02, -7.6270e-01, -9.1846e-01, -3.6108e-01, -3.8354e-01,\n",
       "        -2.6074e-01, -5.8789e-01, -3.0957e-01, -7.6233e-02, -3.4131e-01,\n",
       "        -1.4380e-01, -1.2793e-01, -4.0479e-01, -2.8809e-01, -3.6108e-01,\n",
       "        -3.7988e-01, -4.9347e-02, -4.6973e-01, -2.8467e-01, -1.0138e-01,\n",
       "        -1.1383e-01, -3.0835e-01, -6.2207e-01, -3.3740e-01, -4.5923e-01,\n",
       "        -1.6455e-01, -1.2646e-01, -1.8176e-01, -4.4043e-01, -7.4219e-01,\n",
       "         1.6772e-01, -5.2441e-01, -4.1357e-01, -1.3379e-01, -1.8213e-01,\n",
       "        -6.5918e-01, -3.1616e-01, -1.4575e-01, -4.7803e-01, -5.2344e-01,\n",
       "        -2.7417e-01, -8.3691e-01, -1.4038e-01, -2.4695e-01, -2.2705e-01,\n",
       "        -1.0132e-01, -1.1066e-01, -6.8945e-01, -1.3123e-01, -2.9419e-01,\n",
       "        -2.7295e-01, -2.5146e-01, -2.1594e-01, -6.0352e-01,  2.2831e-03,\n",
       "        -1.2915e-01, -2.4158e-01, -6.1670e-01, -3.6011e-01, -3.2422e-01,\n",
       "        -4.2920e-01, -1.6809e-01, -5.8643e-01, -4.7510e-01, -3.2251e-01,\n",
       "        -7.1350e-02, -3.9893e-01, -2.8296e-01, -1.0004e-01, -7.9199e-01,\n",
       "        -7.9297e-01, -6.6504e-01, -3.7500e-01,  5.2643e-02, -2.1948e-01,\n",
       "        -3.4692e-01, -2.4750e-02, -1.6370e-01, -5.1465e-01, -1.0199e-01,\n",
       "        -2.3401e-01, -1.6602e-01, -2.0044e-01, -1.3013e-01,  1.1345e-02,\n",
       "        -3.8867e-01, -3.0298e-01, -3.4814e-01, -2.0044e-01, -1.9385e-01,\n",
       "        -3.6206e-01, -1.9214e-01, -9.1431e-02, -1.7468e-01, -4.6021e-01,\n",
       "        -4.3628e-01, -8.0994e-02, -1.1224e-01, -1.3000e-01, -1.1322e-01,\n",
       "        -1.4160e-01, -3.5376e-01, -4.0308e-01, -1.3354e-01, -1.6150e-01,\n",
       "        -1.6089e-01, -1.2915e-01, -1.6028e-01,  2.9480e-02, -3.8452e-01,\n",
       "        -4.1443e-02, -5.6299e-01, -6.1719e-01, -1.9946e-01, -3.2635e-03,\n",
       "        -1.4392e-01, -2.8442e-01, -2.2476e-02, -3.5864e-01, -8.8013e-02,\n",
       "        -1.5674e-01, -1.1292e-01, -7.4121e-01, -1.1090e-01, -4.3018e-01,\n",
       "        -1.5491e-01, -5.1697e-02, -3.6841e-01, -1.5540e-01,  1.0577e-01,\n",
       "        -2.8833e-01, -5.1172e-01, -9.6973e-01, -2.2668e-01, -2.9907e-01,\n",
       "        -4.6167e-01, -4.1528e-01, -1.2073e-01, -1.2781e-01, -1.9275e-01,\n",
       "        -1.0559e-01, -2.1094e-01, -4.0015e-01, -6.3574e-01, -1.5125e-01,\n",
       "        -1.9971e-01, -7.1240e-01, -1.7065e-01, -2.7222e-01, -2.3682e-01,\n",
       "        -1.4946e-02, -3.7354e-01, -1.1584e-01, -6.2683e-02, -1.5100e-01,\n",
       "        -1.7151e-01, -4.5947e-01, -4.6411e-01, -6.6846e-01, -1.2457e-01,\n",
       "        -1.5479e-01, -1.1359e-01, -3.5571e-01, -2.8345e-01, -2.4036e-01,\n",
       "        -2.3755e-01, -4.3237e-01, -1.6907e-01, -3.9966e-01, -5.6787e-01,\n",
       "        -1.3879e-01, -5.4785e-01,  2.6230e-02, -6.1865e-01, -6.5002e-02,\n",
       "        -3.1641e-01, -3.3740e-01, -1.8384e-01, -2.8931e-01, -8.8806e-02,\n",
       "        -2.5635e-01, -5.9357e-02, -3.3154e-01, -5.4901e-02, -4.0063e-01,\n",
       "        -1.5601e-01, -9.5276e-02, -3.3960e-01, -3.5913e-01, -4.4434e-01,\n",
       "        -6.8115e-02, -6.0107e-01, -2.0996e-01, -5.7922e-02, -2.5171e-01,\n",
       "        -9.2480e-01, -1.1334e-01, -5.9521e-01, -3.7598e-02, -5.8057e-01,\n",
       "        -2.4109e-01, -3.5352e-01, -7.5439e-02, -2.9248e-01, -3.9215e-02,\n",
       "        -6.6650e-01, -1.5845e-01, -3.2788e-01, -3.0151e-01, -2.7246e-01,\n",
       "        -3.7256e-01, -3.2324e-01, -4.2310e-01, -4.3140e-01, -6.6650e-02,\n",
       "        -1.1383e-01, -2.2476e-02, -3.6377e-01, -5.2930e-01, -6.8115e-01,\n",
       "        -1.4771e-01, -6.9153e-02, -1.0870e-01, -4.2212e-01, -4.8950e-01,\n",
       "        -6.1670e-01, -1.6467e-01, -3.4814e-01, -1.0327e-01, -1.6699e-01,\n",
       "        -2.4121e-01,  7.8552e-02, -8.9417e-02, -3.0811e-01,  6.8787e-02,\n",
       "        -4.6533e-01, -5.6738e-01, -6.6650e-01, -2.5195e-01, -5.5469e-01,\n",
       "        -6.7041e-01,  1.0986e-01, -1.3342e-01, -1.1938e-01, -2.4390e-01,\n",
       "        -7.4219e-01, -7.0752e-01,  1.8707e-02, -2.0642e-01, -4.0430e-01,\n",
       "        -1.2793e-01, -3.9624e-01, -4.8706e-02, -5.2100e-01, -3.0176e-01,\n",
       "        -2.4551e-02, -2.2021e-01, -3.6743e-01, -1.1304e-01, -9.7778e-02,\n",
       "        -4.1382e-01, -5.3711e-01, -4.2676e-01, -2.4939e-01,  2.9083e-02,\n",
       "        -1.1104e+00, -1.6675e-01, -1.3159e-01, -6.6284e-02, -1.7004e-01,\n",
       "        -2.3743e-01, -2.8979e-01, -1.3916e-01, -1.7578e-01, -8.4131e-01,\n",
       "        -2.3462e-01, -2.3206e-01, -5.0018e-02, -4.0625e-01, -1.2939e-01,\n",
       "        -1.0864e-01, -1.4490e-01, -2.0935e-02, -8.8135e-02, -6.1963e-01,\n",
       "        -2.0654e-01, -1.3513e-01, -6.2408e-02, -1.7908e-01, -2.8076e-01,\n",
       "        -4.6509e-02, -4.8071e-01, -2.5610e-01, -2.8735e-01, -1.6251e-02,\n",
       "        -3.0469e-01, -1.7651e-01, -8.2092e-02, -1.8738e-01, -2.2070e-01,\n",
       "        -4.6826e-01, -3.4473e-01, -4.3652e-01, -1.6724e-01, -1.9128e-01,\n",
       "        -4.8242e-01, -2.8345e-01, -1.8542e-01, -7.1729e-01, -3.9795e-01,\n",
       "        -3.6572e-01, -2.6440e-01, -1.3916e-01, -1.3464e-01, -1.1768e+00,\n",
       "        -9.2383e-01, -1.2988e-01,  1.2830e-01, -2.4451e-01,  2.0248e-02,\n",
       "        -1.4343e-01, -3.9032e-02, -3.0078e-01,  6.6162e-02, -3.8208e-01,\n",
       "        -3.7695e-01, -7.5195e-02], device='cuda:0'), tensor([[[ 0.0214,  0.0554],\n",
       "         [ 0.1965,  0.1343],\n",
       "         [ 0.0563,  0.1332],\n",
       "         ...,\n",
       "         [-0.0089, -0.0493],\n",
       "         [-0.0609, -0.0565],\n",
       "         [ 0.1775,  0.1448]],\n",
       "\n",
       "        [[ 0.0201, -0.0822],\n",
       "         [ 0.1063,  0.0501],\n",
       "         [ 0.1725,  0.1716],\n",
       "         ...,\n",
       "         [ 0.1331,  0.1711],\n",
       "         [-0.0099,  0.0205],\n",
       "         [ 0.1428, -0.0825]],\n",
       "\n",
       "        [[ 0.0548,  0.0277],\n",
       "         [-0.0942,  0.0267],\n",
       "         [ 0.2312,  0.0282],\n",
       "         ...,\n",
       "         [ 0.0071,  0.1177],\n",
       "         [-0.0260,  0.0163],\n",
       "         [-0.1019, -0.1031]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2267, -0.0589],\n",
       "         [-0.0163,  0.2529],\n",
       "         [-0.0173,  0.0948],\n",
       "         ...,\n",
       "         [-0.0413, -0.0154],\n",
       "         [-0.1207,  0.0447],\n",
       "         [ 0.0706,  0.0398]],\n",
       "\n",
       "        [[-0.0027, -0.0516],\n",
       "         [ 0.0590, -0.0947],\n",
       "         [ 0.0360,  0.1412],\n",
       "         ...,\n",
       "         [-0.0269, -0.0478],\n",
       "         [-0.0864, -0.0166],\n",
       "         [-0.1039,  0.0642]],\n",
       "\n",
       "        [[ 0.0134, -0.0243],\n",
       "         [ 0.0797, -0.1887],\n",
       "         [ 0.0174,  0.2292],\n",
       "         ...,\n",
       "         [ 0.3982, -0.2153],\n",
       "         [-0.0564, -0.1382],\n",
       "         [-0.0852,  0.0184]]], device='cuda:0'), tensor([-0.0463,  0.2317,  0.3276,  0.3315,  0.1085,  0.1359,  0.0092,  0.1390,\n",
       "         0.0305, -0.0459,  0.1276, -0.0480,  0.0301, -0.1785, -0.2917, -0.0530,\n",
       "         0.0449, -0.0964,  0.2455,  0.1161, -0.0113,  0.0034,  0.3586, -0.0685,\n",
       "         0.0423,  0.0656,  0.4834,  0.0346,  0.0958,  0.0228,  0.3274,  0.2766,\n",
       "        -0.0681,  0.2195,  0.3420, -0.0139,  0.2539, -0.0284, -0.0601,  0.3069,\n",
       "         0.0034,  0.2296,  0.1632,  0.2177,  0.3960, -0.0168,  0.3088,  0.3909,\n",
       "         0.0303,  0.0630,  0.0321,  0.2216,  0.2402,  0.0077,  0.2939, -0.0091,\n",
       "         0.0104,  0.0453, -0.0153, -0.0074, -0.0663, -0.1022, -0.0954,  0.0804,\n",
       "         0.0917, -0.0056,  0.1919,  0.3674,  0.1591,  0.2903,  0.1028,  0.1888,\n",
       "         0.0908,  0.1566,  0.3118,  0.0102,  0.1718, -0.0093, -0.0757,  0.0489,\n",
       "        -0.2052,  0.0258,  0.0626, -0.0380,  0.2905,  0.2354,  0.0346,  0.1613,\n",
       "         0.0635,  0.3438,  0.0019,  0.1335,  0.2881,  0.0892,  0.0620, -0.1088,\n",
       "        -0.0247,  0.0120,  0.0742,  0.1569,  0.2935,  0.0394,  0.0532,  0.1982,\n",
       "         0.0169,  0.1616,  0.1076,  0.3005, -0.1537,  0.0519,  0.0991,  0.2299,\n",
       "        -0.1305,  0.1204,  0.1863, -0.0112,  0.0884,  0.0428,  0.0640,  0.0547,\n",
       "        -0.0820, -0.1406,  0.0633,  0.1603, -0.0527, -0.2308,  0.2260,  0.0030,\n",
       "        -0.1140, -0.0203, -0.0368, -0.0330, -0.1443, -0.1677,  0.2451, -0.0396,\n",
       "         0.1147,  0.2832,  0.3064,  0.2399,  0.0463, -0.0188,  0.0030,  0.0141,\n",
       "        -0.0273,  0.0389,  0.0083,  0.3503, -0.0279,  0.2095, -0.0092,  0.3206,\n",
       "         0.3083,  0.4348,  0.1094,  0.0338, -0.0037,  0.2578,  0.1410,  0.1692,\n",
       "         0.2937,  0.3430,  0.2323, -0.0544,  0.1177,  0.1517,  0.2563, -0.1198,\n",
       "         0.2656,  0.1218,  0.1449, -0.1186, -0.0105,  0.0845,  0.3591,  0.0528,\n",
       "         0.0546, -0.1141,  0.1068,  0.2268, -0.0108, -0.0858, -0.0820, -0.0682,\n",
       "         0.2715,  0.0282,  0.1054,  0.2744,  0.4167,  0.0427,  0.1523, -0.1877,\n",
       "         0.0904,  0.2291, -0.0891,  0.2822,  0.1758,  0.1503,  0.2705, -0.0728,\n",
       "        -0.0764, -0.1147,  0.1089,  0.0100,  0.1306,  0.2327,  0.1152,  0.0005,\n",
       "        -0.0201,  0.1539,  0.1116,  0.2803,  0.2607,  0.1161, -0.0122,  0.1981,\n",
       "         0.0761,  0.0476,  0.1490,  0.2878,  0.0101, -0.0675, -0.0277, -0.1262,\n",
       "         0.0592,  0.1523,  0.1829,  0.0655,  0.0524,  0.0168, -0.0299,  0.1250,\n",
       "         0.0850,  0.0815,  0.0029,  0.0908, -0.0411,  0.2312, -0.0663,  0.3523,\n",
       "        -0.2490,  0.1427, -0.0872,  0.2092,  0.1833,  0.2073,  0.0872,  0.0074,\n",
       "        -0.0701,  0.2939, -0.0596,  0.0457, -0.0308,  0.2898,  0.0371,  0.2771,\n",
       "        -0.0580, -0.0166,  0.2908,  0.1461, -0.0420,  0.0850,  0.1976, -0.0925,\n",
       "         0.3718,  0.0940, -0.1312,  0.0043,  0.1785,  0.2283,  0.2610, -0.0547,\n",
       "         0.0033,  0.3335,  0.1029,  0.0525, -0.1196,  0.0020,  0.3123,  0.1114,\n",
       "        -0.0428, -0.1333,  0.2360,  0.1143, -0.0055,  0.0038,  0.1720,  0.3499,\n",
       "         0.1008,  0.1533,  0.4951, -0.0349, -0.0535,  0.0314, -0.0342, -0.1913,\n",
       "        -0.1134,  0.1093,  0.0188,  0.3303,  0.1528, -0.1426,  0.0431, -0.0307,\n",
       "         0.1936,  0.0154,  0.2375,  0.3726, -0.0809, -0.0466,  0.2429,  0.3896,\n",
       "         0.1379,  0.1098, -0.0040, -0.0517,  0.2510,  0.0887,  0.0572,  0.1971,\n",
       "        -0.0201,  0.2642,  0.1282,  0.1274,  0.1848,  0.0657, -0.0180,  0.1866,\n",
       "         0.0054,  0.4219, -0.1666,  0.0453,  0.1919,  0.1350,  0.0156, -0.0892,\n",
       "        -0.0098,  0.2810,  0.2803, -0.3796, -0.0063, -0.0296, -0.1387,  0.2930,\n",
       "        -0.0202, -0.2208,  0.0308,  0.2783,  0.0006,  0.0562, -0.0167,  0.1182,\n",
       "        -0.0165,  0.0202,  0.3225, -0.0096,  0.3679,  0.0218,  0.0380, -0.0702,\n",
       "        -0.0065, -0.0170,  0.0277,  0.0922,  0.0623, -0.0671,  0.2351,  0.0375,\n",
       "         0.2761,  0.1002,  0.3782,  0.1344,  0.0778, -0.1226,  0.0801,  0.0488,\n",
       "        -0.0930,  0.0292,  0.2462,  0.0248,  0.1174,  0.2068,  0.0087, -0.0245,\n",
       "         0.2056,  0.1761,  0.1295,  0.1346,  0.3171,  0.0202,  0.1218,  0.0050,\n",
       "         0.2356,  0.2001,  0.1437,  0.0227, -0.0190, -0.0847, -0.1138, -0.0908,\n",
       "         0.1553,  0.4426,  0.4609,  0.2668,  0.2145,  0.1382,  0.0082, -0.0788,\n",
       "         0.0016, -0.0248,  0.2927,  0.0064,  0.0117,  0.1208,  0.0429,  0.1210,\n",
       "         0.1836,  0.2285,  0.0599, -0.0555,  0.1360,  0.1963,  0.0204, -0.0583,\n",
       "        -0.0033,  0.2842,  0.2700,  0.1781,  0.0576,  0.3569,  0.3694,  0.1234,\n",
       "         0.3345,  0.3889,  0.2637, -0.1033,  0.0118,  0.0450, -0.0401,  0.0852,\n",
       "         0.1769,  0.1577,  0.1716, -0.0453, -0.0142,  0.0550,  0.0748,  0.0317,\n",
       "         0.1420, -0.0067,  0.0546,  0.0408,  0.1276,  0.1243,  0.0145,  0.0396,\n",
       "         0.4451,  0.1860, -0.1045,  0.0856, -0.0919,  0.2302,  0.0768,  0.2264,\n",
       "         0.1631, -0.2292,  0.1261,  0.0213,  0.0662, -0.0198, -0.0414,  0.3091,\n",
       "        -0.0714, -0.0573,  0.0870,  0.3442,  0.1808,  0.1131, -0.1219,  0.0113,\n",
       "         0.0776,  0.2021,  0.0818, -0.0686,  0.0237,  0.1198,  0.1387,  0.0058,\n",
       "         0.0657,  0.0562, -0.0344, -0.0106,  0.1411,  0.1333,  0.0217, -0.0679,\n",
       "         0.2568, -0.4351, -0.0083, -0.1151,  0.3206, -0.0183,  0.4060, -0.0569,\n",
       "         0.0432, -0.0091,  0.1969,  0.0927, -0.1661,  0.0093,  0.1433,  0.0052],\n",
       "       device='cuda:0'), tensor([0.5889, 0.2610, 0.3306, 0.3723, 0.3132, 0.3481, 0.5278, 0.4551, 0.4729,\n",
       "        0.6313, 0.5068, 0.6802, 0.5127, 0.9526, 0.7178, 0.5112, 0.5137, 1.0283,\n",
       "        0.3311, 0.5703, 0.5552, 0.4597, 0.2866, 1.0127, 0.7271, 0.6250, 0.2703,\n",
       "        0.4399, 0.4690, 0.8232, 0.2998, 0.2905, 0.5850, 0.3496, 0.2507, 0.5039,\n",
       "        0.3232, 0.5674, 0.7549, 0.2515, 0.7471, 0.3423, 0.3420, 0.2874, 0.3054,\n",
       "        0.5684, 0.3289, 0.2458, 0.6011, 0.5298, 0.4866, 0.2661, 0.3235, 0.6099,\n",
       "        0.3489, 0.5522, 0.5552, 0.6035, 0.7812, 0.6743, 0.6152, 0.6523, 0.8467,\n",
       "        0.6040, 0.3862, 0.4829, 0.2817, 0.2759, 0.5747, 0.2715, 0.4663, 0.3169,\n",
       "        0.4304, 0.2661, 0.2830, 0.6436, 0.4094, 0.5933, 0.8638, 0.3916, 0.8838,\n",
       "        0.4995, 0.6050, 0.8540, 0.2507, 0.2891, 0.6899, 0.4304, 0.4282, 0.2832,\n",
       "        0.6816, 0.4182, 0.2952, 0.3987, 0.5532, 0.8428, 0.6812, 0.4363, 0.6885,\n",
       "        0.4399, 0.3313, 0.4941, 0.6401, 0.3132, 0.4771, 0.4880, 0.4458, 0.2891,\n",
       "        0.6875, 0.4587, 0.2644, 0.3442, 0.6396, 0.8457, 0.3003, 0.5239, 0.4006,\n",
       "        0.6338, 0.7441, 0.5342, 0.7881, 0.8721, 0.7617, 0.3337, 0.5620, 1.0117,\n",
       "        0.4128, 0.5479, 0.6138, 0.4648, 0.6680, 0.5425, 0.7222, 0.7490, 0.3430,\n",
       "        0.7559, 0.6777, 0.3599, 0.2979, 0.5181, 0.6899, 0.9341, 0.5908, 0.5889,\n",
       "        0.6318, 0.6836, 0.6201, 0.2408, 0.8032, 0.3970, 0.7520, 0.3406, 0.2754,\n",
       "        0.2617, 0.3247, 0.3452, 0.6094, 0.3044, 0.3247, 0.2964, 0.2988, 0.2900,\n",
       "        0.2971, 0.8433, 0.4326, 0.3599, 0.3677, 0.6846, 0.2622, 0.3479, 0.3486,\n",
       "        0.6172, 0.6753, 0.3982, 0.2228, 0.4102, 0.5020, 0.9775, 0.3481, 0.5938,\n",
       "        0.5815, 0.7549, 0.8037, 0.6494, 0.3005, 0.5703, 0.3452, 0.2808, 0.2644,\n",
       "        0.5854, 0.3218, 0.9883, 0.5132, 0.3047, 0.7891, 0.2598, 0.4292, 0.6152,\n",
       "        0.2217, 0.7354, 0.6587, 0.8579, 0.2788, 0.7910, 0.3254, 0.2786, 0.9058,\n",
       "        0.4575, 0.5928, 0.4185, 0.4543, 0.2769, 0.3794, 0.4221, 0.7095, 0.2913,\n",
       "        0.4829, 0.4500, 0.2832, 0.3247, 0.6230, 0.5649, 0.7002, 0.7554, 0.4697,\n",
       "        0.9199, 0.2788, 0.4199, 0.5620, 0.7900, 0.9053, 0.3997, 0.4033, 0.3665,\n",
       "        0.6392, 0.6338, 0.5781, 0.2830, 0.5737, 0.3088, 0.7803, 0.4270, 0.5928,\n",
       "        0.2900, 0.3284, 0.3066, 0.7627, 0.5229, 0.5366, 0.2751, 0.9976, 0.4883,\n",
       "        0.7305, 0.4395, 0.6812, 0.2460, 0.5776, 0.6592, 0.3049, 0.5659, 0.7017,\n",
       "        0.3950, 0.2274, 0.7676, 0.2563, 0.5962, 0.7896, 0.5625, 0.2306, 0.3386,\n",
       "        0.2942, 0.6133, 0.7339, 0.2776, 0.2998, 0.3662, 0.6465, 0.9067, 0.2979,\n",
       "        0.3684, 0.5327, 0.9570, 0.2866, 1.0225, 0.5537, 0.9512, 0.3271, 0.3411,\n",
       "        0.4580, 0.6880, 0.3052, 0.6045, 0.8906, 0.4519, 0.6216, 0.9253, 0.6421,\n",
       "        0.3594, 0.5073, 0.2957, 0.2632, 0.9741, 0.4504, 0.8047, 0.3892, 0.8364,\n",
       "        0.3127, 0.2649, 0.7510, 0.6235, 0.2961, 0.2881, 0.4121, 0.3560, 0.4492,\n",
       "        0.5405, 0.2937, 0.4888, 0.5200, 0.3103, 0.7759, 0.3530, 0.4797, 0.3606,\n",
       "        0.3508, 0.5605, 0.4668, 0.3213, 0.3884, 0.2416, 0.7598, 0.4900, 0.3477,\n",
       "        0.4761, 0.6997, 0.7793, 0.5728, 0.2443, 0.2976, 0.8164, 0.5942, 0.8794,\n",
       "        0.9590, 0.2639, 0.6431, 0.8418, 0.4985, 0.3687, 0.6543, 0.3884, 0.7075,\n",
       "        0.4517, 0.9224, 0.7031, 0.2954, 0.7090, 0.2467, 0.4756, 0.3071, 0.5791,\n",
       "        1.0254, 0.6309, 0.3982, 0.3274, 1.0596, 1.0010, 0.3413, 0.4399, 0.2922,\n",
       "        0.7026, 0.2881, 0.4766, 0.3975, 0.7437, 0.4561, 0.4915, 0.8735, 0.4463,\n",
       "        0.2776, 0.4963, 0.3235, 0.3750, 0.4695, 0.5132, 0.3779, 0.3289, 0.3435,\n",
       "        0.4380, 0.1956, 0.4695, 0.4224, 0.5962, 0.2549, 0.3228, 0.4338, 0.4480,\n",
       "        0.7866, 0.9092, 0.6519, 1.0742, 0.3416, 0.3093, 0.2561, 0.3105, 0.2834,\n",
       "        0.3391, 0.5854, 0.7871, 0.6724, 0.6333, 0.3103, 0.6606, 0.8623, 0.5669,\n",
       "        0.5024, 0.2656, 0.2588, 0.2974, 0.7661, 0.6909, 0.4382, 0.2625, 0.6738,\n",
       "        0.6519, 0.7603, 0.2742, 0.3542, 0.2996, 0.4019, 0.2559, 0.2900, 0.3611,\n",
       "        0.2517, 0.2664, 0.2810, 0.8701, 0.5317, 0.4021, 0.4641, 0.5034, 0.3347,\n",
       "        0.5010, 0.3181, 0.5542, 0.7861, 0.5317, 0.4565, 0.5293, 0.4561, 0.5762,\n",
       "        0.5117, 1.0518, 0.2927, 0.2683, 0.4917, 0.6812, 0.2722, 0.3213, 0.7622,\n",
       "        0.5581, 0.9658, 0.2883, 0.4084, 0.3135, 0.4253, 0.8315, 0.4976, 0.6636,\n",
       "        0.4973, 0.4478, 0.6489, 0.2573, 0.7476, 1.0039, 0.3789, 0.2573, 0.3411,\n",
       "        0.3376, 0.8486, 0.6274, 0.8208, 0.3442, 0.4707, 0.8018, 0.5425, 0.2983,\n",
       "        0.5771, 0.5449, 0.4739, 0.5024, 0.5054, 0.5894, 0.3130, 0.4675, 0.4211,\n",
       "        0.7271, 0.3618, 0.8433, 0.5586, 1.0420, 0.3118, 0.5825, 0.2319, 0.5947,\n",
       "        0.6699, 0.6953, 0.3188, 0.4485, 1.5283, 0.8496, 0.4807, 0.7529],\n",
       "       device='cuda:0'), tensor([-4.5898e-01, -1.0840e-01, -1.1945e-01, -7.2815e-02, -2.7734e-01,\n",
       "        -1.1395e-01, -5.4150e-01, -2.9883e-01, -4.5947e-01, -5.2490e-01,\n",
       "        -3.4424e-02, -5.5225e-01, -3.2642e-01, -8.1738e-01, -5.6787e-01,\n",
       "        -4.0649e-01, -1.9519e-01, -7.4023e-01, -8.7463e-02, -2.2266e-01,\n",
       "        -4.2847e-01, -3.9673e-01, -8.3252e-02, -7.3584e-01, -5.3516e-01,\n",
       "        -3.4595e-01, -5.9967e-02, -3.2983e-01, -2.3706e-01, -2.9126e-01,\n",
       "         3.5187e-02, -8.3923e-02, -1.9519e-01, -7.2632e-02, -1.1157e-01,\n",
       "        -1.5576e-01,  8.5526e-03, -5.4785e-01, -6.7432e-01, -1.6296e-02,\n",
       "        -5.1904e-01,  3.5334e-04, -6.0974e-02, -4.2786e-02, -1.0307e-02,\n",
       "        -5.6494e-01,  8.3008e-03,  1.7197e-02, -4.0503e-01, -2.3438e-01,\n",
       "        -3.3130e-01, -1.6663e-01,  2.0813e-02, -5.1758e-01, -1.5930e-01,\n",
       "        -2.5928e-01, -2.9785e-01, -4.3677e-01, -4.3311e-01, -4.8901e-01,\n",
       "        -5.3369e-01, -5.6543e-01, -8.8232e-01, -2.8003e-01,  7.1533e-02,\n",
       "        -3.0688e-01, -1.7615e-01, -7.8430e-02, -2.0996e-01, -1.7197e-02,\n",
       "        -2.5146e-01, -1.3538e-01, -2.1777e-01, -1.6345e-01, -8.3923e-02,\n",
       "        -3.0469e-01, -2.2595e-01, -4.2773e-01, -6.3916e-01, -3.8965e-01,\n",
       "        -8.4424e-01, -3.4497e-01, -3.3154e-01, -3.5498e-01, -1.6516e-01,\n",
       "        -1.4612e-01, -3.0884e-01, -3.7451e-01, -2.8247e-01, -4.6783e-02,\n",
       "        -5.0879e-01, -6.0089e-02,  1.4595e-02, -1.2073e-01, -3.4326e-01,\n",
       "        -8.6328e-01, -6.8359e-01, -3.0737e-01, -2.4829e-01, -2.7197e-01,\n",
       "        -7.0312e-02, -2.5366e-01, -2.8735e-01, -1.5450e-02, -3.1641e-01,\n",
       "        -1.1499e-01, -6.5552e-02, -2.7634e-02, -7.9248e-01, -3.3496e-01,\n",
       "        -1.3965e-01, -7.5623e-02, -2.6440e-01, -2.2766e-01, -8.7280e-02,\n",
       "        -2.8418e-01, -3.4448e-01, -4.2896e-01, -2.5928e-01, -1.8201e-01,\n",
       "        -6.4600e-01, -5.5908e-01, -3.4790e-01, -1.0010e-01, -6.6064e-01,\n",
       "        -9.4873e-01, -8.4351e-02, -3.1567e-01, -6.3135e-01, -6.1914e-01,\n",
       "        -6.7773e-01, -5.5615e-01, -2.7246e-01, -3.4326e-01, -9.1614e-02,\n",
       "        -5.5762e-01, -4.8364e-01, -7.3425e-02,  1.6312e-02, -2.0862e-01,\n",
       "        -4.0112e-01, -6.1475e-01, -5.4346e-01, -3.2617e-01, -7.7002e-01,\n",
       "        -3.7427e-01, -4.2871e-01, -1.4111e-01, -3.2568e-01, -1.3953e-01,\n",
       "        -7.3438e-01,  1.6907e-01, -2.9556e-02,  2.7435e-02, -1.8323e-01,\n",
       "        -2.1820e-02, -3.9624e-01,  5.7526e-02,  2.2980e-02, -1.2146e-01,\n",
       "        -6.3416e-02, -7.2510e-02, -6.8970e-02, -2.9419e-01, -1.4624e-01,\n",
       "         7.8735e-03, -1.2329e-01, -6.4355e-01, -1.5991e-02, -4.9866e-02,\n",
       "         2.2156e-02, -6.0693e-01, -5.8105e-01, -5.8014e-02, -1.3367e-02,\n",
       "        -1.5845e-01, -6.9824e-02, -8.4766e-01,  2.7115e-02, -2.0544e-01,\n",
       "        -4.4653e-01, -6.6406e-01, -6.7383e-01, -5.4590e-01,  1.2047e-02,\n",
       "        -8.4863e-01, -2.5543e-02, -5.2246e-02, -1.1975e-01, -4.3896e-01,\n",
       "        -3.0441e-02, -1.0156e+00, -2.7612e-01, -8.2275e-02, -5.8984e-01,\n",
       "        -1.2732e-01, -9.9487e-02, -2.0190e-01,  6.2141e-03, -7.0947e-01,\n",
       "        -3.7573e-01, -8.5986e-01, -8.4167e-02, -5.0537e-01, -4.6204e-02,\n",
       "        -5.2094e-02, -3.8354e-01, -4.7192e-01, -4.8291e-01, -3.5010e-01,\n",
       "        -3.1421e-01, -1.1060e-01, -2.9877e-02, -1.2158e-01, -1.8225e-01,\n",
       "         7.8812e-03, -2.5558e-02, -3.0054e-01, -6.6895e-02,  8.0872e-03,\n",
       "        -5.2197e-01, -5.5762e-01, -8.1201e-01, -7.7393e-01, -3.1226e-01,\n",
       "        -2.4512e-01, -1.1066e-01, -2.6807e-01, -3.0762e-01, -2.6343e-01,\n",
       "        -4.2554e-01, -3.0151e-01, -1.7737e-01, -8.7280e-02, -4.4238e-01,\n",
       "        -2.8735e-01, -4.8364e-01,  9.0714e-03, -8.8086e-01, -4.2877e-02,\n",
       "        -4.2188e-01, -2.4036e-01, -5.4736e-01, -5.5145e-02, -1.1304e-01,\n",
       "        -1.2415e-01, -1.3184e-01, -4.9780e-01, -4.1870e-01, -3.4210e-02,\n",
       "        -6.9580e-01, -4.8413e-01, -4.1309e-01, -5.4138e-02, -4.8657e-01,\n",
       "        -1.4075e-01, -1.9531e-01, -3.9746e-01, -9.3384e-02,  2.2568e-02,\n",
       "        -6.5625e-01, -3.6523e-01, -8.8959e-03, -8.3545e-01, -1.1328e-01,\n",
       "        -1.1926e-01, -4.4604e-01, -4.8730e-01, -5.9601e-02, -1.0980e-01,\n",
       "        -1.1725e-01, -6.1768e-01, -4.8926e-01, -9.7122e-03, -1.5771e-01,\n",
       "        -1.3574e-01, -5.0537e-01, -3.7500e-01, -3.7140e-02, -1.8600e-02,\n",
       "        -5.1367e-01, -4.6411e-01, -1.2798e-03, -2.6831e-01, -4.2383e-01,\n",
       "        -3.8452e-01, -2.0837e-01,  7.6477e-02, -2.5220e-01, -2.3572e-01,\n",
       "        -2.3117e-02, -5.4248e-01, -6.9922e-01, -5.1074e-01, -5.6982e-01,\n",
       "        -5.0244e-01, -2.5269e-01, -5.1941e-02, -2.2058e-01, -1.2323e-01,\n",
       "        -1.4832e-01, -8.2324e-01, -2.7539e-01, -5.0586e-01, -1.0950e-01,\n",
       "        -6.8457e-01, -3.7079e-02, -1.2622e-01, -3.3740e-01, -6.4502e-01,\n",
       "        -2.1683e-02, -8.9340e-03, -2.0129e-01, -1.1230e-01, -3.6597e-01,\n",
       "        -3.1177e-01, -1.2952e-01, -4.1553e-01, -3.7769e-01, -1.0400e-01,\n",
       "        -2.2339e-01, -8.3496e-02, -9.7229e-02, -1.3220e-01, -1.3318e-01,\n",
       "        -1.6089e-01, -5.9180e-01, -1.8030e-01, -1.2939e-01,  4.5990e-02,\n",
       "        -6.7139e-01, -2.3230e-01, -1.0968e-01, -2.8979e-01, -2.1802e-01,\n",
       "        -6.7627e-01, -4.6045e-01, -3.8452e-02,  1.8127e-02, -5.0000e-01,\n",
       "        -4.0161e-01, -6.2549e-01, -4.7119e-01, -1.5479e-01, -6.7432e-01,\n",
       "        -5.1758e-01, -2.2742e-01, -3.1952e-02, -4.7656e-01, -3.1860e-02,\n",
       "        -3.6475e-01, -2.4878e-01, -5.7422e-01, -5.1611e-01, -4.2000e-03,\n",
       "        -3.7012e-01, -1.7297e-01, -3.0688e-01,  7.7881e-02, -4.4824e-01,\n",
       "        -4.4287e-01, -6.2793e-01,  2.6657e-02, -7.7454e-02, -3.4644e-01,\n",
       "        -7.4023e-01, -1.0217e-01, -3.2349e-01, -9.3567e-02, -5.2246e-01,\n",
       "         5.8685e-02, -1.6479e-01, -2.5299e-02, -5.1318e-01, -3.9868e-01,\n",
       "         3.8971e-02, -8.1592e-01, -4.3311e-01, -8.5571e-02, -5.2832e-01,\n",
       "        -6.5186e-02, -8.8623e-02, -3.2910e-01, -2.5562e-01, -1.0419e-01,\n",
       "        -2.3285e-02, -6.7688e-02, -2.6074e-01, -1.2793e-01, -1.1401e-01,\n",
       "        -1.6553e-01, -4.4849e-01, -4.4098e-02,  8.4925e-04, -1.5234e-01,\n",
       "        -4.4006e-02, -6.6309e-01, -8.7402e-01, -7.6123e-01, -7.8809e-01,\n",
       "         7.5867e-02,  1.0300e-02,  1.0010e-01, -1.3330e-01, -1.3293e-01,\n",
       "         2.0630e-01, -3.9307e-01, -7.7344e-01, -7.5830e-01, -6.8018e-01,\n",
       "        -4.7119e-02, -6.6260e-01, -2.2986e-01,  2.5742e-02, -2.1545e-01,\n",
       "        -1.6565e-01, -1.6589e-01, -1.7712e-01, -1.6760e-01, -5.1416e-01,\n",
       "        -1.4893e-01, -9.0088e-02, -4.0405e-01, -5.0732e-01, -4.3701e-01,\n",
       "        -4.8126e-02,  3.2495e-01, -1.7822e-01, -3.3228e-01, -2.3651e-02,\n",
       "        -7.2937e-02, -1.7151e-01, -2.4048e-02, -1.3229e-02, -7.4768e-02,\n",
       "        -7.2949e-01, -5.7275e-01, -8.8257e-02, -4.8145e-01, -1.4709e-01,\n",
       "        -1.0828e-01, -2.4927e-01, -7.9041e-02, -4.9805e-01, -3.0127e-01,\n",
       "        -2.5562e-01, -1.7969e-01, -5.0586e-01, -1.9617e-01, -5.0684e-01,\n",
       "        -3.4473e-01, -3.2520e-01, -1.3062e-01, -1.6510e-02, -2.7222e-01,\n",
       "        -4.4263e-01,  3.7079e-02, -3.7537e-02, -8.9600e-01, -1.1017e-01,\n",
       "        -8.8330e-01, -1.1017e-01, -1.2042e-01, -1.5808e-01, -1.2421e-01,\n",
       "        -5.2393e-01, -4.4653e-01, -3.1079e-01, -3.7012e-01, -5.0000e-01,\n",
       "        -4.3848e-01, -1.0522e-01, -7.9395e-01, -5.7422e-01,  1.8616e-03,\n",
       "        -5.5634e-02,  1.1780e-01, -4.3182e-02, -7.1533e-01, -5.4248e-01,\n",
       "        -3.5791e-01, -9.9487e-02, -3.1494e-01, -7.5439e-01, -4.4141e-01,\n",
       "        -2.2363e-01,  2.6989e-03, -6.0400e-01, -4.1211e-01, -3.0908e-01,\n",
       "        -5.6445e-01, -4.8828e-01, -1.7749e-01, -1.6260e-01, -3.6206e-01,\n",
       "        -4.9829e-01,  1.4966e-01, -5.2881e-01, -6.7480e-01, -7.9492e-01,\n",
       "        -5.0018e-02, -3.6914e-01, -3.3169e-03, -1.7224e-01, -2.8394e-01,\n",
       "        -3.3911e-01, -5.8777e-02, -2.6270e-01, -9.2969e-01, -6.4502e-01,\n",
       "        -1.9421e-01, -4.2871e-01], device='cuda:0'), tensor([[[-6.1279e-02,  2.9617e-02],\n",
       "         [ 1.1658e-01, -2.5562e-01],\n",
       "         [ 1.1493e-01,  8.0933e-02],\n",
       "         ...,\n",
       "         [-1.0645e-01, -8.4167e-02],\n",
       "         [-2.6587e-01,  2.5830e-01],\n",
       "         [ 5.9128e-03,  1.7593e-02]],\n",
       "\n",
       "        [[-1.2109e-01, -2.0447e-01],\n",
       "         [ 1.3977e-01,  8.6975e-02],\n",
       "         [ 3.4451e-05,  6.4575e-02],\n",
       "         ...,\n",
       "         [ 1.0754e-01,  1.1145e-01],\n",
       "         [-3.6804e-02, -4.9866e-02],\n",
       "         [ 4.6326e-02, -3.6240e-03]],\n",
       "\n",
       "        [[-2.9358e-02,  5.3436e-02],\n",
       "         [ 3.1030e-01, -1.5552e-01],\n",
       "         [ 9.9976e-02,  6.8665e-02],\n",
       "         ...,\n",
       "         [-2.1255e-02,  2.1378e-02],\n",
       "         [ 5.3528e-02,  5.6534e-03],\n",
       "         [-1.2932e-03,  1.3867e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-4.2236e-02, -7.3608e-02],\n",
       "         [-2.7539e-01,  6.4148e-02],\n",
       "         [ 1.4832e-01, -1.1761e-01],\n",
       "         ...,\n",
       "         [-6.2286e-02, -7.6599e-02],\n",
       "         [ 9.2407e-02,  1.3440e-01],\n",
       "         [ 8.9874e-03,  2.8641e-02]],\n",
       "\n",
       "        [[ 7.5867e-02, -5.1384e-03],\n",
       "         [-1.3452e-01,  2.3083e-01],\n",
       "         [ 1.4380e-01,  1.9341e-03],\n",
       "         ...,\n",
       "         [ 2.8152e-03, -3.5034e-02],\n",
       "         [ 3.7720e-02,  1.6296e-01],\n",
       "         [-2.4646e-01, -1.3306e-01]],\n",
       "\n",
       "        [[-6.3965e-02, -6.6589e-02],\n",
       "         [ 7.1045e-02,  1.9360e-01],\n",
       "         [ 7.5378e-03, -4.7339e-01],\n",
       "         ...,\n",
       "         [ 7.8369e-02,  1.4900e-02],\n",
       "         [ 2.1631e-01, -2.8809e-01],\n",
       "         [-1.1284e-02,  1.0939e-03]]], device='cuda:0'), tensor([ 3.3325e-01,  3.8513e-02,  2.3926e-02,  2.5342e-01,  2.4731e-01,\n",
       "         3.7671e-01, -3.0766e-03, -3.6102e-02,  5.1178e-02,  3.7079e-02,\n",
       "        -2.1652e-02,  2.9774e-03,  3.9429e-01,  5.9601e-02,  6.6406e-02,\n",
       "         4.9591e-02,  7.4043e-03,  2.5122e-01,  1.0205e-01,  3.4973e-02,\n",
       "         2.8076e-01,  3.0304e-02,  3.6182e-01,  3.5425e-01, -3.3379e-03,\n",
       "         6.7810e-02,  1.3647e-01, -2.8137e-02,  9.8511e-02, -7.4585e-02,\n",
       "         2.8271e-01,  4.2383e-01,  3.6438e-02,  6.3965e-02, -1.3885e-02,\n",
       "         2.1301e-01,  3.4448e-01,  9.2041e-02, -1.0735e-02,  1.9516e-02,\n",
       "         1.1993e-02,  9.7656e-02,  1.2573e-01, -2.8915e-02, -1.7643e-03,\n",
       "        -1.8341e-02,  4.5715e-02,  1.2917e-02,  2.1973e-01,  3.3032e-01,\n",
       "         9.2773e-02,  1.1787e-02,  8.6853e-02, -5.2948e-02,  4.1733e-03,\n",
       "         4.6509e-02,  5.7678e-02,  4.2603e-02,  9.6558e-02, -2.0638e-03,\n",
       "         6.7444e-02, -1.0156e-01,  2.3206e-01, -1.8219e-02,  5.8136e-02,\n",
       "         6.0394e-02, -4.1321e-02, -7.0763e-03,  3.1348e-01, -2.9480e-02,\n",
       "        -1.2311e-01,  8.7891e-02, -2.6306e-02,  1.0117e-02,  1.6357e-02,\n",
       "         2.5562e-01,  8.5815e-02,  3.4790e-01,  2.4805e-01,  6.6650e-02,\n",
       "         1.1005e-01, -3.4210e-02,  1.8701e-01, -2.5452e-02,  6.0463e-03,\n",
       "         2.9785e-01,  2.4573e-01,  6.7261e-02,  7.3792e-02,  3.5303e-01,\n",
       "         4.9133e-02,  2.9688e-01,  1.8481e-01, -4.2053e-02,  2.7808e-01,\n",
       "         5.3760e-01,  2.0093e-01,  2.6929e-01,  2.6074e-01,  1.7444e-01,\n",
       "        -3.0914e-02,  2.8955e-01,  3.7262e-02, -2.3605e-02,  5.5695e-02,\n",
       "         1.4307e-01,  5.8929e-02,  1.3330e-01,  2.2021e-01,  2.1106e-01,\n",
       "         2.5537e-01,  3.1952e-02,  3.6206e-01,  3.0884e-02,  1.2646e-01,\n",
       "         3.1030e-01,  4.7192e-01,  1.5955e-01, -5.0842e-02,  3.0098e-03,\n",
       "        -9.8267e-03,  2.5732e-01, -9.8114e-03, -9.6436e-02,  1.6772e-01,\n",
       "         1.6174e-01,  4.7388e-01,  4.2871e-01,  7.2021e-02, -2.2141e-02,\n",
       "         1.2338e-04,  6.6956e-02,  4.6936e-02,  6.6040e-02,  5.4512e-03,\n",
       "         7.9041e-02,  1.5022e-02,  6.1676e-02,  3.4253e-01,  8.5938e-02,\n",
       "         4.3732e-02,  3.1641e-01,  5.1147e-02, -1.2619e-02,  9.3002e-03,\n",
       "         4.2896e-01,  3.2910e-01,  5.2917e-02,  3.7524e-01,  5.0385e-02,\n",
       "        -3.1113e-02,  1.7685e-02,  3.1641e-01, -6.7017e-02,  7.1777e-02,\n",
       "         3.5736e-02,  6.7566e-02,  2.7368e-01,  1.4001e-01,  2.6276e-02,\n",
       "        -8.8989e-02,  8.3923e-02,  1.2964e-01,  7.1960e-02,  1.6040e-01,\n",
       "         1.9730e-02,  1.1401e-01,  2.4765e-02,  1.2659e-01, -3.8727e-02,\n",
       "         9.7229e-02,  2.1118e-01,  3.0136e-02,  4.2090e-01,  2.9846e-02,\n",
       "         2.1997e-01,  1.3391e-01,  1.0895e-02, -3.5736e-02,  1.3831e-01,\n",
       "         3.6865e-01,  1.3039e-02,  1.9913e-02,  7.3181e-02,  7.1526e-03,\n",
       "         1.3306e-01,  4.3243e-02, -6.5727e-03,  6.2195e-02,  3.4839e-01,\n",
       "         3.7402e-01,  4.7021e-01,  2.7441e-01,  1.1481e-01,  4.4403e-02,\n",
       "         2.6929e-01,  3.5059e-01, -8.3069e-02,  3.5000e-03,  3.4180e-02,\n",
       "         3.3765e-01,  1.3391e-01, -1.8097e-02,  6.7444e-03,  1.1829e-01,\n",
       "         2.3865e-01,  2.9321e-01,  8.1177e-02,  3.1006e-01, -5.6946e-02,\n",
       "         1.3098e-01,  2.7319e-01,  2.6880e-01,  2.6489e-01,  1.0907e-01,\n",
       "         2.8516e-01,  4.7722e-03, -8.9783e-02,  4.1382e-02,  2.9877e-02,\n",
       "         3.7918e-03, -8.1604e-02,  2.1411e-01, -1.1169e-02,  9.3018e-02,\n",
       "         1.7166e-02, -2.7740e-02,  1.4343e-03,  3.3051e-02,  3.4839e-01,\n",
       "         9.9304e-02,  8.9111e-02,  2.3499e-02,  2.3474e-01,  1.3013e-01,\n",
       "         1.2184e-02, -5.5695e-04,  2.8467e-01,  2.4780e-01,  1.9189e-01,\n",
       "         4.4281e-02,  3.1396e-01,  1.5369e-01,  2.3926e-01,  3.6182e-01,\n",
       "         2.7783e-01,  2.7905e-01,  1.0571e-01,  1.9946e-01,  3.2031e-01,\n",
       "         8.1909e-02,  2.6904e-01,  1.1102e-01,  1.6495e-02,  1.5511e-02,\n",
       "         2.9199e-01,  4.3384e-01,  9.7473e-02,  4.0649e-02,  1.8665e-01,\n",
       "         2.0813e-02,  3.3643e-01,  1.7346e-01,  7.4097e-02,  5.3131e-02,\n",
       "         5.4810e-02,  2.8467e-01,  5.9021e-02,  2.9327e-02,  6.0822e-02,\n",
       "         2.3621e-01, -3.5431e-02,  6.8237e-02, -2.0554e-02,  1.4380e-01,\n",
       "         2.9434e-02,  5.9174e-02,  2.3834e-02,  6.1523e-02,  3.0591e-01,\n",
       "         2.5928e-01,  5.1575e-02, -2.5574e-02,  7.1777e-02, -4.4861e-02,\n",
       "         2.7344e-01,  5.0934e-02,  3.7109e-01,  1.0272e-01, -1.0925e-01,\n",
       "         3.2422e-01,  1.2000e-01,  2.4902e-01,  2.5562e-01, -3.0121e-02,\n",
       "         8.6060e-02,  4.7211e-02,  9.8694e-02,  1.0626e-01,  2.5903e-01,\n",
       "         2.3486e-01,  2.2913e-01,  7.9727e-03,  5.4169e-02,  7.5012e-02,\n",
       "         3.6621e-01,  3.1421e-01,  5.1758e-02,  9.2468e-03,  3.0347e-01,\n",
       "         1.1978e-02,  7.3059e-02,  2.9541e-01,  5.1819e-02,  2.9956e-01,\n",
       "        -4.3526e-03, -9.5703e-02,  5.0842e-02,  3.6499e-02,  1.6370e-01,\n",
       "         8.6060e-02,  3.0640e-01,  1.7944e-01,  9.4055e-02,  3.0298e-01,\n",
       "        -7.1350e-02,  3.5010e-01,  2.3608e-01,  2.9126e-01,  3.1250e-01,\n",
       "        -6.0059e-02,  2.9639e-01,  1.7090e-01,  3.2007e-01,  3.6865e-01,\n",
       "         2.9678e-02, -3.0212e-02,  1.5649e-01,  1.7407e-01, -7.6904e-03,\n",
       "         6.5247e-02,  2.9346e-01,  6.8237e-02,  2.0959e-01,  1.1208e-02,\n",
       "         2.6535e-02,  2.1305e-03,  4.0479e-01,  6.2561e-02,  9.0942e-02,\n",
       "        -2.8717e-02, -9.2834e-02,  2.5562e-01,  1.6565e-01,  2.0166e-01,\n",
       "        -7.1678e-03,  2.2534e-01,  1.1200e-02,  9.6359e-03, -4.6921e-03,\n",
       "         4.2114e-02,  8.7738e-03,  1.9812e-01, -3.4332e-03,  3.0542e-01,\n",
       "         3.0591e-01,  1.0028e-01,  8.8562e-02, -4.4739e-02,  8.3557e-02,\n",
       "         7.4524e-02,  1.0724e-01,  2.4512e-01,  3.2007e-01,  3.5645e-02,\n",
       "         1.3573e-02, -1.0429e-02, -4.2450e-02,  2.1887e-01,  3.6401e-01,\n",
       "         1.8787e-01, -6.4659e-03,  3.1299e-01,  8.6487e-02,  1.9116e-01,\n",
       "         1.5613e-01,  3.1665e-01,  2.8351e-02,  3.3472e-01,  2.3376e-01,\n",
       "         3.1128e-01,  7.0007e-02,  2.5000e-01,  1.7838e-02,  1.9897e-02,\n",
       "         1.9788e-01, -1.3519e-02,  2.4597e-01, -2.7222e-02,  2.4811e-02,\n",
       "         1.3525e-01,  1.3779e-02,  2.7222e-01,  5.4504e-02,  7.6141e-03,\n",
       "         3.4790e-01,  1.0870e-01,  2.1948e-01, -1.1047e-02,  3.0298e-01,\n",
       "         5.5084e-02,  1.1877e-01, -3.7811e-02,  1.5173e-01,  9.2896e-02,\n",
       "         4.1797e-01,  2.2534e-01,  2.2107e-01, -1.9180e-02,  2.2266e-01,\n",
       "         2.6489e-02, -1.7578e-02,  2.7051e-01, -1.7583e-05,  7.1838e-02,\n",
       "         1.1499e-01,  7.8918e-02,  1.9910e-01,  3.3051e-02,  3.0380e-02,\n",
       "         3.3154e-01,  8.6670e-03, -8.2855e-03,  2.4994e-02,  1.4392e-01,\n",
       "        -2.9037e-02,  2.1400e-03,  8.8623e-02,  1.8115e-01,  2.2400e-01,\n",
       "         7.5867e-02,  9.4543e-02,  3.0457e-02,  3.4473e-01,  2.7026e-01,\n",
       "         8.3847e-03,  6.2927e-02, -4.9927e-02,  2.9846e-02,  8.3923e-02,\n",
       "         8.4778e-02,  6.2469e-02,  1.2659e-01,  1.8951e-02,  7.6256e-03,\n",
       "         2.7539e-01,  7.5562e-02,  2.9761e-01,  6.5125e-02,  1.8604e-01,\n",
       "         2.5537e-01, -3.2074e-02,  3.0029e-01, -2.2156e-02,  1.3649e-02,\n",
       "        -2.6382e-02,  8.2458e-02,  7.7942e-02,  2.6807e-01,  5.8319e-02,\n",
       "        -8.8867e-02,  4.8706e-02, -2.0599e-02,  2.5879e-01,  4.1284e-01,\n",
       "        -1.3695e-02,  3.3032e-01,  2.7173e-01,  1.6953e-02, -4.8561e-03,\n",
       "         8.5022e-02,  2.7026e-01, -5.0293e-02,  2.0859e-02,  2.9224e-01,\n",
       "         1.2146e-01,  1.0199e-01,  2.6660e-01,  3.9429e-02,  9.1553e-02,\n",
       "         4.6069e-01,  1.2909e-02,  2.8516e-01,  9.6558e-02,  4.6295e-02,\n",
       "         1.1798e-01,  1.2543e-02,  5.3131e-02,  1.9873e-01, -6.4697e-02,\n",
       "         1.2964e-01,  3.4521e-01,  3.2446e-01,  2.6047e-02, -1.5732e-02,\n",
       "         1.7346e-01,  2.9004e-01, -3.8330e-02,  1.7249e-01,  1.9153e-01,\n",
       "         6.2103e-02,  1.6211e-01], device='cuda:0'), tensor([0.1343, 0.5264, 0.6792, 0.1519, 0.1465, 0.1437, 0.6479, 0.6177, 0.4595,\n",
       "        0.5498, 0.6484, 0.5244, 0.1498, 0.6460, 0.6118, 0.6450, 0.4805, 0.1310,\n",
       "        0.4934, 0.4399, 0.1362, 0.5898, 0.1447, 0.1404, 0.4446, 0.4526, 0.2035,\n",
       "        0.7568, 0.4407, 0.5400, 0.1392, 0.1512, 0.4302, 0.4302, 0.5142, 0.1482,\n",
       "        0.1680, 0.4617, 0.4529, 0.6323, 0.7632, 0.5410, 0.2069, 0.5986, 0.6890,\n",
       "        0.6470, 0.6768, 0.5869, 0.1754, 0.1331, 0.5718, 0.7197, 0.3784, 0.7461,\n",
       "        0.5142, 0.3989, 0.4314, 0.4673, 0.1824, 0.6309, 0.1696, 0.7627, 0.1755,\n",
       "        0.5933, 0.7261, 0.5830, 0.5776, 0.6216, 0.1531, 0.6440, 0.7017, 0.5229,\n",
       "        0.7778, 0.6240, 0.6279, 0.1475, 0.5771, 0.1539, 0.1355, 0.4250, 0.2705,\n",
       "        0.6323, 0.1209, 0.5420, 0.5454, 0.1505, 0.1476, 0.7729, 0.4561, 0.1421,\n",
       "        0.5537, 0.1428, 0.1587, 0.7354, 0.1478, 0.1293, 0.1544, 0.1528, 0.1620,\n",
       "        0.1678, 0.7397, 0.1313, 0.5200, 0.6011, 0.1785, 0.3521, 0.4802, 0.3376,\n",
       "        0.1270, 0.1874, 0.1365, 0.7280, 0.1482, 0.7583, 0.2108, 0.1522, 0.1226,\n",
       "        0.1621, 0.4724, 0.4478, 0.6846, 0.1433, 0.4661, 0.7974, 0.1870, 0.1510,\n",
       "        0.1226, 0.1266, 0.4529, 0.5156, 0.6929, 0.3833, 0.7085, 0.5781, 0.5874,\n",
       "        0.8813, 0.5737, 0.4739, 0.1842, 0.4788, 0.5693, 0.1531, 0.6675, 0.6274,\n",
       "        0.4436, 0.1475, 0.1567, 0.7227, 0.1445, 0.4158, 0.5679, 0.4326, 0.1309,\n",
       "        0.6528, 0.4382, 0.3970, 0.4338, 0.1553, 0.3083, 0.6650, 0.9932, 0.4626,\n",
       "        0.1610, 0.5156, 0.1605, 0.6567, 0.4148, 0.6060, 0.3425, 0.1245, 0.5107,\n",
       "        0.1406, 0.5298, 0.1316, 0.5078, 0.1520, 0.5669, 0.4016, 0.5254, 0.3992,\n",
       "        0.1311, 0.7329, 0.7183, 0.4995, 0.5571, 0.1672, 0.6255, 0.6553, 0.6318,\n",
       "        0.1287, 0.1312, 0.1464, 0.1329, 0.5576, 0.6938, 0.1206, 0.1360, 0.5742,\n",
       "        0.4150, 0.7295, 0.1406, 0.1459, 0.7178, 0.5132, 0.2451, 0.1555, 0.2025,\n",
       "        0.4558, 0.1400, 0.7280, 0.4919, 0.1429, 0.1489, 0.1399, 0.1772, 0.1501,\n",
       "        0.4912, 0.7173, 0.5454, 0.3528, 0.5605, 0.6533, 0.2404, 0.5078, 0.3191,\n",
       "        0.5820, 0.5674, 0.5020, 0.4326, 0.1281, 0.4326, 0.4702, 0.4648, 0.1689,\n",
       "        0.6323, 0.4299, 0.5664, 0.1354, 0.1442, 0.1843, 0.3882, 0.1470, 0.2532,\n",
       "        0.1881, 0.1305, 0.1383, 0.1270, 0.3540, 0.1279, 0.1175, 0.4761, 0.1418,\n",
       "        0.2964, 0.4412, 0.5576, 0.1411, 0.1399, 0.6963, 0.6948, 0.1661, 0.4929,\n",
       "        0.1326, 0.1825, 0.6489, 0.5708, 0.6128, 0.1669, 0.3848, 0.6143, 0.5391,\n",
       "        0.1631, 0.6143, 0.6289, 0.6714, 0.3496, 0.5923, 0.6187, 0.4817, 0.3770,\n",
       "        0.1231, 0.1663, 0.4490, 0.5142, 0.4158, 0.5396, 0.1343, 0.3481, 0.1425,\n",
       "        0.5537, 0.6743, 0.1400, 0.2688, 0.1353, 0.1555, 0.6196, 0.3262, 0.6143,\n",
       "        0.4678, 0.7510, 0.1377, 0.1483, 0.1588, 0.6489, 0.3906, 0.5684, 0.1475,\n",
       "        0.1366, 0.4705, 0.5366, 0.1399, 0.6597, 0.7314, 0.1421, 0.4331, 0.1504,\n",
       "        0.6343, 0.6499, 0.4846, 0.4287, 0.1768, 0.4421, 0.1348, 0.1561, 0.4546,\n",
       "        0.1469, 0.6929, 0.1396, 0.1741, 0.1338, 0.1555, 0.7686, 0.1458, 0.1414,\n",
       "        0.1808, 0.1310, 0.4961, 0.7036, 0.2568, 0.1731, 0.7358, 0.6821, 0.1235,\n",
       "        0.2130, 0.2991, 0.5776, 0.7363, 0.5635, 0.1259, 0.4519, 0.4790, 0.6553,\n",
       "        0.6274, 0.1714, 0.1664, 0.1448, 0.5405, 0.1379, 0.6846, 0.7920, 0.4534,\n",
       "        0.6802, 0.2585, 0.1948, 0.6255, 0.1348, 0.1436, 0.3767, 0.5557, 0.6753,\n",
       "        0.7178, 0.5552, 0.3237, 0.1385, 0.1302, 0.5249, 0.5820, 0.4912, 0.4956,\n",
       "        0.1600, 0.1401, 0.1898, 0.1611, 0.1252, 0.3123, 0.2651, 0.1327, 0.1498,\n",
       "        0.5918, 0.1334, 0.1735, 0.1342, 0.1638, 0.1487, 0.4519, 0.6006, 0.1760,\n",
       "        0.9453, 0.1603, 0.6309, 0.5073, 0.3701, 0.3879, 0.1376, 0.4844, 0.7451,\n",
       "        0.1237, 0.5474, 0.1646, 0.6528, 0.1659, 0.4548, 0.2046, 0.5483, 0.3638,\n",
       "        0.5674, 0.1337, 0.1990, 0.1815, 0.7192, 0.2209, 0.6440, 0.5254, 0.1250,\n",
       "        0.1936, 0.4885, 0.3831, 0.6470, 0.1611, 0.5791, 0.6655, 0.1202, 0.6313,\n",
       "        0.7993, 0.5190, 0.3677, 0.5327, 0.5386, 0.5918, 0.2510, 0.1373, 0.6382,\n",
       "        0.5132, 0.5518, 0.1442, 0.1559, 0.7231, 0.7842, 0.5693, 0.6831, 0.6133,\n",
       "        0.6514, 0.5303, 0.4507, 0.7236, 0.4399, 0.1323, 0.4910, 0.1344, 0.4810,\n",
       "        0.1788, 0.1392, 0.6172, 0.1272, 0.5688, 0.6064, 0.5884, 0.4978, 0.4055,\n",
       "        0.1571, 0.1460, 0.6528, 0.5146, 0.6631, 0.1781, 0.1326, 0.6289, 0.1345,\n",
       "        0.1334, 0.4929, 0.4226, 0.3877, 0.1248, 0.5469, 0.4421, 0.1250, 0.2096,\n",
       "        0.2328, 0.1499, 0.7524, 0.6064, 0.1271, 0.6860, 0.1348, 0.4846, 0.7505,\n",
       "        0.1560, 0.4084, 0.4221, 0.1672, 0.7295, 0.1791, 0.1298, 0.1469, 0.4819,\n",
       "        0.5444, 0.3796, 0.1616, 0.5454, 0.1394, 0.1312, 0.4941, 0.1179],\n",
       "       device='cuda:0'), tensor([-0.2020, -0.6030, -0.3850, -0.2024, -0.1904, -0.1864, -0.6348, -0.5977,\n",
       "        -0.3811, -0.5679, -0.4792, -0.7114, -0.1923, -0.5596, -0.5444, -0.5186,\n",
       "        -0.5444, -0.1875, -0.4832, -0.3877, -0.1870, -0.4673, -0.1810, -0.2053,\n",
       "        -0.3516, -0.4543, -0.2396, -0.4644, -0.3230, -0.5186, -0.1842, -0.1707,\n",
       "        -0.4387, -0.4214, -0.4429, -0.1979, -0.1810, -0.4204, -0.3530, -0.3669,\n",
       "        -0.8521, -0.4199, -0.1904, -0.6758, -0.6089, -0.5439, -0.7188, -0.4841,\n",
       "        -0.2159, -0.1929, -0.4658, -0.4927, -0.3679, -0.4543, -0.5112, -0.3157,\n",
       "        -0.4380, -0.3716, -0.1594, -0.6494, -0.1240, -0.9062, -0.1855, -0.4580,\n",
       "        -0.7954, -0.3909, -0.6396, -0.3423, -0.1913, -0.5381, -0.7905, -0.4548,\n",
       "        -0.3660, -0.6011, -0.6079, -0.1891, -0.3201, -0.1740, -0.2024, -0.4177,\n",
       "        -0.2903, -0.5591, -0.2001, -0.5903, -0.4229, -0.1537, -0.1764, -0.7363,\n",
       "        -0.4226, -0.1801, -0.4563, -0.1731, -0.2247, -0.6216, -0.1879, -0.1548,\n",
       "        -0.1453, -0.2019, -0.1921, -0.1410, -0.8530, -0.1890, -0.3679, -0.4346,\n",
       "        -0.1783, -0.3125, -0.4692, -0.2976, -0.1985, -0.2095, -0.1820, -0.5791,\n",
       "        -0.2108, -0.8188, -0.2201, -0.2030, -0.1499, -0.2134, -0.5347, -0.5132,\n",
       "        -0.5713, -0.1748, -0.4038, -0.8062, -0.2037, -0.1754, -0.1528, -0.1821,\n",
       "        -0.5020, -0.6021, -0.7031, -0.3228, -0.6196, -0.6626, -0.5562, -0.5420,\n",
       "        -0.6099, -0.4888, -0.2167, -0.3049, -0.4373, -0.1904, -0.4766, -0.5972,\n",
       "        -0.4180, -0.1771, -0.1842, -0.4424, -0.1801, -0.3613, -0.7437, -0.3447,\n",
       "        -0.1865, -0.6875, -0.3994, -0.2822, -0.3848, -0.1992, -0.2791, -0.7866,\n",
       "        -0.6528, -0.3130, -0.1719, -0.3755, -0.1599, -0.5854, -0.2954, -0.6724,\n",
       "        -0.3599, -0.2025, -0.4917, -0.1438, -0.4756, -0.1740, -0.4214, -0.2133,\n",
       "        -0.3369, -0.3401, -0.6436, -0.3596, -0.1829, -0.4756, -0.7598, -0.3765,\n",
       "        -0.5474, -0.0626, -0.6484, -0.3789, -0.4021, -0.1754, -0.1995, -0.1869,\n",
       "        -0.1726, -0.3591, -0.7524, -0.2046, -0.1974, -0.6631, -0.3354, -0.5435,\n",
       "        -0.1996, -0.1541, -0.6499, -0.4980, -0.2727, -0.2086, -0.1808, -0.4990,\n",
       "        -0.1620, -0.6831, -0.3499, -0.2217, -0.2059, -0.2007, -0.1807, -0.1781,\n",
       "        -0.4287, -0.7031, -0.4495, -0.3809, -0.6274, -0.4624, -0.1733, -0.4041,\n",
       "        -0.2471, -0.5127, -0.5112, -0.5273, -0.2578, -0.1707, -0.4648, -0.3701,\n",
       "        -0.4312, -0.1794, -0.3782, -0.3340, -0.5039, -0.2108, -0.2135, -0.1906,\n",
       "        -0.3379, -0.1886, -0.1638, -0.2288, -0.1776, -0.1661, -0.1757, -0.3264,\n",
       "        -0.2114, -0.1801, -0.4954, -0.1697, -0.2913, -0.4160, -0.5122, -0.1819,\n",
       "        -0.1797, -0.6802, -0.4380, -0.2317, -0.4316, -0.1744, -0.1953, -0.6641,\n",
       "        -0.3735, -0.5044, -0.1960, -0.3574, -0.5093, -0.4395, -0.1849, -0.6055,\n",
       "        -0.8071, -0.5752, -0.3191, -0.5952, -0.4412, -0.4941, -0.3196, -0.1722,\n",
       "        -0.1998, -0.4099, -0.6958, -0.2419, -0.6377, -0.2017, -0.2024, -0.2256,\n",
       "        -0.5195, -0.7729, -0.1788, -0.2450, -0.2034, -0.2120, -0.6616, -0.2280,\n",
       "        -0.5391, -0.3113, -0.4167, -0.1663, -0.1948, -0.1877, -0.7109, -0.3357,\n",
       "        -0.5239, -0.2128, -0.1978, -0.3367, -0.4038, -0.1863, -0.6963, -0.5225,\n",
       "        -0.1697, -0.4248, -0.1589, -0.6748, -0.4941, -0.4373, -0.3306, -0.1821,\n",
       "        -0.3030, -0.1654, -0.1948, -0.3313, -0.1978, -0.4768, -0.2017, -0.2329,\n",
       "        -0.2019, -0.1949, -0.4370, -0.2115, -0.2266, -0.1790, -0.1903, -0.3726,\n",
       "        -0.7568, -0.1909, -0.1832, -0.5347, -0.5205, -0.1879, -0.1321, -0.4607,\n",
       "        -0.6675, -0.4551, -0.6870, -0.1840, -0.5459, -0.4475, -0.5176, -0.7637,\n",
       "        -0.1929, -0.2075, -0.2010, -0.4983, -0.1788, -0.4451, -0.5737, -0.4114,\n",
       "        -0.6025, -0.0381, -0.1963, -0.4480, -0.2100, -0.1967, -0.3457, -0.3374,\n",
       "        -0.5312, -0.3572, -0.4941, -0.3225, -0.2074, -0.1852, -0.5156, -0.6064,\n",
       "        -0.5225, -0.5254, -0.1910, -0.1788, -0.1782, -0.1387, -0.2034, -0.2747,\n",
       "        -0.2871, -0.1816, -0.1718, -0.6934, -0.1851, -0.1772, -0.1886, -0.1407,\n",
       "        -0.2098, -0.4622, -0.5684, -0.1992, -0.5728, -0.2213, -0.6616, -0.5000,\n",
       "        -0.3079, -0.3987, -0.1917, -0.4824, -0.5254, -0.1870, -0.3062, -0.2128,\n",
       "        -0.6055, -0.1978, -0.4436, -0.2220, -0.4785, -0.3289, -0.5347, -0.2057,\n",
       "        -0.2489, -0.1126, -0.4204, -0.1908, -0.5923, -0.5220, -0.1799, -0.1224,\n",
       "        -0.4172, -0.2869, -0.6084, -0.2064, -0.4263, -0.4771, -0.1691, -0.5703,\n",
       "        -0.5166, -0.5640, -0.4089, -0.6143, -0.4780, -0.5205, -0.2041, -0.2113,\n",
       "        -0.6758, -0.4341, -0.5425, -0.1848, -0.1957, -0.8228, -0.4124, -0.5459,\n",
       "        -0.7271, -0.5439, -0.4607, -0.5762, -0.3152, -0.5864, -0.4949, -0.2111,\n",
       "        -0.4373, -0.2009, -0.3457, -0.1660, -0.1819, -0.5947, -0.1586, -0.7412,\n",
       "        -0.4639, -0.5078, -0.2456, -0.4431, -0.1530, -0.1410, -0.6411, -0.4512,\n",
       "        -0.6172, -0.1841, -0.1804, -0.3267, -0.1846, -0.1908, -0.4470, -0.4368,\n",
       "        -0.4285, -0.2037, -0.5415, -0.4709, -0.2061, -0.2076, -0.2323, -0.2152,\n",
       "        -0.6113, -0.5498, -0.1715, -0.5654, -0.1927, -0.5156, -0.4829, -0.1716,\n",
       "        -0.2529, -0.2954, -0.2133, -0.4661, -0.1688, -0.1696, -0.1748, -0.4883,\n",
       "        -0.4761, -0.2986, -0.1882, -0.6709, -0.1134, -0.2089, -0.4282, -0.2203],\n",
       "       device='cuda:0'), tensor([1.4446, 1.4680, 1.8968, 1.5431, 1.3285, 1.4617, 1.3047, 1.8141, 1.6933,\n",
       "        1.3897, 1.7088, 1.6192, 1.4382, 1.3701, 1.1397, 1.9625, 1.6340, 1.6033,\n",
       "        1.3152, 1.5267, 1.5451, 1.8547, 1.4134, 1.5230, 2.1361, 1.4626, 1.7870,\n",
       "        1.5476, 1.8730, 1.7899, 1.5715, 1.3739, 1.7394, 1.5230, 1.3527, 1.5797,\n",
       "        1.4255, 1.5673, 1.9314, 1.8934, 1.4458, 1.7989, 1.8902, 1.4876, 1.5021,\n",
       "        1.4301, 1.3771, 1.1804, 1.4161, 1.5846, 1.4536, 1.4156, 1.4302, 1.6807,\n",
       "        1.2968, 2.7050, 1.6502, 1.2809, 1.2517, 1.4650, 1.1409, 1.4963, 1.5275,\n",
       "        1.8613, 1.2864, 1.8412, 1.2123, 1.8950, 1.5024, 1.3736, 1.3470, 1.5698,\n",
       "        1.5288, 1.3158, 1.5287, 1.5635, 2.2009, 1.4491, 1.5273, 1.6242, 1.7424,\n",
       "        1.5587, 1.6068, 1.5078, 1.7059, 1.3265, 1.5758, 1.3251, 1.3126, 1.4460,\n",
       "        1.7523, 1.5059, 1.5838, 1.6658, 1.4272, 1.1432, 0.9929, 1.4630, 1.5582,\n",
       "        0.8582, 1.6531, 1.5750, 1.4115, 1.8143, 1.6210, 1.9582, 1.8120, 1.3560,\n",
       "        1.5287, 1.7592, 1.5433, 1.2826, 1.5390, 1.1995, 1.4381, 1.5009, 1.5820,\n",
       "        1.6924, 1.7322, 1.8584, 1.4049, 1.5631, 2.6817, 1.4277, 1.6660, 1.6575,\n",
       "        1.5220, 1.2859, 1.6193, 1.3914, 1.3935, 1.7785, 1.1621, 1.4974, 1.4519,\n",
       "        1.2580, 1.3182, 1.2966, 1.2897, 1.2682, 1.2835, 1.3886, 1.3743, 1.6542,\n",
       "        1.9315, 1.4054, 1.4477, 1.3257, 1.4995, 1.3121, 1.6749, 2.3663, 1.4802,\n",
       "        1.7677, 1.2358, 2.1221, 1.6327, 1.5274, 1.4243, 1.0941, 1.5504, 1.7528,\n",
       "        1.1415, 1.5653, 1.4600, 1.5234, 1.6340, 1.1814, 1.6498, 1.8788, 1.2133,\n",
       "        1.2443, 1.3002, 1.4853, 1.6236, 1.4642, 1.2104, 1.6203, 2.5752, 1.4610,\n",
       "        1.4427, 1.3006, 1.3544, 1.5027, 1.3367, 1.3575, 1.2088, 1.8339, 1.1074,\n",
       "        1.6096, 1.4289, 1.4069, 1.4192, 1.6996, 1.1152, 1.6579, 1.5034, 1.4164,\n",
       "        3.0729, 1.1836, 1.5339, 1.3371, 1.3831, 1.3873, 1.4851, 1.4707, 1.3919,\n",
       "        1.2727, 1.3379, 1.1625, 1.0171, 1.4936, 1.5760, 1.5773, 1.4593, 1.3882,\n",
       "        2.0986, 1.5928, 1.3580, 1.3978, 1.5601, 2.0480, 1.2310, 1.7020, 1.9968,\n",
       "        1.2382, 1.9008, 1.8341, 2.0164, 1.5136, 1.5156, 1.7951, 1.6698, 1.6546,\n",
       "        1.1174, 2.0515, 1.4449, 1.4945, 1.6124, 1.6361, 1.7976, 1.5458, 1.4119,\n",
       "        1.2085, 1.4447, 1.2353, 1.4495, 1.7273, 1.6742, 1.3978, 1.4158, 1.1886,\n",
       "        1.6796, 1.8708, 1.8310, 1.5604, 1.4786, 1.3101, 1.5845, 1.6377, 2.0134,\n",
       "        1.5819, 1.5369, 1.0666, 1.2619, 1.4638, 1.4972, 1.3768, 1.7475, 1.4287,\n",
       "        1.4334, 1.6287, 1.0630, 1.6659, 1.8945, 1.3838, 1.3307, 1.4274, 1.7239,\n",
       "        1.4255, 1.3916, 1.7336, 1.7201, 1.9810, 1.4712, 1.5492, 2.2928, 1.2428,\n",
       "        1.4617, 1.6647, 1.5314, 1.7289, 1.3352, 1.4977, 1.4789, 2.4794, 1.6156,\n",
       "        2.0525, 1.3505, 1.3252, 1.5322, 1.2652, 1.3475, 2.0619, 1.3076, 1.4997,\n",
       "        1.5646, 1.7505, 1.4845, 1.5097, 1.7493, 1.3210, 1.5361, 1.7462, 1.3752,\n",
       "        1.4011, 2.0073, 2.0639, 2.1542, 1.5458, 1.8805, 1.4330, 1.5808, 1.4012,\n",
       "        1.3935, 1.6805, 1.4436, 1.5975, 1.4835, 1.4718, 1.6820, 1.5154, 1.5909,\n",
       "        1.4130, 1.5296, 2.0187, 1.4179, 1.4689, 1.6370, 1.7233, 1.2044, 1.5090,\n",
       "        1.0053, 0.5604, 1.5247, 1.4594, 1.2157, 1.4225, 1.1862, 1.1312, 1.8157,\n",
       "        1.3091, 1.4810, 1.6289, 1.6178, 1.6790, 1.4177, 1.4096, 1.2662, 1.9373,\n",
       "        1.3217, 0.6537, 1.6367, 1.7283, 1.4447, 1.4403, 1.7651, 1.2411, 1.6036,\n",
       "        1.1950, 1.0983, 1.4710, 1.2901, 1.5262, 1.4005, 1.3675, 1.4567, 2.1374,\n",
       "        1.5556, 1.4941, 1.4029, 1.4178, 1.6376, 2.1806, 1.7447, 1.5252, 1.4526,\n",
       "        1.5293, 1.4256, 1.5377, 1.5080, 1.3661, 1.5584, 1.6694, 1.2824, 1.5353,\n",
       "        1.5386, 1.5630, 1.2891, 1.7726, 1.6670, 1.6129, 1.5414, 1.3438, 1.4521,\n",
       "        1.5667, 1.2492, 1.4046, 1.2235, 1.4374, 1.3015, 1.6963, 1.8893, 1.3764,\n",
       "        1.0988, 1.1349, 1.6777, 1.3017, 1.5158, 1.4341, 1.4252, 2.2121, 1.3576,\n",
       "        1.1249, 1.3422, 1.3709, 1.3423, 1.2584, 1.4946, 1.5932, 1.4245, 1.9604,\n",
       "        1.1740, 1.3338, 1.0517, 1.4801, 1.4134, 1.0545, 1.7943, 1.5216, 1.1207,\n",
       "        1.5683, 1.2389, 1.5033, 1.5475, 1.3275, 1.1823, 1.2544, 1.1758, 1.1229,\n",
       "        1.4474, 1.3899, 1.5708, 1.3286, 1.5829, 1.6284, 1.2574, 1.5669, 1.8738,\n",
       "        1.5205, 1.4941, 2.0171, 1.5302, 1.4462, 1.2826, 1.4248, 1.8200, 1.3528,\n",
       "        1.2421, 1.5129, 1.5386, 1.3764, 1.2521, 1.4786, 1.5047, 1.5549, 1.5038,\n",
       "        1.4396, 1.5724, 1.7731, 1.6480, 1.6101, 1.8188, 1.8017, 1.5853, 1.3924,\n",
       "        1.7892, 1.5872, 1.1974, 1.0156, 1.3869, 1.1766, 1.5391, 1.3532, 1.3778,\n",
       "        1.5413, 2.7331, 1.6996, 1.5231, 1.6698, 1.6449, 1.4586, 1.4635, 1.7256,\n",
       "        1.6669, 1.6148, 1.3751, 1.4742, 1.1795, 1.6193, 1.4331, 2.0655],\n",
       "       device='cuda:0'), tensor([-0.3364,  0.5018,  0.3648, -0.3921, -0.2523, -0.3875,  0.3528,  0.5940,\n",
       "         0.3760,  0.4681, -0.0015,  0.2919, -0.3427,  0.4452,  0.5437,  0.4587,\n",
       "         0.4520, -0.3749,  0.2481,  0.2492, -0.3946,  0.7426, -0.3790, -0.3632,\n",
       "         0.1067,  0.4143, -0.1507,  0.5502,  0.1565,  0.4747, -0.2589, -0.5436,\n",
       "         0.0772,  0.1780,  0.4199, -0.2999, -0.4199,  0.2573,  0.1753,  0.2454,\n",
       "         0.5456,  0.3186, -0.0701,  0.3257,  0.4628,  0.3467,  0.3119,  0.2995,\n",
       "        -0.2091, -0.3976,  0.4869,  0.6510,  0.1678,  0.5299,  0.4427, -0.4193,\n",
       "         0.4131,  0.2671, -0.1764,  0.5697,  0.0381,  0.7359, -0.3364,  0.4696,\n",
       "         0.5042,  0.1030,  0.4346, -0.1064, -0.3181,  0.3749,  0.3644,  0.2642,\n",
       "         0.3751,  0.5009,  0.3690, -0.3082,  0.0435, -0.3987, -0.3427,  0.1922,\n",
       "        -0.0866,  0.4473, -0.2048,  0.4916,  0.0571, -0.1724, -0.2025,  0.4108,\n",
       "         0.4747, -0.4516,  0.4358, -0.3681, -0.3455,  0.4705, -0.3776, -0.2957,\n",
       "        -0.1841, -0.3447, -0.1963, -0.1301,  0.2697, -0.3980,  0.3875,  0.3814,\n",
       "        -0.0260,  0.0528,  0.2787,  0.2416, -0.3948, -0.2018, -0.3318,  0.5143,\n",
       "        -0.3511,  0.3390,  0.0813, -0.3262, -0.4531, -0.2350,  0.3754,  0.4674,\n",
       "         0.4524, -0.2695,  0.0655,  0.3300, -0.2072, -0.0950, -0.3945, -0.2385,\n",
       "         0.3284,  0.3232,  0.5459, -0.1356,  0.5367,  0.4860,  0.3819,  0.4231,\n",
       "         0.4743,  0.3615, -0.0668,  0.2287,  0.5527, -0.4004,  0.5023,  0.4345,\n",
       "         0.1733, -0.3630, -0.2731,  0.4334, -0.3498,  0.1707,  0.5741,  0.0155,\n",
       "        -0.2785,  0.4486,  0.2344, -0.1246,  0.1774, -0.2957,  0.0279,  0.4627,\n",
       "         0.4541,  0.2277,  0.0865,  0.2962, -0.0499,  0.3305, -0.0789,  0.5891,\n",
       "         0.0316,  0.1100,  0.3718, -0.1457,  0.4248, -0.3542,  0.2972, -0.2095,\n",
       "         0.3577,  0.2223,  0.0493,  0.0374, -0.3639,  0.4929,  0.5657,  0.4016,\n",
       "         0.3714, -0.0879,  0.6079,  0.4220,  0.3616, -0.4309, -0.4158, -0.4371,\n",
       "        -0.3614,  0.5569,  0.3580, -0.2981, -0.3298,  0.5775, -0.2589,  0.6738,\n",
       "        -0.2719, -0.1350,  0.3218,  0.4164, -0.0878, -0.2251, -0.4133,  0.2938,\n",
       "        -0.1470,  0.4712,  0.2636, -0.5015, -0.2754, -0.2581, -0.2100, -0.2147,\n",
       "         0.1676,  0.4161,  0.3381,  0.1890,  0.4113,  0.3940, -0.2600,  0.1272,\n",
       "        -0.0099,  0.5835,  0.3803,  0.3015,  0.1142, -0.4452,  0.2647, -0.1418,\n",
       "         0.2887, -0.1990,  0.3949,  0.1686,  0.2294, -0.3871, -0.2565, -0.2403,\n",
       "         0.1694, -0.4597, -0.3584,  0.0548, -0.3727, -0.1655, -0.3999, -0.0377,\n",
       "        -0.2988, -0.4190,  0.3458, -0.2769,  0.0481,  0.2418,  0.5155, -0.3833,\n",
       "        -0.4043,  0.5391,  0.3641, -0.2579,  0.1288, -0.3267, -0.2543,  0.4518,\n",
       "         0.5462,  0.3609, -0.3568,  0.2369,  0.3026,  0.4815, -0.2005,  0.5531,\n",
       "         0.4665,  0.2629,  0.0164,  0.4703,  0.6015,  0.3388,  0.1627, -0.2520,\n",
       "        -0.4569,  0.1973,  0.1833,  0.0149,  0.4860, -0.3707, -0.1479, -0.0575,\n",
       "         0.5023,  0.6726, -0.3827, -0.0783, -0.4610, -0.1777,  0.5289, -0.2834,\n",
       "         0.2503,  0.2326,  0.6282, -0.3333, -0.3546, -0.2512,  0.4034, -0.0085,\n",
       "         0.3245, -0.3762, -0.3103,  0.2087,  0.3096, -0.3434,  0.3433,  0.5606,\n",
       "        -0.0484,  0.2303, -0.2536,  0.5585,  0.3037,  0.1855,  0.1293, -0.1450,\n",
       "         0.1460, -0.2601, -0.2675,  0.3052, -0.3339,  0.5499, -0.3784, -0.1819,\n",
       "        -0.3583, -0.2999,  0.3307, -0.4266, -0.3816, -0.4045, -0.3868,  0.2173,\n",
       "         0.5939, -0.1088,  0.0348,  0.5687,  0.2800, -0.4652, -0.0442, -0.1042,\n",
       "         0.3820,  0.6129,  0.4872, -0.4545,  0.2520,  0.4132,  0.3792,  0.4956,\n",
       "        -0.2816, -0.2629, -0.2949,  0.5606, -0.3683,  0.4747,  0.4539,  0.2540,\n",
       "         0.4096,  0.0506, -0.1696,  0.1269, -0.3577, -0.4743,  0.0832,  0.3506,\n",
       "         0.4971,  0.2102,  0.4591, -0.0058, -0.0140, -0.4052,  0.2423,  0.7191,\n",
       "         0.4821,  0.2140, -0.0942, -0.4305, -0.0986,  0.0547, -0.3641, -0.1418,\n",
       "        -0.1027, -0.1087, -0.4723,  0.4857, -0.4723, -0.2716, -0.4109, -0.0485,\n",
       "        -0.3268,  0.3333,  0.4866, -0.3069,  0.6157, -0.2420,  0.5400,  0.3844,\n",
       "         0.0653,  0.0562, -0.4484,  0.4122,  0.3660, -0.4473,  0.4384, -0.2409,\n",
       "         0.4535, -0.3524,  0.2902, -0.1885,  0.2959, -0.0034,  0.5810, -0.1689,\n",
       "        -0.2404,  0.0438,  0.4626, -0.3399,  0.5601,  0.0579, -0.2945,  0.0532,\n",
       "         0.1808,  0.1381,  0.4976,  0.0658,  0.4180,  0.3836, -0.4903,  0.4754,\n",
       "         0.4512,  0.5683,  0.3325,  0.2979,  0.4146,  0.4324, -0.1765, -0.3196,\n",
       "         0.4962,  0.3124,  0.3971, -0.4046, -0.4558,  0.2382,  0.4866,  0.4324,\n",
       "         0.5133,  0.4300,  0.4658,  0.5109,  0.1353,  0.6398,  0.2849, -0.2866,\n",
       "         0.2460, -0.3883,  0.3479, -0.0949, -0.3049,  0.5137, -0.3523,  0.3587,\n",
       "         0.3626,  0.5165,  0.4122,  0.3465, -0.2969,  0.0674,  0.4831,  0.3524,\n",
       "         0.6410, -0.3215, -0.3168,  0.3963, -0.3591, -0.4016,  0.3058,  0.3467,\n",
       "         0.1312, -0.2976,  0.0890,  0.0671, -0.3946, -0.0611,  0.0963, -0.4024,\n",
       "         0.5056,  0.5577, -0.5229,  0.5637, -0.1933,  0.3571,  0.3511, -0.1842,\n",
       "        -0.0740,  0.0147, -0.1880,  0.2935,  0.1148, -0.4296, -0.2889,  0.2233,\n",
       "         0.0864,  0.0898, -0.2287,  0.2913, -0.1291, -0.3428,  0.3550, -0.1827],\n",
       "       device='cuda:0'), tensor([[ 2.6344e-02, -1.2667e-02,  2.4751e-04,  ...,  4.9673e-02,\n",
       "         -3.0290e-02,  3.7999e-01],\n",
       "        [-2.8602e-01,  5.2311e-03, -1.0892e-02,  ...,  1.8951e-01,\n",
       "          4.2592e-02, -2.1246e-01],\n",
       "        [ 6.0683e-02, -9.1011e-02, -1.0854e-02,  ...,  2.4868e-01,\n",
       "         -1.2687e-02,  1.7698e-01],\n",
       "        ...,\n",
       "        [ 1.7535e-01,  4.5202e-03, -8.7661e-03,  ..., -3.7988e-01,\n",
       "         -6.6857e-03, -4.4819e-03],\n",
       "        [ 6.4656e-02,  1.1953e-02, -8.4174e-03,  ...,  6.9762e-02,\n",
       "          1.5257e-02,  1.9000e-01],\n",
       "        [ 1.1948e-01,  9.4148e-03, -5.9433e-03,  ...,  1.2717e-01,\n",
       "         -3.7064e-02,  3.0279e-02]], device='cuda:0'), tensor([ 0.0299,  0.0861, -0.3360,  ...,  0.0298, -0.0327, -0.0780],\n",
       "       device='cuda:0'), tensor([-0.3185, -0.2718,  1.0411,  ..., -0.3314,  0.2833,  0.1328],\n",
       "       device='cuda:0'), tensor([[[0.3242, 0.1334, 0.0968, 0.1026, 0.0969, 0.0749, 0.0610, 0.0654,\n",
       "          0.0640, 0.0775, 0.0868, 0.0831, 0.0823, 0.0840, 0.0806, 0.0749,\n",
       "          0.0692, 0.0851, 0.0930, 0.0914, 0.0854, 0.0981, 0.1061, 0.1010,\n",
       "          0.1019, 0.1014, 0.1129, 0.1064, 0.1101, 0.1093, 0.1202, 0.1234,\n",
       "          0.1244, 0.1351, 0.1207, 0.1413, 0.1314, 0.1441, 0.1452, 0.1437,\n",
       "          0.1590, 0.1683, 0.1707, 0.1824, 0.1860, 0.1899, 0.2098, 0.2157,\n",
       "          0.2482, 0.2735, 0.3204, 0.3580, 0.4004, 0.4460, 0.4976, 0.6189,\n",
       "          0.6895, 0.7433, 0.8547, 0.9627, 1.2527, 1.4761, 2.0968, 4.6091,\n",
       "          8.7427, 4.3431, 1.8510, 1.3097, 1.0507, 0.8757, 0.7461, 0.6414,\n",
       "          0.5483, 0.4751, 0.4031, 0.3637, 0.3469, 0.3100, 0.2977, 0.2717,\n",
       "          0.2528, 0.2242, 0.2222, 0.2181, 0.2183, 0.1937, 0.1795, 0.1745,\n",
       "          0.1742, 0.1668, 0.1576, 0.1477, 0.1490, 0.1427, 0.1314, 0.1327,\n",
       "          0.1369, 0.1255, 0.1290, 0.1193, 0.1223, 0.1139, 0.1192, 0.1077,\n",
       "          0.1079, 0.1007, 0.0987, 0.0977, 0.0997, 0.0912, 0.0879, 0.0913,\n",
       "          0.0830, 0.0911, 0.0958, 0.0906, 0.0807, 0.0970, 0.0935, 0.0799,\n",
       "          0.0801, 0.0783, 0.0810, 0.0908, 0.0976, 0.1105, 0.1353, 0.3602]]],\n",
       "       device='cuda:0'), tensor([[[ 1.4424e-02, -6.2053e-03,  1.3046e-02,  ...,  1.2778e-02,\n",
       "           2.7203e-02,  4.7139e-02],\n",
       "         [-1.1244e-02, -3.7094e-02, -2.2959e-02,  ..., -1.6171e-02,\n",
       "          -1.7366e-02, -1.6687e-02],\n",
       "         [ 5.2330e-02,  2.8765e-02,  1.8040e-02,  ...,  2.0919e-04,\n",
       "          -3.6009e-03,  1.6923e-03],\n",
       "         ...,\n",
       "         [ 4.6339e-02,  2.5617e-02,  2.1589e-02,  ..., -1.2066e-02,\n",
       "          -1.0606e-02,  1.6486e-02],\n",
       "         [-3.8792e-03, -1.9194e-02, -1.7703e-02,  ..., -9.7605e-03,\n",
       "          -2.8995e-02, -4.4986e-02],\n",
       "         [-3.3538e-03, -1.3205e-02, -1.2435e-02,  ...,  3.8488e-03,\n",
       "          -2.8648e-03, -2.2509e-03]],\n",
       "\n",
       "        [[ 9.1014e-04,  2.1759e-02, -4.6657e-04,  ...,  1.8661e-02,\n",
       "           5.6222e-03, -6.5753e-03],\n",
       "         [-8.9360e-03, -1.0034e-02, -8.6529e-03,  ..., -7.4442e-03,\n",
       "          -1.3153e-02, -3.4003e-02],\n",
       "         [ 7.9283e-02,  4.1832e-02,  3.5833e-02,  ...,  2.0648e-02,\n",
       "           1.9688e-02,  3.9761e-02],\n",
       "         ...,\n",
       "         [ 8.8727e-02,  4.5179e-02,  2.7423e-02,  ..., -5.9522e-03,\n",
       "          -1.0218e-02,  8.9648e-03],\n",
       "         [-2.2394e-02,  3.9294e-03,  5.0255e-03,  ..., -3.6512e-03,\n",
       "          -8.2149e-03, -3.7249e-02],\n",
       "         [ 2.6247e-02,  4.6901e-03, -4.8956e-03,  ...,  1.0961e-02,\n",
       "           1.1412e-02,  5.8183e-03]],\n",
       "\n",
       "        [[ 9.8864e-02,  2.4623e-02, -5.8868e-02,  ...,  7.4930e-02,\n",
       "           1.0608e-01,  1.3500e-01],\n",
       "         [ 2.2080e-01,  1.5237e-01,  1.2874e-01,  ..., -6.8108e-02,\n",
       "          -4.9663e-02, -1.1979e-02],\n",
       "         [-8.2683e-02,  5.6750e-02,  8.3949e-02,  ..., -9.2568e-02,\n",
       "          -6.7125e-02, -1.7401e-01],\n",
       "         ...,\n",
       "         [-4.2660e-01, -2.0603e-01, -1.3580e-01,  ...,  3.5648e-03,\n",
       "           8.3072e-03, -4.6678e-02],\n",
       "         [ 6.6135e-02,  1.1685e-02,  3.1982e-02,  ...,  1.3467e-02,\n",
       "           5.3755e-02,  4.5440e-03],\n",
       "         [ 1.3682e-01,  1.4482e-01,  1.5205e-01,  ...,  7.6581e-02,\n",
       "           8.6060e-02,  5.7939e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.1865e-02,  5.9571e-03, -8.3761e-03,  ..., -5.8671e-03,\n",
       "          -5.8353e-03, -1.5884e-02],\n",
       "         [ 2.1369e-02,  3.3215e-03,  9.9678e-03,  ..., -1.4906e-04,\n",
       "          -1.4306e-02,  6.3706e-03],\n",
       "         [ 1.6733e-02,  1.4137e-02,  1.6255e-02,  ...,  4.8421e-03,\n",
       "           6.4937e-03,  3.2781e-03],\n",
       "         ...,\n",
       "         [ 2.0685e-03, -1.0869e-02,  5.3385e-03,  ...,  1.9970e-02,\n",
       "           1.7058e-02,  2.1766e-02],\n",
       "         [-3.7907e-02, -2.9942e-02, -3.1849e-02,  ..., -1.3094e-02,\n",
       "          -1.2140e-02, -1.7553e-02],\n",
       "         [ 1.8043e-02,  1.5232e-02,  2.6351e-02,  ..., -1.2220e-02,\n",
       "           2.1411e-03, -8.0527e-04]],\n",
       "\n",
       "        [[ 2.9070e-02,  9.7065e-03,  4.9773e-02,  ...,  2.4412e-02,\n",
       "          -2.2531e-02,  6.7887e-03],\n",
       "         [ 2.5633e-02,  8.0056e-03,  2.8764e-02,  ..., -3.3575e-02,\n",
       "           4.9334e-03,  7.8681e-03],\n",
       "         [-1.0039e-03, -2.8478e-02,  4.3876e-02,  ...,  1.9964e-02,\n",
       "           3.4281e-02,  6.9813e-02],\n",
       "         ...,\n",
       "         [ 6.2002e-02,  3.0719e-02, -1.1294e-02,  ..., -3.0161e-02,\n",
       "           9.8971e-03,  1.9267e-02],\n",
       "         [-1.0379e-01, -8.0459e-02, -5.7399e-02,  ..., -1.0847e-01,\n",
       "          -8.7446e-02, -1.6558e-01],\n",
       "         [ 4.5195e-03, -5.8207e-03,  6.6313e-02,  ..., -4.5058e-03,\n",
       "          -8.0895e-03, -2.5703e-03]],\n",
       "\n",
       "        [[ 1.4761e-02,  3.6863e-02,  5.5135e-02,  ..., -1.0867e-02,\n",
       "          -3.7914e-03,  7.7000e-03],\n",
       "         [ 9.8720e-02,  5.7007e-02,  2.8873e-03,  ..., -1.1948e-02,\n",
       "           1.2534e-02,  1.9561e-02],\n",
       "         [ 4.7040e-02, -1.3744e-03,  3.2543e-03,  ...,  6.1925e-03,\n",
       "           2.4791e-02,  1.7735e-02],\n",
       "         ...,\n",
       "         [-1.0076e-01, -9.7916e-02, -4.9382e-02,  ..., -1.4100e-03,\n",
       "          -7.8170e-03,  1.8770e-02],\n",
       "         [-3.1102e-02,  1.1001e-03, -9.3775e-03,  ..., -7.5548e-04,\n",
       "          -2.3087e-03, -3.9874e-02],\n",
       "         [-1.1940e-01, -1.2081e-01, -5.4408e-02,  ..., -4.5960e-02,\n",
       "          -7.2085e-02, -1.6351e-01]]], device='cuda:0'), tensor([0.2858, 0.3905, 0.2084,  ..., 0.4207, 0.3822, 0.2519], device='cuda:0'), tensor([-0.0578,  0.0934,  0.0107,  ...,  0.0341, -0.0845,  0.0234],\n",
       "       device='cuda:0'), tensor([[ 0.0034, -0.0094, -0.0348,  ...,  0.0121,  0.0236,  0.1424],\n",
       "        [ 0.0303,  0.0127,  0.0109,  ...,  0.0281, -0.0207, -0.1543],\n",
       "        [ 0.0303,  0.0112,  0.0080,  ..., -0.0129, -0.0009, -0.0542],\n",
       "        ...,\n",
       "        [ 0.0237, -0.0045, -0.0243,  ...,  0.1029, -0.0575,  0.1113],\n",
       "        [-0.0271,  0.0204,  0.0053,  ..., -0.0202, -0.0393, -0.0065],\n",
       "        [-0.0003, -0.0174, -0.0220,  ...,  0.0056,  0.0225,  0.0767]],\n",
       "       device='cuda:0'), tensor([ 0.0316,  0.0004, -0.0081,  ..., -0.0017,  0.0059, -0.0143],\n",
       "       device='cuda:0'), tensor([[ 1.6352e-02, -8.4738e-03,  1.2746e-02,  ..., -1.1907e-01,\n",
       "         -2.3274e-01,  3.6517e-02],\n",
       "        [ 8.8664e-03,  6.1765e-03,  2.9845e-02,  ..., -2.6692e-02,\n",
       "          2.9905e-01,  6.3739e-03],\n",
       "        [ 7.2936e-02,  3.9058e-03,  6.2839e-03,  ...,  1.3629e-02,\n",
       "          2.6746e-01, -5.9620e-02],\n",
       "        ...,\n",
       "        [ 1.2332e-02, -1.1889e-02,  9.1507e-03,  ..., -4.7113e-02,\n",
       "         -2.7278e-02, -1.3039e-01],\n",
       "        [-1.9463e-02, -1.0264e-02,  1.7683e-04,  ..., -1.0366e-02,\n",
       "          2.9494e-01, -4.1985e-03],\n",
       "        [ 5.1310e-02, -1.1835e-03,  2.1818e-02,  ..., -5.1368e-02,\n",
       "          1.6597e-01,  2.4864e-02]], device='cuda:0'), tensor([-0.0048,  0.0108, -0.0197,  ...,  0.0007, -0.0066, -0.0095],\n",
       "       device='cuda:0'), tensor([[ 0.0051,  0.0160,  0.0102,  ..., -0.0528,  0.0883,  0.0749],\n",
       "        [-0.0403,  0.0060,  0.0323,  ..., -0.0273, -0.0515, -0.0753],\n",
       "        [ 0.0907, -0.0025, -0.0090,  ...,  0.0081,  0.0694, -0.0512],\n",
       "        ...,\n",
       "        [ 0.0109,  0.0390,  0.0219,  ..., -0.0579,  0.0182,  0.0497],\n",
       "        [ 0.0213, -0.0168, -0.0217,  ...,  0.0758, -0.0476,  0.0922],\n",
       "        [ 0.0034,  0.0342,  0.0184,  ...,  0.0036,  0.0334, -0.0852]],\n",
       "       device='cuda:0'), tensor([-0.4591,  0.0072,  0.0338,  ..., -0.2484, -0.2176,  0.0681],\n",
       "       device='cuda:0'), tensor([[ 0.0129,  0.0557,  0.0317,  ...,  0.0235, -0.0513, -0.0222],\n",
       "        [-0.1685,  0.2022, -0.0053,  ...,  0.0336,  0.0181,  0.0098],\n",
       "        [ 0.0033,  0.0672,  0.0012,  ..., -0.0124,  0.0452, -0.0469],\n",
       "        ...,\n",
       "        [-0.1093, -0.0564, -0.0465,  ..., -0.0045,  0.0722,  0.0077],\n",
       "        [-0.1574,  0.1419,  0.3187,  ...,  0.0358, -0.0856, -0.1232],\n",
       "        [ 0.0175,  0.0368, -0.0499,  ...,  0.0643, -0.0833, -0.0280]],\n",
       "       device='cuda:0'), tensor([ 0.0327,  0.0513, -0.2833,  ...,  0.1139, -0.0388, -0.0416],\n",
       "       device='cuda:0'), tensor([0.2191, 0.0004, 0.0025,  ..., 0.2080, 0.2027, 0.1881], device='cuda:0'), tensor([ 0.0634, -0.0002, -0.0020,  ...,  0.0493, -0.0383, -0.0268],\n",
       "       device='cuda:0'), tensor([[ 0.0824, -0.0175,  0.0135,  ...,  0.0479,  0.0322, -0.0494],\n",
       "        [ 0.0460,  0.1802,  0.0018,  ..., -0.1827,  0.0197,  0.3195],\n",
       "        [-0.0017,  0.0843,  0.0593,  ..., -0.0300,  0.0579,  0.0020],\n",
       "        ...,\n",
       "        [ 0.1101,  0.0306, -0.0123,  ...,  0.0220,  0.0097,  0.1066],\n",
       "        [-0.0031,  0.0133, -0.0592,  ...,  0.0433, -0.0261, -0.0570],\n",
       "        [ 0.0173,  0.0889,  0.0558,  ...,  0.1348, -0.1436,  0.0507]],\n",
       "       device='cuda:0'), tensor([-0.1215, -0.1287, -0.0606,  ..., -0.0837, -0.0837, -0.0859],\n",
       "       device='cuda:0'), tensor([[-0.0543,  0.1162, -0.0391,  ..., -0.0425,  0.0536, -0.0138],\n",
       "        [ 0.0963, -0.0373, -0.0144,  ...,  0.0538,  0.0674, -0.0177],\n",
       "        [-0.0367, -0.0359,  0.0015,  ..., -0.0982,  0.0523, -0.0243],\n",
       "        ...,\n",
       "        [-0.0091,  0.0694, -0.0854,  ...,  0.0007, -0.0443, -0.0080],\n",
       "        [-0.0848,  0.0029, -0.0150,  ...,  0.0327,  0.0626,  0.0684],\n",
       "        [ 0.0905, -0.1769, -0.0368,  ..., -0.2333, -0.0019,  0.0517]],\n",
       "       device='cuda:0'), tensor([ 0.0316, -0.0389, -0.0802,  ...,  0.1192, -0.0170, -0.1214],\n",
       "       device='cuda:0'), tensor([0.2076, 0.0908, 0.0424,  ..., 0.1606, 0.1354, 0.1532], device='cuda:0'), tensor([ 0.0493, -0.0420, -0.0356,  ...,  0.0350, -0.0071, -0.0374],\n",
       "       device='cuda:0'), tensor([[-0.2829,  0.0693, -0.0496,  ..., -0.1730,  0.0164, -0.1461],\n",
       "        [-0.1192, -0.1712,  0.0439,  ..., -0.1276,  0.0381, -0.0675],\n",
       "        [ 0.0892,  0.0826,  0.0111,  ...,  0.0898,  0.0009,  0.0294],\n",
       "        ...,\n",
       "        [-0.1159, -0.0248,  0.0342,  ..., -0.1123,  0.0860, -0.1111],\n",
       "        [-0.0036, -0.0877,  0.0625,  ...,  0.0983, -0.0189, -0.1035],\n",
       "        [ 0.1269,  0.0413, -0.0702,  ...,  0.0669,  0.0759,  0.1168]],\n",
       "       device='cuda:0'), tensor([ 0.0140,  0.0046,  0.0286,  ..., -0.0077, -0.0138, -0.0048],\n",
       "       device='cuda:0'), tensor([[-0.1141, -0.0998,  0.0458,  ..., -0.0457,  0.0577, -0.1037],\n",
       "        [ 0.1625,  0.1585,  0.0331,  ..., -0.0622, -0.3605,  0.0305],\n",
       "        [ 0.3691,  0.0186, -0.0094,  ...,  0.0267, -0.1221, -0.0918],\n",
       "        ...,\n",
       "        [ 0.1620, -0.0735,  0.0032,  ...,  0.2001,  0.1151, -0.0269],\n",
       "        [-0.1806, -0.0488,  0.0625,  ...,  0.0621, -0.0902, -0.0702],\n",
       "        [-0.0668, -0.1014, -0.0843,  ..., -0.0793, -0.0182, -0.0344]],\n",
       "       device='cuda:0'), tensor([ 0.0054, -0.0051,  0.0192,  ..., -0.0423,  0.0259,  0.0281],\n",
       "       device='cuda:0'), tensor([[-0.2028, -0.0636, -0.0807,  ..., -0.0085,  0.0121, -0.0415],\n",
       "        [-0.0602,  0.1047, -0.0265,  ...,  0.0857, -0.0186, -0.0029],\n",
       "        [ 0.0488,  0.0900, -0.0848,  ...,  0.0701, -0.0619,  0.0559],\n",
       "        ...,\n",
       "        [ 0.1495,  0.0584, -0.0013,  ..., -0.1361, -0.0836, -0.0099],\n",
       "        [-0.1227, -0.0589,  0.0753,  ..., -0.0779,  0.0211, -0.0533],\n",
       "        [-0.0634,  0.0973, -0.1156,  ...,  0.0990,  0.0262,  0.2194]],\n",
       "       device='cuda:0'), tensor([-0.0057, -0.2011, -0.1260,  ...,  0.1622, -0.4674,  0.1137],\n",
       "       device='cuda:0'), tensor([[ 0.1010, -0.0900, -0.2789,  ..., -0.1364,  0.0006,  0.0456],\n",
       "        [ 0.1489, -0.0945, -0.0530,  ..., -0.0406, -0.1492,  0.0144],\n",
       "        [ 0.0345,  0.0029,  0.0253,  ...,  0.0004, -0.0172, -0.0340],\n",
       "        ...,\n",
       "        [ 0.0219,  0.0229, -0.0262,  ..., -0.1079, -0.0211, -0.0478],\n",
       "        [-0.0874,  0.2790,  0.0309,  ...,  0.0616,  0.0674, -0.1339],\n",
       "        [ 0.1341, -0.0327,  0.0796,  ..., -0.0058,  0.1330, -0.0572]],\n",
       "       device='cuda:0'), tensor([-0.0278,  0.0151, -0.0452,  ...,  0.1265, -0.0177, -0.0818],\n",
       "       device='cuda:0'), tensor([0.2885, 0.1628, 0.1290,  ..., 0.2344, 0.2722, 0.1378], device='cuda:0'), tensor([ 3.6917e-02, -8.8901e-05, -1.7713e-04,  ...,  7.8131e-04,\n",
       "        -2.0026e-02,  1.1361e-02], device='cuda:0'), tensor([[-0.0446,  0.2569, -0.0156,  ..., -0.0375,  0.0756, -0.0348],\n",
       "        [-0.0583,  0.0206,  0.0323,  ..., -0.1438, -0.0135, -0.0289],\n",
       "        [-0.1012, -0.0230,  0.1095,  ..., -0.1501, -0.0800, -0.1593],\n",
       "        ...,\n",
       "        [ 0.1378,  0.0428, -0.0591,  ...,  0.0032,  0.1669, -0.1552],\n",
       "        [-0.0589,  0.3096, -0.0213,  ...,  0.0633,  0.0421,  0.1264],\n",
       "        [ 0.0885, -0.0024,  0.0550,  ...,  0.0667, -0.0912,  0.0597]],\n",
       "       device='cuda:0'), tensor([-0.0084, -0.0052, -0.0841,  ..., -0.0432, -0.0542, -0.0511],\n",
       "       device='cuda:0'), tensor([[-0.0117,  0.1649, -0.1122,  ...,  0.0881,  0.0882,  0.0500],\n",
       "        [ 0.0340,  0.0290,  0.1325,  ..., -0.0947, -0.0625, -0.0465],\n",
       "        [-0.0128, -0.0355,  0.0199,  ...,  0.0257, -0.0164,  0.0538],\n",
       "        ...,\n",
       "        [-0.0486,  0.0244, -0.0457,  ...,  0.0183,  0.1135,  0.0534],\n",
       "        [ 0.1447, -0.0150, -0.0868,  ..., -0.0144, -0.0437, -0.1006],\n",
       "        [-0.0655, -0.0156, -0.0092,  ..., -0.0956,  0.0251,  0.0604]],\n",
       "       device='cuda:0'), tensor([-0.0114,  0.0141, -0.0650,  ...,  0.0343,  0.0053, -0.0267],\n",
       "       device='cuda:0'), tensor([0.3395, 0.2610, 0.1770,  ..., 0.2909, 0.2883, 0.2312], device='cuda:0'), tensor([-0.0079, -0.0364, -0.0214,  ...,  0.0386, -0.0447, -0.0941],\n",
       "       device='cuda:0'), tensor([[ 1.6297e-02, -7.3382e-02,  3.8943e-02,  ..., -9.2870e-02,\n",
       "          8.0758e-02,  2.5640e-02],\n",
       "        [ 8.6599e-02, -3.9211e-03, -9.4413e-02,  ..., -4.7311e-02,\n",
       "         -3.7311e-02,  1.4513e-01],\n",
       "        [ 1.5825e-02, -1.1829e-01, -8.1813e-02,  ..., -2.2926e-04,\n",
       "          8.5109e-02,  2.8741e-02],\n",
       "        ...,\n",
       "        [ 9.0706e-02, -1.9482e-01, -7.2465e-01,  ..., -9.4776e-02,\n",
       "          1.2484e-02,  1.2909e-01],\n",
       "        [ 1.2383e-01,  4.8293e-02, -5.7895e-01,  ...,  5.5878e-02,\n",
       "         -5.7925e-02,  8.5515e-02],\n",
       "        [-9.0963e-02,  5.5280e-02, -3.1517e-01,  ...,  1.4835e-01,\n",
       "          2.1312e-02, -1.1604e-01]], device='cuda:0'), tensor([ 0.0088,  0.0114,  0.0009,  ..., -0.0710,  0.1172,  0.0330],\n",
       "       device='cuda:0'), tensor([[ 0.1445,  0.0452, -0.0046,  ..., -0.1614,  0.0378,  0.0183],\n",
       "        [-0.0401, -0.0944,  0.0173,  ..., -0.1548, -0.0483, -0.0402],\n",
       "        [-0.0451, -0.0937,  0.0112,  ...,  0.1439,  0.1508,  0.0250],\n",
       "        ...,\n",
       "        [-0.0777, -0.0454, -0.0288,  ...,  0.0583,  0.0273, -0.0012],\n",
       "        [ 0.0623, -0.0498,  0.0090,  ..., -0.0192,  0.0717,  0.0034],\n",
       "        [ 0.0761,  0.0329, -0.0251,  ..., -0.0335,  0.0187,  0.0249]],\n",
       "       device='cuda:0'), tensor([ 0.0249, -0.0343,  0.1169,  ..., -0.0003, -0.0017,  0.0054],\n",
       "       device='cuda:0'), tensor([[ 0.0638,  0.1078,  0.0895,  ...,  0.1012,  0.1212,  0.0612],\n",
       "        [-0.0192,  0.0602, -0.0040,  ...,  0.0234, -0.0440,  0.1203],\n",
       "        [ 0.0947,  0.0326, -0.0347,  ..., -0.0955, -0.1139,  0.0246],\n",
       "        ...,\n",
       "        [ 0.1291, -0.0083, -0.4675,  ..., -0.0124, -0.0557,  0.0313],\n",
       "        [ 0.0305,  0.0560, -0.4129,  ...,  0.1340, -0.0162,  0.0464],\n",
       "        [ 0.0609,  0.1972, -0.1074,  ..., -0.1848, -0.0788, -0.0329]],\n",
       "       device='cuda:0'), tensor([-0.1255,  0.5548,  0.0435,  ..., -0.2995,  0.2302,  0.0048],\n",
       "       device='cuda:0'), tensor([[-0.1660,  0.0407,  0.1552,  ...,  0.0911, -0.1686, -0.2060],\n",
       "        [-0.0406, -0.0505, -0.1171,  ...,  0.0879,  0.0157,  0.0068],\n",
       "        [-0.0227, -0.0564, -0.0360,  ..., -0.0143,  0.0152,  0.0509],\n",
       "        ...,\n",
       "        [ 0.0380,  0.0695, -0.1679,  ..., -0.0022, -0.1617, -0.0301],\n",
       "        [-0.0528,  0.1081, -0.1983,  ...,  0.0458,  0.0121,  0.0383],\n",
       "        [-0.0889,  0.0495,  0.0150,  ...,  0.1293,  0.0539, -0.0095]],\n",
       "       device='cuda:0'), tensor([ 0.0007, -0.0336, -0.0417,  ...,  0.0970,  0.0238, -0.0717],\n",
       "       device='cuda:0'), tensor([0.3061, 0.2418, 0.2864,  ..., 0.2532, 0.2957, 0.1815], device='cuda:0'), tensor([-0.0209,  0.0423,  0.0013,  ..., -0.0100, -0.0152,  0.0290],\n",
       "       device='cuda:0'), tensor([[-8.8790e-02, -3.1132e-02, -1.1971e-01,  ..., -8.6871e-03,\n",
       "         -5.3573e-02, -1.9731e-01],\n",
       "        [ 1.2726e-04,  6.1563e-02, -4.4537e-02,  ...,  1.1975e-01,\n",
       "          9.5133e-02,  1.5990e-01],\n",
       "        [ 7.6157e-03, -8.7615e-03,  7.2864e-02,  ..., -4.0548e-02,\n",
       "          2.7660e-02,  3.7607e-02],\n",
       "        ...,\n",
       "        [ 1.7616e-01, -1.1987e-02,  1.4833e-02,  ..., -2.9650e-02,\n",
       "         -3.7827e-03,  1.8840e-01],\n",
       "        [ 1.1761e-01,  1.0911e-01,  2.6353e-02,  ...,  1.1574e-01,\n",
       "         -8.1724e-02, -1.6765e-02],\n",
       "        [-2.5391e-02,  1.0741e-01,  7.3954e-02,  ..., -3.5276e-02,\n",
       "         -4.4665e-02, -5.2952e-02]], device='cuda:0'), tensor([-0.0410, -0.0251,  0.0033,  ..., -0.0397,  0.0326, -0.0212],\n",
       "       device='cuda:0'), tensor([[ 0.0300,  0.0123, -0.0560,  ...,  0.0005,  0.1645, -0.0083],\n",
       "        [ 0.0778,  0.1540,  0.0542,  ..., -0.0663,  0.0368,  0.0608],\n",
       "        [ 0.0607, -0.0083,  0.0205,  ..., -0.0893,  0.0391, -0.0249],\n",
       "        ...,\n",
       "        [-0.0170, -0.0242, -0.0205,  ...,  0.0165,  0.1549, -0.0708],\n",
       "        [ 0.0271,  0.0078, -0.0479,  ..., -0.0791, -0.0924,  0.1401],\n",
       "        [ 0.2441, -0.0163,  0.0102,  ...,  0.0524,  0.0097, -0.0176]],\n",
       "       device='cuda:0'), tensor([ 0.0440, -0.0454, -0.0324,  ...,  0.0713, -0.0197, -0.0468],\n",
       "       device='cuda:0'), tensor([0.3207, 0.2869, 0.1276,  ..., 0.2959, 0.2821, 0.2649], device='cuda:0'), tensor([-0.0862,  0.0614, -0.2002,  ...,  0.0106, -0.0256, -0.0733],\n",
       "       device='cuda:0'), tensor([[ 0.0200, -0.0658,  0.0519,  ..., -0.0978,  0.0330,  0.0627],\n",
       "        [ 0.0628,  0.0279, -0.0147,  ..., -0.0125,  0.0199, -0.0535],\n",
       "        [-0.0783, -0.0101,  0.0052,  ...,  0.0336,  0.0783,  0.1276],\n",
       "        ...,\n",
       "        [-0.0231, -0.0402, -0.0356,  ...,  0.0675,  0.0228, -0.1023],\n",
       "        [-0.1786,  0.0194,  0.0670,  ..., -0.0082, -0.0562,  0.0499],\n",
       "        [ 0.1750,  0.0097, -0.1717,  ...,  0.0311, -0.1063, -0.1704]],\n",
       "       device='cuda:0'), tensor([-0.0059,  0.0182,  0.0020,  ..., -0.0179,  0.0221,  0.0337],\n",
       "       device='cuda:0'), tensor([[ 0.1307, -0.0924, -0.0115,  ...,  0.1105, -0.0202,  0.0184],\n",
       "        [-0.1248, -0.0490, -0.0491,  ...,  0.0941, -0.0735, -0.1795],\n",
       "        [-0.0135, -0.0915, -0.0466,  ..., -0.1309,  0.1490,  0.0433],\n",
       "        ...,\n",
       "        [ 0.0730, -0.2146, -0.0020,  ..., -0.0328,  0.0878, -0.0384],\n",
       "        [-0.1186, -0.1334,  0.0206,  ...,  0.1573, -0.0285,  0.0284],\n",
       "        [ 0.0570,  0.0429, -0.0119,  ..., -0.2065,  0.0022,  0.1305]],\n",
       "       device='cuda:0'), tensor([-0.0211, -0.0053,  0.0378,  ..., -0.0006, -0.1287, -0.0190],\n",
       "       device='cuda:0'), tensor([[ 1.0705e-01,  5.6310e-02,  1.1893e-02,  ...,  5.3247e-03,\n",
       "         -2.9575e-05,  1.1019e-01],\n",
       "        [ 1.0415e-01,  1.6980e-02,  6.5154e-02,  ..., -1.2958e-01,\n",
       "         -7.5502e-02, -6.4326e-02],\n",
       "        [ 4.4001e-02,  8.2917e-02, -4.1481e-02,  ..., -4.4211e-02,\n",
       "         -5.7606e-02,  5.0808e-02],\n",
       "        ...,\n",
       "        [-2.2219e-03, -7.3808e-03, -6.3760e-02,  ...,  1.9912e-02,\n",
       "          4.2495e-02, -2.6558e-02],\n",
       "        [-1.0748e-01, -1.0326e-01,  3.5654e-02,  ..., -3.6551e-02,\n",
       "         -4.4523e-02,  1.3152e-01],\n",
       "        [ 3.0739e-02,  1.3265e-02, -1.6064e-01,  ..., -6.0213e-02,\n",
       "          1.3490e-01,  3.0273e-02]], device='cuda:0'), tensor([ 0.1537, -0.1088, -0.0499,  ...,  0.1869,  0.1466,  0.0714],\n",
       "       device='cuda:0'), tensor([[-0.1888,  0.0708,  0.0685,  ...,  0.0527,  0.0486, -0.1630],\n",
       "        [-0.0668, -0.1211, -0.0331,  ..., -0.0002,  0.0430, -0.0635],\n",
       "        [-0.0045,  0.0295,  0.0559,  ..., -0.0010, -0.0034,  0.0665],\n",
       "        ...,\n",
       "        [-0.0782, -0.0754,  0.0408,  ...,  0.0244, -0.0597,  0.0525],\n",
       "        [ 0.0589,  0.0963, -0.0206,  ..., -0.0259,  0.0835,  0.0627],\n",
       "        [-0.0444, -0.0652,  0.1189,  ..., -0.1229, -0.0471, -0.0857]],\n",
       "       device='cuda:0'), tensor([ 0.0952, -0.0239, -0.1014,  ...,  0.0512, -0.0482, -0.0889],\n",
       "       device='cuda:0'), tensor([0.3022, 0.2321, 0.4688,  ..., 0.2795, 0.2805, 0.2366], device='cuda:0'), tensor([-0.0016, -0.0031,  0.0390,  ...,  0.0057,  0.0172,  0.0232],\n",
       "       device='cuda:0'), tensor([[ 0.0312, -0.1710,  0.0247,  ..., -0.0425,  0.0073,  0.1272],\n",
       "        [ 0.1482, -0.0101, -0.0167,  ..., -0.0990, -0.0132, -0.1926],\n",
       "        [-0.0429, -0.2456, -0.0160,  ..., -0.0472, -0.0958,  0.0018],\n",
       "        ...,\n",
       "        [-0.0179, -0.0150,  0.0976,  ...,  0.0421,  0.0188,  0.2394],\n",
       "        [ 0.0062,  0.1625,  0.0275,  ..., -0.1488,  0.3113,  0.1961],\n",
       "        [-0.1053, -0.1545,  0.0723,  ..., -0.1259,  0.0563,  0.0179]],\n",
       "       device='cuda:0'), tensor([-0.0252,  0.0118, -0.0298,  ..., -0.0516, -0.0261, -0.0369],\n",
       "       device='cuda:0'), tensor([[-0.1248, -0.0332,  0.0641,  ..., -0.0238, -0.0585, -0.1648],\n",
       "        [ 0.0730, -0.0629,  0.0587,  ..., -0.0223,  0.0826,  0.1190],\n",
       "        [-0.0069,  0.0521, -0.0110,  ...,  0.0170, -0.0250,  0.1306],\n",
       "        ...,\n",
       "        [ 0.0190,  0.0538,  0.0018,  ...,  0.0130,  0.1287,  0.0906],\n",
       "        [ 0.0666, -0.0218,  0.0376,  ..., -0.0827,  0.1051, -0.0078],\n",
       "        [ 0.0364,  0.1766, -0.0866,  ..., -0.0268, -0.1214, -0.0541]],\n",
       "       device='cuda:0'), tensor([ 0.0338, -0.0230, -0.0337,  ..., -0.0417,  0.0075, -0.0231],\n",
       "       device='cuda:0'), tensor([0.2977, 0.2884, 0.0907,  ..., 0.3000, 0.3102, 0.2804], device='cuda:0'), tensor([ 0.0339, -0.0451, -0.2057,  ...,  0.0721, -0.0982, -0.0757],\n",
       "       device='cuda:0'), tensor([[-0.1502, -0.0655, -0.0464,  ..., -0.0716,  0.0628, -0.0141],\n",
       "        [ 0.0973, -0.0968, -0.0685,  ...,  0.0277,  0.0281, -0.0655],\n",
       "        [ 0.1241,  0.0006,  0.0855,  ..., -0.0012,  0.0443,  0.0102],\n",
       "        ...,\n",
       "        [ 0.0062, -0.0818,  0.0021,  ...,  0.1731, -0.1352,  0.1789],\n",
       "        [ 0.0913,  0.1076,  0.0331,  ...,  0.0845,  0.1491,  0.0097],\n",
       "        [ 0.1780,  0.1662, -0.0206,  ...,  0.1726, -0.1259,  0.0331]],\n",
       "       device='cuda:0'), tensor([-0.0122, -0.0045, -0.0035,  ..., -0.0055,  0.0098, -0.0198],\n",
       "       device='cuda:0'), tensor([[-0.0137, -0.1230,  0.0173,  ..., -0.0407, -0.0259,  0.1208],\n",
       "        [ 0.0603, -0.0947,  0.0621,  ..., -0.2213,  0.2030,  0.0100],\n",
       "        [ 0.0388,  0.1585,  0.0654,  ..., -0.1092,  0.2483,  0.0741],\n",
       "        ...,\n",
       "        [ 0.1994, -0.2035,  0.0271,  ..., -0.0180, -0.0141, -0.1684],\n",
       "        [ 0.0077,  0.0315,  0.0234,  ..., -0.0817, -0.0016,  0.0654],\n",
       "        [ 0.0201, -0.1476, -0.0460,  ...,  0.0079, -0.0613, -0.0580]],\n",
       "       device='cuda:0'), tensor([-0.0748, -0.0151, -0.0098,  ..., -0.0177,  0.0221,  0.0096],\n",
       "       device='cuda:0'), tensor([[-0.1482, -0.0313, -0.0261,  ...,  0.0143, -0.0727, -0.2050],\n",
       "        [-0.1048,  0.0374, -0.0288,  ...,  0.0113, -0.0789, -0.0877],\n",
       "        [-0.1944, -0.1960,  0.0334,  ..., -0.0271, -0.0679,  0.0382],\n",
       "        ...,\n",
       "        [-0.0759, -0.0040,  0.0032,  ..., -0.0587,  0.0014,  0.1272],\n",
       "        [ 0.1044,  0.0782,  0.0436,  ...,  0.0217,  0.1007,  0.1067],\n",
       "        [ 0.1711,  0.0303, -0.0457,  ...,  0.0661,  0.0205,  0.2080]],\n",
       "       device='cuda:0'), tensor([ 0.1145, -0.5338, -0.0045,  ...,  0.3750, -0.1059,  0.1378],\n",
       "       device='cuda:0'), tensor([[ 0.0487, -0.0594, -0.0408,  ..., -0.1650,  0.0977, -0.0252],\n",
       "        [ 0.1145,  0.0395, -0.1234,  ...,  0.2131, -0.0003,  0.1512],\n",
       "        [-0.0614, -0.0751, -0.1269,  ..., -0.0975, -0.0543, -0.0281],\n",
       "        ...,\n",
       "        [-0.0918,  0.1854,  0.0849,  ...,  0.0201,  0.0785, -0.0563],\n",
       "        [-0.0008, -0.0120, -0.1019,  ..., -0.0070,  0.0394, -0.0927],\n",
       "        [-0.0279,  0.0379, -0.0435,  ...,  0.1431,  0.0953, -0.0249]],\n",
       "       device='cuda:0'), tensor([ 0.0347, -0.0280, -0.0536,  ...,  0.0371, -0.0627, -0.0486],\n",
       "       device='cuda:0'), tensor([0.2921, 0.2657, 0.3213,  ..., 0.2705, 0.2848, 0.2550], device='cuda:0'), tensor([-0.0101,  0.0150,  0.0392,  ..., -0.0178,  0.0423, -0.0033],\n",
       "       device='cuda:0'), tensor([[-0.0902, -0.0038,  0.0692,  ..., -0.1333, -0.0415, -0.0100],\n",
       "        [ 0.0125,  0.0420,  0.0105,  ...,  0.2528,  0.0909, -0.2038],\n",
       "        [ 0.1759, -0.1963,  0.0629,  ...,  0.0077, -0.0309,  0.0479],\n",
       "        ...,\n",
       "        [-0.0072,  0.0497,  0.0847,  ..., -0.0163,  0.0302,  0.0028],\n",
       "        [ 0.0571, -0.0950, -0.0644,  ...,  0.2633, -0.0726, -0.0075],\n",
       "        [-0.0781,  0.0758, -0.0056,  ...,  0.1174,  0.0163,  0.0666]],\n",
       "       device='cuda:0'), tensor([-0.0177, -0.0601, -0.0621,  ..., -0.0069, -0.0191, -0.0178],\n",
       "       device='cuda:0'), tensor([[-0.0348, -0.0204,  0.2162,  ..., -0.0688, -0.0133,  0.0988],\n",
       "        [-0.0241,  0.0325, -0.0941,  ..., -0.1623,  0.0238, -0.0216],\n",
       "        [-0.0551,  0.0972,  0.0242,  ...,  0.0745,  0.0597,  0.0021],\n",
       "        ...,\n",
       "        [ 0.1635, -0.0185,  0.0334,  ..., -0.1604, -0.1293, -0.1636],\n",
       "        [-0.0302,  0.0257, -0.0373,  ..., -0.1074,  0.0175, -0.0698],\n",
       "        [ 0.0935, -0.1543, -0.1689,  ...,  0.1699,  0.0109, -0.0779]],\n",
       "       device='cuda:0'), tensor([ 0.0106, -0.0359, -0.0235,  ..., -0.0268, -0.0035, -0.0529],\n",
       "       device='cuda:0'), tensor([0.2974, 0.3008, 0.1787,  ..., 0.2958, 0.3168, 0.2826], device='cuda:0'), tensor([-0.0016,  0.0887, -0.0941,  ..., -0.0098, -0.0092, -0.1247],\n",
       "       device='cuda:0'), tensor([[-0.1083,  0.1882,  0.0275,  ..., -0.0930,  0.0249, -0.0907],\n",
       "        [-0.0025, -0.0376,  0.0356,  ...,  0.0301, -0.1799,  0.0594],\n",
       "        [-0.2200,  0.2874,  0.0415,  ...,  0.0166, -0.0642, -0.0735],\n",
       "        ...,\n",
       "        [ 0.0119, -0.0368,  0.0681,  ..., -0.0003, -0.0827,  0.0022],\n",
       "        [ 0.0138,  0.1648, -0.0059,  ...,  0.0464,  0.0604, -0.0494],\n",
       "        [ 0.1433, -0.0899, -0.0176,  ...,  0.2785, -0.0438,  0.0639]],\n",
       "       device='cuda:0'), tensor([-0.0154,  0.0009,  0.0034,  ..., -0.0044,  0.0559, -0.0204],\n",
       "       device='cuda:0'), tensor([[ 0.1065, -0.0753, -0.0980,  ..., -0.0195,  0.0155, -0.0426],\n",
       "        [-0.0560, -0.0637,  0.0430,  ..., -0.0229,  0.2150, -0.1534],\n",
       "        [ 0.0993, -0.0682,  0.0321,  ..., -0.0287, -0.0292,  0.0589],\n",
       "        ...,\n",
       "        [ 0.0101, -0.0089, -0.0317,  ..., -0.0377, -0.0183,  0.1921],\n",
       "        [-0.0282, -0.1333,  0.0552,  ..., -0.0822, -0.1391, -0.2454],\n",
       "        [-0.1026,  0.0628, -0.0491,  ..., -0.0828, -0.0427, -0.1064]],\n",
       "       device='cuda:0'), tensor([-0.0233,  0.0290, -0.0555,  ..., -0.0087, -0.0494,  0.0049],\n",
       "       device='cuda:0'), tensor([[-0.1212,  0.0468, -0.0745,  ..., -0.1494, -0.1471,  0.0701],\n",
       "        [ 0.0728, -0.2170,  0.0783,  ..., -0.0715, -0.0154, -0.0021],\n",
       "        [ 0.0203,  0.0970, -0.0291,  ..., -0.1226,  0.0313,  0.0214],\n",
       "        ...,\n",
       "        [-0.0472, -0.0136,  0.1642,  ...,  0.1491,  0.0209,  0.1453],\n",
       "        [ 0.0958,  0.1156, -0.0315,  ..., -0.1868,  0.1290, -0.0433],\n",
       "        [ 0.0459, -0.0160,  0.0016,  ...,  0.0757,  0.1750,  0.0698]],\n",
       "       device='cuda:0'), tensor([-0.0144,  0.1063, -0.4193,  ..., -0.2547,  0.1319, -0.0994],\n",
       "       device='cuda:0'), tensor([[-0.1520,  0.0383, -0.0501,  ..., -0.0930,  0.0480,  0.0814],\n",
       "        [ 0.0703, -0.0346,  0.2088,  ...,  0.0016,  0.0414, -0.1032],\n",
       "        [ 0.0490, -0.0133,  0.0699,  ...,  0.0418, -0.0129,  0.0478],\n",
       "        ...,\n",
       "        [-0.0290,  0.0423, -0.0775,  ..., -0.0093, -0.0909,  0.0445],\n",
       "        [ 0.0543, -0.1600,  0.0592,  ..., -0.0594,  0.0139,  0.0218],\n",
       "        [-0.0763,  0.0614, -0.0901,  ..., -0.2531,  0.3076,  0.1138]],\n",
       "       device='cuda:0'), tensor([ 0.0269,  0.0277, -0.0976,  ..., -0.0272, -0.0563, -0.1119],\n",
       "       device='cuda:0'), tensor([0.2966, 0.2679, 0.3514,  ..., 0.3170, 0.2913, 0.2573], device='cuda:0'), tensor([-0.0020, -0.0239,  0.0367,  ..., -0.0030,  0.0506, -0.0471],\n",
       "       device='cuda:0'), tensor([[-0.2537, -0.1094,  0.0471,  ..., -0.0347, -0.1293,  0.0898],\n",
       "        [ 0.0304, -0.0560, -0.0448,  ...,  0.0165,  0.0405,  0.1743],\n",
       "        [-0.1345, -0.0218, -0.0590,  ..., -0.0259,  0.0577, -0.1115],\n",
       "        ...,\n",
       "        [-0.0442,  0.0896, -0.0302,  ...,  0.0163, -0.1050,  0.1119],\n",
       "        [ 0.1591, -0.1917,  0.0564,  ..., -0.1987,  0.0887, -0.0793],\n",
       "        [-0.1561, -0.0667, -0.0613,  ...,  0.1583, -0.1308, -0.0804]],\n",
       "       device='cuda:0'), tensor([-0.0726,  0.0205, -0.0720,  ...,  0.0452, -0.0534, -0.0657],\n",
       "       device='cuda:0'), tensor([[-0.2649, -0.1274, -0.0029,  ...,  0.0418, -0.0020,  0.1431],\n",
       "        [ 0.0712,  0.0211, -0.0141,  ..., -0.0221,  0.0577, -0.0360],\n",
       "        [ 0.0307,  0.0725,  0.0109,  ...,  0.0110, -0.0600, -0.1174],\n",
       "        ...,\n",
       "        [-0.1453, -0.0785, -0.1277,  ..., -0.0663,  0.2582,  0.2073],\n",
       "        [ 0.0108,  0.0292, -0.0432,  ...,  0.1548,  0.0629, -0.1260],\n",
       "        [ 0.0870, -0.0636,  0.0276,  ..., -0.1357,  0.0789, -0.3189]],\n",
       "       device='cuda:0'), tensor([-0.0263, -0.0359, -0.0037,  ..., -0.0643, -0.0520, -0.0516],\n",
       "       device='cuda:0'), tensor([0.3075, 0.2646, 0.1213,  ..., 0.2998, 0.3011, 0.2866], device='cuda:0'), tensor([-0.0076,  0.0294, -0.1609,  ...,  0.0428, -0.0421, -0.1653],\n",
       "       device='cuda:0'), tensor([[ 0.0458,  0.1068,  0.0406,  ...,  0.0095, -0.0283, -0.1045],\n",
       "        [-0.0531,  0.1343,  0.0963,  ...,  0.0569, -0.0356,  0.1120],\n",
       "        [ 0.0521,  0.0038,  0.1317,  ..., -0.1186, -0.1129,  0.0335],\n",
       "        ...,\n",
       "        [ 0.1267, -0.0677,  0.1930,  ...,  0.1495,  0.0366, -0.0038],\n",
       "        [ 0.0118,  0.0425,  0.3247,  ...,  0.0980, -0.0540, -0.1834],\n",
       "        [ 0.1667,  0.1935, -0.2099,  ...,  0.0957, -0.0285,  0.0214]],\n",
       "       device='cuda:0'), tensor([ 0.0111, -0.0078, -0.0175,  ..., -0.0449, -0.0017, -0.0076],\n",
       "       device='cuda:0'), tensor([[ 0.0243, -0.2390,  0.1659,  ...,  0.0092, -0.1053, -0.1521],\n",
       "        [-0.1888, -0.0191, -0.0896,  ..., -0.0188,  0.1186,  0.2407],\n",
       "        [-0.2908, -0.1042,  0.1412,  ...,  0.1572,  0.2493,  0.0038],\n",
       "        ...,\n",
       "        [ 0.0239, -0.1181,  0.0283,  ...,  0.1060,  0.0333, -0.0203],\n",
       "        [-0.0353, -0.0019, -0.0411,  ..., -0.0090, -0.0811,  0.0931],\n",
       "        [-0.0851, -0.0305,  0.0282,  ..., -0.0041,  0.1109, -0.1044]],\n",
       "       device='cuda:0'), tensor([-0.0476, -0.0644,  0.0347,  ...,  0.0310, -0.0153,  0.0190],\n",
       "       device='cuda:0'), tensor([[ 0.0955,  0.2855,  0.0872,  ...,  0.1037, -0.1067, -0.0224],\n",
       "        [-0.0310, -0.0953,  0.0813,  ..., -0.1237,  0.0605, -0.0621],\n",
       "        [-0.0216,  0.0339,  0.1444,  ..., -0.0617, -0.0970, -0.0276],\n",
       "        ...,\n",
       "        [-0.0871,  0.0329,  0.0397,  ...,  0.0418, -0.0116, -0.1459],\n",
       "        [ 0.0877,  0.1116,  0.1397,  ..., -0.1270,  0.0458, -0.0719],\n",
       "        [-0.0516, -0.0233,  0.4097,  ..., -0.2265, -0.1153, -0.0247]],\n",
       "       device='cuda:0'), tensor([ 0.2523,  0.1725,  0.1885,  ...,  0.5249, -0.0226,  0.5791],\n",
       "       device='cuda:0'), tensor([[-3.5622e-02,  8.6844e-02,  2.1572e-01,  ..., -4.8987e-02,\n",
       "         -5.8903e-02,  1.6960e-01],\n",
       "        [ 1.4580e-01,  5.5663e-02,  7.5421e-02,  ..., -4.9076e-02,\n",
       "          2.7617e-02, -1.5888e-01],\n",
       "        [-7.4971e-01,  3.7030e-03, -5.8752e-01,  ...,  2.0447e-01,\n",
       "         -2.5356e-03, -1.9202e-01],\n",
       "        ...,\n",
       "        [ 1.7552e-03,  2.8262e-02,  3.1943e-03,  ...,  2.4045e-02,\n",
       "         -4.7784e-02,  3.3811e-02],\n",
       "        [ 9.0217e-02, -1.0115e-01, -2.2230e-01,  ...,  2.0728e-02,\n",
       "         -1.2048e-01,  4.2031e-02],\n",
       "        [ 1.3922e-01, -2.3958e-01, -3.6004e-02,  ...,  3.6417e-03,\n",
       "         -9.4666e-02,  4.0464e-04]], device='cuda:0'), tensor([ 0.0138,  0.0365, -0.0377,  ..., -0.0217, -0.0794, -0.0241],\n",
       "       device='cuda:0'), tensor([0.2915, 0.2447, 0.3949,  ..., 0.2873, 0.2495, 0.2475], device='cuda:0'), tensor([ 0.0318, -0.0048,  0.0414,  ..., -0.0127,  0.0526, -0.0148],\n",
       "       device='cuda:0'), tensor([[ 0.0151, -0.0933, -0.0487,  ..., -0.1015, -0.1050,  0.0791],\n",
       "        [-0.0657, -0.0063,  0.1824,  ..., -0.0228, -0.0453, -0.1530],\n",
       "        [ 0.2109,  0.0373,  0.0759,  ..., -0.0368, -0.0279, -0.1026],\n",
       "        ...,\n",
       "        [-0.0473,  0.1246,  0.0618,  ...,  0.0820,  0.0979, -0.0571],\n",
       "        [ 0.0458,  0.1138, -0.0345,  ..., -0.1157, -0.0948, -0.0555],\n",
       "        [-0.0746,  0.1019, -0.0040,  ..., -0.0706,  0.1844, -0.0350]],\n",
       "       device='cuda:0'), tensor([-0.0285, -0.0505, -0.0809,  ..., -0.0521, -0.0214, -0.0303],\n",
       "       device='cuda:0'), tensor([[-0.1683,  0.0112,  0.1388,  ...,  0.0354,  0.0730, -0.0687],\n",
       "        [ 0.0706, -0.1132,  0.0030,  ..., -0.1580, -0.0242,  0.0527],\n",
       "        [ 0.0058,  0.1221, -0.0627,  ...,  0.1359, -0.0081, -0.0364],\n",
       "        ...,\n",
       "        [-0.0020,  0.1827, -0.1228,  ..., -0.1194, -0.0236,  0.0120],\n",
       "        [-0.0026,  0.0022,  0.0944,  ..., -0.0208, -0.1833, -0.0796],\n",
       "        [ 0.0038, -0.1808, -0.1070,  ...,  0.0120,  0.1234,  0.2279]],\n",
       "       device='cuda:0'), tensor([-0.0198,  0.0178,  0.0231,  ..., -0.0598, -0.0718, -0.0034],\n",
       "       device='cuda:0'), tensor([0.2950, 0.3130, 0.1532,  ..., 0.2774, 0.2943, 0.2884], device='cuda:0'), tensor([ 0.0069,  0.0668, -0.0945,  ...,  0.0119,  0.0045, -0.0810],\n",
       "       device='cuda:0'), tensor([[-0.0835, -0.1020,  0.2266,  ..., -0.0284, -0.0482, -0.0721],\n",
       "        [-0.0533,  0.0463,  0.0932,  ..., -0.0098,  0.2319, -0.1336],\n",
       "        [ 0.0005, -0.0019,  0.0548,  ..., -0.0367, -0.1450,  0.0034],\n",
       "        ...,\n",
       "        [ 0.0005, -0.0148, -0.0228,  ...,  0.0761, -0.1317,  0.0593],\n",
       "        [-0.1486, -0.0067,  0.1679,  ..., -0.1251,  0.1967,  0.0035],\n",
       "        [ 0.1625,  0.0164, -0.0460,  ..., -0.0802,  0.0174,  0.1565]],\n",
       "       device='cuda:0'), tensor([ 0.0069, -0.0222, -0.0443,  ..., -0.1297, -0.0706, -0.0159],\n",
       "       device='cuda:0'), tensor([[-0.1856,  0.1261,  0.0478,  ...,  0.1768, -0.1066, -0.1072],\n",
       "        [ 0.1171,  0.2300, -0.0047,  ...,  0.2785,  0.0641, -0.0063],\n",
       "        [ 0.0319, -0.1443, -0.0007,  ..., -0.1551,  0.0521,  0.0004],\n",
       "        ...,\n",
       "        [ 0.1962,  0.0898,  0.0976,  ...,  0.1742,  0.1512, -0.0478],\n",
       "        [-0.2520, -0.0069,  0.1080,  ..., -0.1610,  0.0273,  0.0439],\n",
       "        [-0.1201,  0.0602,  0.0347,  ...,  0.0468, -0.1094,  0.0497]],\n",
       "       device='cuda:0'), tensor([-0.0231, -0.0154, -0.0199,  ...,  0.0523, -0.0340, -0.0639],\n",
       "       device='cuda:0'), tensor([[ 0.0782, -0.0918,  0.1771,  ..., -0.2299, -0.0242,  0.0552],\n",
       "        [-0.1429,  0.0887,  0.0434,  ..., -0.0171,  0.0438, -0.0077],\n",
       "        [-0.0934, -0.1847,  0.2038,  ...,  0.0277, -0.0906,  0.0296],\n",
       "        ...,\n",
       "        [-0.0654, -0.0551,  0.0225,  ..., -0.0841,  0.0240,  0.0657],\n",
       "        [ 0.0213,  0.3120, -0.0577,  ..., -0.1740,  0.0534,  0.2049],\n",
       "        [ 0.0827,  0.0790, -0.0982,  ...,  0.1862,  0.0204, -0.0368]],\n",
       "       device='cuda:0'), tensor([ 0.0742,  0.0263,  0.3139,  ...,  0.2023,  0.1736, -0.0148],\n",
       "       device='cuda:0'), tensor([[ 0.1060, -0.1020, -0.1650,  ...,  0.0324,  0.1000, -0.1714],\n",
       "        [-0.0356, -0.1473,  0.0623,  ...,  0.1018, -0.0326, -0.0818],\n",
       "        [-0.0593, -0.1445,  0.0522,  ..., -0.1585,  0.1007,  0.0906],\n",
       "        ...,\n",
       "        [ 0.0080, -0.0781,  0.0921,  ...,  0.1654,  0.0899,  0.0596],\n",
       "        [ 0.0638, -0.1158, -0.0581,  ...,  0.0418, -0.1076, -0.0446],\n",
       "        [ 0.0552, -0.0074,  0.0085,  ..., -0.0171, -0.0777, -0.1338]],\n",
       "       device='cuda:0'), tensor([-0.0260,  0.0537, -0.0452,  ..., -0.0146, -0.0622, -0.0380],\n",
       "       device='cuda:0'), tensor([0.3291, 0.2713, 0.2065,  ..., 0.2877, 0.2811, 0.2975], device='cuda:0'), tensor([ 0.0252, -0.0121,  0.0275,  ..., -0.0127,  0.0411,  0.0067],\n",
       "       device='cuda:0'), tensor([[ 0.0152, -0.0769, -0.0760,  ...,  0.0129, -0.0731,  0.1756],\n",
       "        [ 0.0363,  0.0298,  0.0436,  ...,  0.0107, -0.2092, -0.0828],\n",
       "        [-0.0691,  0.0475, -0.0329,  ...,  0.1414,  0.0246,  0.0586],\n",
       "        ...,\n",
       "        [ 0.2360, -0.0515,  0.0013,  ...,  0.0354,  0.0512,  0.0983],\n",
       "        [ 0.0166, -0.1108, -0.0043,  ...,  0.0163,  0.2043, -0.1333],\n",
       "        [-0.0787, -0.2722,  0.0927,  ..., -0.2377, -0.0820,  0.1618]],\n",
       "       device='cuda:0'), tensor([-0.0505, -0.0552,  0.0071,  ..., -0.0461, -0.0584, -0.0803],\n",
       "       device='cuda:0'), tensor([[-0.1455, -0.0785,  0.1346,  ...,  0.0549,  0.0086,  0.0355],\n",
       "        [-0.3280,  0.0094, -0.0140,  ..., -0.0724,  0.1739, -0.0255],\n",
       "        [-0.0457, -0.0906,  0.0970,  ...,  0.0028,  0.1139,  0.0658],\n",
       "        ...,\n",
       "        [-0.0683,  0.1172, -0.1289,  ...,  0.1833,  0.1975, -0.0786],\n",
       "        [-0.0583,  0.2376, -0.0076,  ...,  0.2063, -0.0554, -0.0243],\n",
       "        [ 0.1475, -0.1537, -0.1676,  ..., -0.0289,  0.1116, -0.0360]],\n",
       "       device='cuda:0'), tensor([ 0.0096, -0.0239,  0.0460,  ..., -0.0405, -0.0859,  0.0500],\n",
       "       device='cuda:0'), tensor([0.3463, 0.3219, 0.1595,  ..., 0.3102, 0.3182, 0.2821], device='cuda:0'), tensor([-0.0701,  0.1050, -0.1128,  ..., -0.0039, -0.0325, -0.0767],\n",
       "       device='cuda:0'), tensor([[ 0.0207,  0.0107, -0.0825,  ..., -0.1852,  0.0065,  0.0357],\n",
       "        [ 0.1477, -0.1155,  0.0054,  ...,  0.1521, -0.0401, -0.0667],\n",
       "        [ 0.0295,  0.0510,  0.0121,  ...,  0.0636,  0.0613,  0.0375],\n",
       "        ...,\n",
       "        [-0.1248, -0.0445,  0.2041,  ...,  0.1673,  0.0036,  0.0392],\n",
       "        [ 0.0617, -0.2184, -0.0042,  ...,  0.0860,  0.0883,  0.1243],\n",
       "        [-0.0482, -0.0210,  0.0470,  ..., -0.1305,  0.0548,  0.2116]],\n",
       "       device='cuda:0'), tensor([-0.0074,  0.0060,  0.0524,  ..., -0.0302,  0.0190, -0.0144],\n",
       "       device='cuda:0'), tensor([[ 0.0289,  0.1551, -0.0248,  ..., -0.1380, -0.0449,  0.0074],\n",
       "        [ 0.0126, -0.0048, -0.0525,  ...,  0.0904,  0.1770, -0.0345],\n",
       "        [ 0.0904,  0.0108,  0.0308,  ...,  0.0027,  0.0597,  0.1372],\n",
       "        ...,\n",
       "        [-0.0189,  0.0385, -0.0071,  ..., -0.0534, -0.0653,  0.1202],\n",
       "        [-0.0806,  0.1480, -0.0159,  ...,  0.0516, -0.1306, -0.1627],\n",
       "        [ 0.1235,  0.1671, -0.0863,  ..., -0.0808,  0.0733,  0.0003]],\n",
       "       device='cuda:0'), tensor([ 0.0149,  0.0030,  0.0081,  ..., -0.0137, -0.0031,  0.0850],\n",
       "       device='cuda:0'), tensor([[ 0.1108, -0.1929, -0.1638,  ..., -0.1218,  0.1225, -0.1166],\n",
       "        [ 0.1660, -0.0580,  0.1096,  ..., -0.0894, -0.1619,  0.0759],\n",
       "        [-0.0425, -0.1487, -0.0110,  ...,  0.1268, -0.1856, -0.0047],\n",
       "        ...,\n",
       "        [-0.0211,  0.1555,  0.0537,  ...,  0.1519, -0.0439, -0.0458],\n",
       "        [ 0.0869, -0.0359, -0.1165,  ..., -0.0140,  0.0837, -0.0610],\n",
       "        [-0.0450,  0.1324,  0.2071,  ...,  0.0422,  0.0606, -0.0840]],\n",
       "       device='cuda:0'), tensor([-0.0566, -0.4449, -0.3524,  ..., -0.0395, -0.0547,  0.2805],\n",
       "       device='cuda:0'), tensor([[ 0.1349, -0.0901, -0.1499,  ..., -0.0597, -0.0797, -0.0584],\n",
       "        [-0.1832, -0.0462, -0.1220,  ..., -0.0186,  0.0612, -0.1799],\n",
       "        [-0.0611,  0.1079, -0.0618,  ..., -0.1951,  0.1313,  0.0116],\n",
       "        ...,\n",
       "        [ 0.0871,  0.0434, -0.0186,  ..., -0.1306,  0.0486,  0.2171],\n",
       "        [ 0.1258, -0.1508, -0.1072,  ..., -0.0491,  0.0404,  0.1476],\n",
       "        [ 0.0706, -0.2402, -0.1376,  ..., -0.0737, -0.0658, -0.1316]],\n",
       "       device='cuda:0'), tensor([ 0.0049,  0.0879, -0.0193,  ...,  0.0504, -0.0817,  0.0591],\n",
       "       device='cuda:0'), tensor([0.2607, 0.2352, 0.2638,  ..., 0.2906, 0.2618, 0.2490], device='cuda:0'), tensor([ 0.0087, -0.0198,  0.0099,  ..., -0.0087,  0.0168,  0.0235],\n",
       "       device='cuda:0'), tensor([[-0.1393, -0.0154, -0.0320,  ..., -0.1114,  0.2161,  0.0076],\n",
       "        [ 0.2622, -0.3780,  0.0535,  ..., -0.0317,  0.0553, -0.1640],\n",
       "        [ 0.2126, -0.1486,  0.1336,  ...,  0.1599, -0.0117,  0.0263],\n",
       "        ...,\n",
       "        [ 0.1350, -0.1350,  0.1163,  ..., -0.1313, -0.0965,  0.1716],\n",
       "        [ 0.0971, -0.2287,  0.0678,  ...,  0.0553,  0.0934,  0.0047],\n",
       "        [ 0.1079, -0.1596, -0.0949,  ...,  0.2183, -0.1359, -0.0612]],\n",
       "       device='cuda:0'), tensor([-0.0885, -0.1108, -0.0621,  ..., -0.0603, -0.0886, -0.0341],\n",
       "       device='cuda:0'), tensor([[ 0.1993,  0.2038,  0.3749,  ..., -0.0224,  0.0983,  0.0094],\n",
       "        [ 0.0351,  0.1393,  0.0286,  ..., -0.0203, -0.1266,  0.0843],\n",
       "        [-0.0909, -0.0147, -0.0246,  ..., -0.0418,  0.0189,  0.0672],\n",
       "        ...,\n",
       "        [-0.1550,  0.1127,  0.1293,  ...,  0.2045,  0.1265,  0.0303],\n",
       "        [-0.1484,  0.2419, -0.1692,  ...,  0.0549,  0.2148,  0.0231],\n",
       "        [-0.1475,  0.0813, -0.0180,  ...,  0.1605, -0.2380, -0.0460]],\n",
       "       device='cuda:0'), tensor([-0.0229, -0.0230,  0.0653,  ..., -0.0088, -0.0743,  0.0494],\n",
       "       device='cuda:0'), tensor([0.3447, 0.3058, 0.1635,  ..., 0.3250, 0.3160, 0.2683], device='cuda:0'), tensor([-0.0673,  0.0443, -0.0936,  ...,  0.0326, -0.0141,  0.0183],\n",
       "       device='cuda:0'), tensor([[ 0.0876,  0.0395, -0.0947,  ..., -0.0893,  0.0712,  0.0514],\n",
       "        [-0.0614, -0.0584, -0.0263,  ...,  0.1799,  0.1165,  0.0357],\n",
       "        [-0.1602,  0.2214,  0.0748,  ..., -0.0810,  0.1983, -0.0599],\n",
       "        ...,\n",
       "        [ 0.0347,  0.0163,  0.1433,  ...,  0.2101, -0.0013,  0.0880],\n",
       "        [-0.0949,  0.0486,  0.5731,  ...,  0.0386, -0.0347, -0.0685],\n",
       "        [ 0.0584,  0.0211,  0.0478,  ...,  0.0986, -0.0385,  0.1095]],\n",
       "       device='cuda:0'), tensor([ 0.0261,  0.1612, -0.0154,  ...,  0.0383,  0.0766, -0.0343],\n",
       "       device='cuda:0'), tensor([[-0.0487, -0.1233, -0.0337,  ...,  0.0990, -0.1381, -0.0827],\n",
       "        [ 0.0796,  0.0434,  0.0235,  ..., -0.1108,  0.1431,  0.0990],\n",
       "        [ 0.0645,  0.0096,  0.0217,  ...,  0.0061,  0.1220, -0.2526],\n",
       "        ...,\n",
       "        [-0.1708, -0.1004,  0.0093,  ..., -0.0492, -0.1323, -0.0606],\n",
       "        [ 0.0254,  0.0293,  0.0896,  ..., -0.0733, -0.0360, -0.0776],\n",
       "        [ 0.1944, -0.0261,  0.0299,  ...,  0.1974,  0.0879,  0.0411]],\n",
       "       device='cuda:0'), tensor([ 0.0237, -0.1781,  0.0132,  ..., -0.0214, -0.0115, -0.0339],\n",
       "       device='cuda:0'), tensor([[-0.1949, -0.0733, -0.0165,  ..., -0.1863,  0.1903, -0.0737],\n",
       "        [-0.0531, -0.0288,  0.0251,  ...,  0.1404, -0.1563,  0.0937],\n",
       "        [ 0.0059,  0.1474,  0.1051,  ...,  0.1651,  0.0994, -0.1072],\n",
       "        ...,\n",
       "        [-0.0559,  0.0150,  0.3621,  ..., -0.0714, -0.0457,  0.0681],\n",
       "        [-0.0432, -0.0646,  0.9687,  ...,  0.0845, -0.0917,  0.1377],\n",
       "        [ 0.0619,  0.0296,  0.1174,  ..., -0.0043, -0.0929,  0.0165]],\n",
       "       device='cuda:0'), tensor([-0.2463, -0.3608, -0.1581,  ...,  0.0205, -0.3099,  0.0850],\n",
       "       device='cuda:0'), tensor([[-0.0090, -0.0056, -0.0349,  ...,  0.0250, -0.0765, -0.0358],\n",
       "        [ 0.0562, -0.0216,  0.0687,  ...,  0.1349,  0.1133,  0.0872],\n",
       "        [-0.0042,  0.0716,  0.0115,  ...,  0.1116,  0.2829, -0.0405],\n",
       "        ...,\n",
       "        [ 0.1220,  0.0758, -0.2172,  ...,  0.0849, -0.0831, -0.0752],\n",
       "        [ 0.0606, -0.0855, -0.1659,  ...,  0.0063, -0.0218, -0.0384],\n",
       "        [ 0.1277, -0.0973, -0.0502,  ..., -0.1511,  0.1767,  0.0499]],\n",
       "       device='cuda:0'), tensor([ 0.0427, -0.0037,  0.0108,  ..., -0.0162, -0.0044,  0.0280],\n",
       "       device='cuda:0'), tensor([0.2938, 0.2637, 0.3289,  ..., 0.3033, 0.3068, 0.2734], device='cuda:0'), tensor([-0.0072, -0.0092, -0.0093,  ..., -0.0104,  0.0090,  0.0091],\n",
       "       device='cuda:0'), tensor([[-0.0699,  0.0115,  0.0597,  ..., -0.0870, -0.0914, -0.0132],\n",
       "        [-0.2725, -0.0154, -0.1580,  ...,  0.0276, -0.1834, -0.1457],\n",
       "        [-0.0631,  0.0425,  0.0462,  ..., -0.1425, -0.1743,  0.1393],\n",
       "        ...,\n",
       "        [-0.0055,  0.1142,  0.3842,  ...,  0.1027,  0.1427, -0.0272],\n",
       "        [-0.0390,  0.0132, -0.0605,  ..., -0.0414,  0.0623,  0.0495],\n",
       "        [-0.1508,  0.0465,  0.0268,  ...,  0.0420, -0.1479,  0.0310]],\n",
       "       device='cuda:0'), tensor([ 0.1654, -0.0893, -0.0555,  ..., -0.0266,  0.0300, -0.0279],\n",
       "       device='cuda:0'), tensor([[ 0.1319,  0.0169, -0.1681,  ...,  0.0075,  0.0180,  0.1951],\n",
       "        [ 0.0047,  0.1921,  0.1237,  ..., -0.0509, -0.0434, -0.0909],\n",
       "        [ 0.1375, -0.1033, -0.0513,  ..., -0.0059,  0.0057,  0.0648],\n",
       "        ...,\n",
       "        [ 0.1101,  0.0198, -0.0591,  ...,  0.0410,  0.0862,  0.0222],\n",
       "        [ 0.0178,  0.0427, -0.2628,  ..., -0.1331,  0.0213, -0.0221],\n",
       "        [ 0.0063, -0.1526,  0.1113,  ..., -0.0140, -0.0466, -0.0104]],\n",
       "       device='cuda:0'), tensor([ 0.0114, -0.0364,  0.0482,  ..., -0.0191, -0.0683,  0.0076],\n",
       "       device='cuda:0'), tensor([0.3853, 0.3548, 0.1912,  ..., 0.3706, 0.3570, 0.3137], device='cuda:0'), tensor([-0.0687,  0.1382, -0.0940,  ...,  0.0689,  0.0182, -0.0440],\n",
       "       device='cuda:0'), tensor([[-0.1683, -0.1340, -0.1638,  ..., -0.1601, -0.0404,  0.0097],\n",
       "        [-0.1643, -0.0214, -0.0037,  ...,  0.0818,  0.1141,  0.0041],\n",
       "        [-0.0066, -0.0654, -0.0135,  ..., -0.0152,  0.0884,  0.0682],\n",
       "        ...,\n",
       "        [ 0.0376, -0.1811, -0.0827,  ..., -0.1699, -0.1580,  0.0882],\n",
       "        [ 0.1210,  0.0733,  0.1679,  ..., -0.1194,  0.1992,  0.2239],\n",
       "        [-0.0763,  0.1088, -0.1508,  ..., -0.0923,  0.1229, -0.0675]],\n",
       "       device='cuda:0'), tensor([ 0.0023, -0.0061, -0.0027,  ...,  0.0166,  0.0368, -0.0662],\n",
       "       device='cuda:0'), tensor([[-0.0135, -0.0021, -0.0827,  ...,  0.1416, -0.0811,  0.0267],\n",
       "        [-0.1454, -0.0026,  0.0093,  ..., -0.0981,  0.0191, -0.0382],\n",
       "        [ 0.1313, -0.0359, -0.1874,  ..., -0.0955, -0.0041,  0.2307],\n",
       "        ...,\n",
       "        [-0.0003, -0.0597, -0.0021,  ..., -0.0172, -0.1024,  0.1473],\n",
       "        [ 0.0242,  0.0214, -0.0598,  ...,  0.0672,  0.0675,  0.0950],\n",
       "        [ 0.0061,  0.0879,  0.0600,  ...,  0.1820,  0.0653, -0.1758]],\n",
       "       device='cuda:0'), tensor([ 0.0413, -0.0141, -0.0267,  ...,  0.0361, -0.0192,  0.0699],\n",
       "       device='cuda:0'), tensor([[ 0.0245,  0.1270, -0.0720,  ...,  0.0721,  0.0175, -0.0234],\n",
       "        [ 0.2036,  0.0589,  0.2449,  ...,  0.0358, -0.1150, -0.0850],\n",
       "        [ 0.0003, -0.0065, -0.1554,  ..., -0.0937, -0.1383,  0.1988],\n",
       "        ...,\n",
       "        [ 0.1377, -0.1087, -0.0496,  ...,  0.0692, -0.1493, -0.0091],\n",
       "        [-0.0636,  0.0005,  0.1230,  ..., -0.0481,  0.0416,  0.2235],\n",
       "        [-0.1730,  0.1542, -0.1061,  ..., -0.0014,  0.2677, -0.0497]],\n",
       "       device='cuda:0'), tensor([ 0.1800,  0.0245,  0.1723,  ...,  0.1922,  0.1727, -0.0673],\n",
       "       device='cuda:0'), tensor([[-0.1263, -0.0759,  0.0122,  ...,  0.0353,  0.0122, -0.0864],\n",
       "        [ 0.1585,  0.0638, -0.2296,  ...,  0.0552, -0.0242, -0.0591],\n",
       "        [ 0.1689,  0.1568,  0.1593,  ...,  0.0313,  0.0498, -0.0857],\n",
       "        ...,\n",
       "        [-0.0433, -0.0132,  0.0592,  ..., -0.0332, -0.0595, -0.1347],\n",
       "        [-0.0867, -0.1078, -0.0228,  ...,  0.0843, -0.0175, -0.0188],\n",
       "        [ 0.0383,  0.0541, -0.0450,  ..., -0.1048, -0.0698,  0.1508]],\n",
       "       device='cuda:0'), tensor([ 0.0097, -0.0202,  0.0797,  ...,  0.0200,  0.0316, -0.0107],\n",
       "       device='cuda:0'), tensor([0.3127, 0.2577, 0.2522,  ..., 0.2942, 0.3053, 0.2710], device='cuda:0'), tensor([ 0.0229, -0.0233,  0.0231,  ...,  0.0148,  0.0182,  0.0069],\n",
       "       device='cuda:0'), tensor([[-0.0178,  0.0416,  0.0658,  ...,  0.0471,  0.1039,  0.0287],\n",
       "        [ 0.0381,  0.0526, -0.1271,  ..., -0.0783, -0.0064,  0.1798],\n",
       "        [ 0.0519, -0.0737,  0.0125,  ..., -0.0081, -0.0765, -0.1019],\n",
       "        ...,\n",
       "        [-0.1234, -0.1060,  0.2590,  ...,  0.0975,  0.0789,  0.1002],\n",
       "        [-0.0376,  0.0256,  0.1628,  ..., -0.3761, -0.0188,  0.1330],\n",
       "        [ 0.0433, -0.2847,  0.0774,  ..., -0.0800,  0.0966,  0.0516]],\n",
       "       device='cuda:0'), tensor([ 0.0454, -0.0058,  0.0253,  ..., -0.0625, -0.0547, -0.0727],\n",
       "       device='cuda:0'), tensor([[ 0.0327,  0.1133, -0.0884,  ..., -0.0358, -0.1046, -0.0281],\n",
       "        [ 0.0232,  0.0405, -0.0792,  ..., -0.0295, -0.0494,  0.0077],\n",
       "        [-0.0477,  0.0954,  0.0071,  ..., -0.0235,  0.2176,  0.1445],\n",
       "        ...,\n",
       "        [-0.0338,  0.1468,  0.0954,  ...,  0.0435, -0.0277, -0.0496],\n",
       "        [-0.0747, -0.0866,  0.0174,  ...,  0.3760,  0.0032,  0.0209],\n",
       "        [-0.0349,  0.1064,  0.0737,  ...,  0.1704, -0.0584,  0.3650]],\n",
       "       device='cuda:0'), tensor([-0.0120, -0.0487,  0.0911,  ...,  0.0117, -0.0250,  0.0175],\n",
       "       device='cuda:0'), tensor([0.3992, 0.3662, 0.2122,  ..., 0.3777, 0.4057, 0.3765], device='cuda:0'), tensor([-0.0069,  0.0548, -0.0325,  ...,  0.0705,  0.1413, -0.1424],\n",
       "       device='cuda:0'), tensor([[-0.0298, -0.0064,  0.0190,  ...,  0.0127, -0.0797,  0.0604],\n",
       "        [ 0.1781, -0.0154, -0.0716,  ..., -0.0573, -0.2334, -0.0129],\n",
       "        [ 0.1033, -0.1689, -0.0591,  ...,  0.0402,  0.0328,  0.1770],\n",
       "        ...,\n",
       "        [-0.0228,  0.0854,  0.1121,  ...,  0.0823,  0.1691, -0.0799],\n",
       "        [ 0.0228, -0.0712,  0.0601,  ...,  0.0565, -0.0322, -0.0476],\n",
       "        [ 0.1119, -0.2063, -0.0377,  ..., -0.0441, -0.0050,  0.1197]],\n",
       "       device='cuda:0'), tensor([0.0042, 0.0031, 0.0098,  ..., 0.0113, 0.0066, 0.0100], device='cuda:0'), tensor([[ 0.0062,  0.0050, -0.2112,  ..., -0.0071,  0.1661, -0.0703],\n",
       "        [ 0.0535,  0.1251, -0.1132,  ..., -0.0323,  0.0294,  0.0652],\n",
       "        [-0.0640,  0.0145,  0.1749,  ...,  0.1684,  0.0638, -0.0803],\n",
       "        ...,\n",
       "        [ 0.1099,  0.2201, -0.0985,  ...,  0.0894,  0.0357,  0.1390],\n",
       "        [ 0.0277, -0.0153,  0.1146,  ..., -0.0636,  0.3705,  0.2174],\n",
       "        [-0.2031, -0.0222, -0.0411,  ...,  0.0243, -0.1275,  0.2215]],\n",
       "       device='cuda:0'), tensor([-0.0078,  0.0023, -0.0984,  ..., -0.0133, -0.0100, -0.0155],\n",
       "       device='cuda:0'), tensor([[-0.0410,  0.0551, -0.0310,  ..., -0.0175,  0.0052, -0.0525],\n",
       "        [-0.1059, -0.1189,  0.0145,  ...,  0.0503, -0.2511,  0.0513],\n",
       "        [-0.0591,  0.0057, -0.0534,  ..., -0.1344,  0.0055,  0.1202],\n",
       "        ...,\n",
       "        [ 0.0947,  0.0900,  0.2075,  ..., -0.1967, -0.0167, -0.0525],\n",
       "        [ 0.0525, -0.1018,  0.0747,  ..., -0.0271, -0.1959,  0.0426],\n",
       "        [-0.0542, -0.0954,  0.0398,  ..., -0.0281,  0.0029, -0.0860]],\n",
       "       device='cuda:0'), tensor([-0.1476, -0.0536, -0.1333,  ..., -0.2815,  0.3717,  0.0974],\n",
       "       device='cuda:0'), tensor([[-0.0374, -0.1463,  0.1460,  ...,  0.0474, -0.1590,  0.2041],\n",
       "        [ 0.0725, -0.1538, -0.0842,  ..., -0.0571, -0.0029, -0.0550],\n",
       "        [ 0.2010, -0.0392, -0.0236,  ...,  0.1529, -0.2362, -0.0865],\n",
       "        ...,\n",
       "        [ 0.1579, -0.0535, -0.2337,  ..., -0.0291,  0.0923,  0.0314],\n",
       "        [-0.0836, -0.0467, -0.0346,  ..., -0.0269, -0.2290,  0.0776],\n",
       "        [-0.1253,  0.0167,  0.0815,  ..., -0.0850, -0.2168, -0.2250]],\n",
       "       device='cuda:0'), tensor([ 0.0305, -0.0567,  0.0236,  ...,  0.0020,  0.0216,  0.0005],\n",
       "       device='cuda:0'), tensor([0.3394, 0.2760, 0.2672,  ..., 0.3040, 0.3002, 0.2784], device='cuda:0'), tensor([-0.0167, -0.0139,  0.0295,  ...,  0.0304,  0.0258, -0.0138],\n",
       "       device='cuda:0'), tensor([[ 0.0121, -0.0306,  0.0677,  ..., -0.0035,  0.0006,  0.0113],\n",
       "        [ 0.1277,  0.0585,  0.1760,  ..., -0.0895,  0.1544, -0.0110],\n",
       "        [-0.0687, -0.0053, -0.1868,  ..., -0.1245,  0.0451,  0.1639],\n",
       "        ...,\n",
       "        [ 0.2536, -0.1949, -0.0295,  ..., -0.0250,  0.0035, -0.2116],\n",
       "        [ 0.0705,  0.1333, -0.0538,  ...,  0.1972,  0.0357,  0.0031],\n",
       "        [ 0.2928, -0.0299,  0.0270,  ...,  0.1451, -0.0179, -0.3509]],\n",
       "       device='cuda:0'), tensor([ 0.0490, -0.0146,  0.0037,  ..., -0.0677, -0.0461, -0.0836],\n",
       "       device='cuda:0'), tensor([[-0.0623, -0.1659, -0.2409,  ...,  0.2114,  0.0936,  0.2784],\n",
       "        [-0.0235,  0.0692,  0.1331,  ..., -0.1665,  0.1361,  0.2220],\n",
       "        [-0.0855, -0.0147, -0.0893,  ...,  0.1585, -0.2179, -0.1530],\n",
       "        ...,\n",
       "        [ 0.0429, -0.0878,  0.0210,  ..., -0.0905,  0.0909,  0.1514],\n",
       "        [-0.0418, -0.0764,  0.1156,  ..., -0.1502,  0.2206,  0.2551],\n",
       "        [ 0.0128, -0.0553, -0.2007,  ..., -0.3723, -0.0976, -0.3105]],\n",
       "       device='cuda:0'), tensor([-0.1047, -0.0643,  0.0246,  ...,  0.0173, -0.0534,  0.0586],\n",
       "       device='cuda:0'), tensor([0.4363, 0.4132, 0.2385,  ..., 0.4163, 0.4530, 0.4197], device='cuda:0'), tensor([-0.0599,  0.0718,  0.0113,  ...,  0.0825,  0.1562, -0.2039],\n",
       "       device='cuda:0'), tensor([[-0.0296, -0.1670, -0.2056,  ..., -0.0839, -0.1283, -0.1810],\n",
       "        [-0.0031, -0.0560,  0.0282,  ...,  0.0581,  0.1253,  0.0821],\n",
       "        [ 0.1810, -0.1267,  0.0151,  ...,  0.0177,  0.1498,  0.2203],\n",
       "        ...,\n",
       "        [ 0.0378,  0.1045, -0.0812,  ..., -0.1504,  0.0060, -0.1040],\n",
       "        [ 0.1180,  0.1008, -0.0900,  ...,  0.1016,  0.0350, -0.0008],\n",
       "        [ 0.0944,  0.0650, -0.0511,  ...,  0.0126, -0.1475, -0.1902]],\n",
       "       device='cuda:0'), tensor([0.0019, 0.0012, 0.0070,  ..., 0.0420, 0.1932, 0.0051], device='cuda:0'), tensor([[ 0.0683, -0.0155, -0.0171,  ..., -0.1329,  0.2030, -0.0155],\n",
       "        [ 0.0396,  0.0381, -0.0408,  ...,  0.0296,  0.0319,  0.1161],\n",
       "        [ 0.1674,  0.0777,  0.1754,  ...,  0.0103,  0.0850,  0.0885],\n",
       "        ...,\n",
       "        [-0.1102, -0.0851, -0.0148,  ..., -0.0028, -0.2064,  0.0278],\n",
       "        [ 0.0122, -0.1330,  0.0642,  ...,  0.1230, -0.2444,  0.0990],\n",
       "        [-0.1051, -0.1802, -0.1140,  ..., -0.0871, -0.0696, -0.0089]],\n",
       "       device='cuda:0'), tensor([-0.0209,  0.0015, -0.0720,  ..., -0.0004,  0.0426, -0.0471],\n",
       "       device='cuda:0'), tensor([[ 0.0749, -0.0639, -0.0080,  ...,  0.0435, -0.0265, -0.0877],\n",
       "        [ 0.1845,  0.2142,  0.0064,  ...,  0.0144, -0.0727,  0.0992],\n",
       "        [-0.0898,  0.0018,  0.0469,  ..., -0.1150,  0.2492,  0.0342],\n",
       "        ...,\n",
       "        [-0.0195, -0.0333,  0.0232,  ...,  0.0625, -0.0402, -0.0408],\n",
       "        [-0.0206,  0.0703, -0.0195,  ...,  0.1190, -0.0847, -0.0155],\n",
       "        [ 0.0921,  0.1520, -0.0247,  ..., -0.0038, -0.2099, -0.0649]],\n",
       "       device='cuda:0'), tensor([ 0.6118, -0.0156,  0.0272,  ..., -0.5455, -0.7676, -0.1832],\n",
       "       device='cuda:0'), tensor([[ 0.1360,  0.0391,  0.0203,  ...,  0.1444, -0.0122,  0.0987],\n",
       "        [-0.0324,  0.0246, -0.0748,  ...,  0.0217,  0.0569,  0.1622],\n",
       "        [-0.0477,  0.1096, -0.1808,  ..., -0.0849, -0.0628,  0.1634],\n",
       "        ...,\n",
       "        [-0.0324,  0.1019,  0.0747,  ..., -0.0364, -0.1472,  0.0927],\n",
       "        [ 0.0189, -0.2495, -0.0846,  ...,  0.1785,  0.1950,  0.0130],\n",
       "        [ 0.0299,  0.0201,  0.1485,  ..., -0.0128, -0.0489, -0.0260]],\n",
       "       device='cuda:0'), tensor([-0.0396, -0.0176,  0.1124,  ...,  0.0344,  0.0315, -0.0341],\n",
       "       device='cuda:0'), tensor([0.3482, 0.2845, 0.2836,  ..., 0.3325, 0.3288, 0.2994], device='cuda:0'), tensor([-0.0156,  0.0092,  0.0380,  ...,  0.0442,  0.0225, -0.0121],\n",
       "       device='cuda:0'), tensor([[ 0.3451,  0.0900,  0.1673,  ..., -0.0723,  0.0339,  0.0621],\n",
       "        [-0.0384, -0.1500,  0.0528,  ..., -0.0702,  0.0783,  0.1522],\n",
       "        [ 0.0525, -0.1169,  0.1449,  ..., -0.1211, -0.0577,  0.1031],\n",
       "        ...,\n",
       "        [ 0.1876, -0.2729, -0.1639,  ..., -0.0927,  0.0676, -0.0685],\n",
       "        [-0.1888, -0.2868, -0.0025,  ..., -0.2319, -0.0920,  0.0085],\n",
       "        [ 0.1135, -0.0171,  0.0896,  ...,  0.0479,  0.0025,  0.2093]],\n",
       "       device='cuda:0'), tensor([-0.0722, -0.0678, -0.0104,  ..., -0.0900, -0.0649,  0.0063],\n",
       "       device='cuda:0'), tensor([[ 0.2477, -0.0044, -0.0469,  ...,  0.0643, -0.2544,  0.0270],\n",
       "        [ 0.0760, -0.3173,  0.0367,  ...,  0.0456,  0.1087,  0.1389],\n",
       "        [ 0.2511,  0.2894, -0.1028,  ...,  0.0048, -0.0150, -0.1363],\n",
       "        ...,\n",
       "        [-0.0873,  0.0294,  0.0187,  ...,  0.0646, -0.2009,  0.0490],\n",
       "        [ 0.0193, -0.0515,  0.1276,  ..., -0.2259,  0.0222, -0.0979],\n",
       "        [-0.0384, -0.1145, -0.0369,  ..., -0.0676, -0.0344, -0.0690]],\n",
       "       device='cuda:0'), tensor([-0.0518,  0.0334,  0.0611,  ..., -0.0125,  0.0194,  0.0268],\n",
       "       device='cuda:0'), tensor([0.4326, 0.4213, 0.2224,  ..., 0.4252, 0.4567, 0.3995], device='cuda:0'), tensor([ 0.0059,  0.1183,  0.0011,  ...,  0.0967,  0.1576, -0.1876],\n",
       "       device='cuda:0'), tensor([[-0.1629, -0.0773, -0.0530,  ..., -0.0248,  0.1060,  0.1245],\n",
       "        [-0.0068,  0.1173, -0.2693,  ...,  0.1210,  0.0993,  0.0172],\n",
       "        [-0.0266, -0.0576,  0.1012,  ..., -0.1073,  0.0583,  0.0280],\n",
       "        ...,\n",
       "        [ 0.0682, -0.0097, -0.0378,  ..., -0.2170,  0.1151, -0.0544],\n",
       "        [ 0.0972, -0.0598, -0.0130,  ..., -0.0861, -0.0218, -0.1529],\n",
       "        [-0.1868, -0.0777, -0.0308,  ...,  0.0192,  0.1219,  0.0279]],\n",
       "       device='cuda:0'), tensor([ 0.2195, -0.8257, -0.1682,  ...,  0.1370, -0.0649, -0.0677],\n",
       "       device='cuda:0'), tensor([[ 0.0502, -0.0741, -0.0007,  ...,  0.0217,  0.0573,  0.1318],\n",
       "        [ 0.2228,  0.1321, -0.1022,  ...,  0.0690, -0.0437,  0.1667],\n",
       "        [-0.0990,  0.0920, -0.1069,  ...,  0.3172,  0.0971, -0.1422],\n",
       "        ...,\n",
       "        [ 0.1534, -0.0465,  0.0482,  ..., -0.0513, -0.0555, -0.0159],\n",
       "        [-0.1265,  0.0622, -0.0352,  ..., -0.0725,  0.0476, -0.1028],\n",
       "        [-0.2104,  0.2599, -0.0238,  ..., -0.0414, -0.0422, -0.0152]],\n",
       "       device='cuda:0'), tensor([ 0.0195, -0.0161, -0.1501,  ...,  0.0040,  0.0477,  0.0563],\n",
       "       device='cuda:0'), tensor([[-0.0558,  0.0337, -0.0875,  ..., -0.0400,  0.1178,  0.0498],\n",
       "        [ 0.0207,  0.0667, -0.0466,  ...,  0.1920, -0.2487, -0.1636],\n",
       "        [ 0.0698,  0.2191,  0.0028,  ...,  0.0332,  0.1571,  0.0231],\n",
       "        ...,\n",
       "        [-0.0743, -0.1963, -0.0522,  ...,  0.0630,  0.0730,  0.0328],\n",
       "        [ 0.1009, -0.1168,  0.0288,  ..., -0.0382, -0.0266, -0.0274],\n",
       "        [ 0.0796,  0.0826, -0.1310,  ...,  0.1424,  0.0640,  0.1445]],\n",
       "       device='cuda:0'), tensor([ 0.0672, -0.7182, -0.0558,  ..., -0.6951,  0.5169,  0.5309],\n",
       "       device='cuda:0'), tensor([[ 0.0101,  0.1172, -0.0335,  ...,  0.0620, -0.0085,  0.0248],\n",
       "        [ 0.0222,  0.0756,  0.1649,  ..., -0.0371,  0.1334,  0.0759],\n",
       "        [ 0.0152, -0.1801,  0.0089,  ..., -0.0149,  0.1186, -0.0684],\n",
       "        ...,\n",
       "        [ 0.0994, -0.0227,  0.0864,  ...,  0.1116,  0.1683, -0.1656],\n",
       "        [-0.0199, -0.0892, -0.0848,  ...,  0.0195,  0.2305,  0.0570],\n",
       "        [ 0.1050,  0.0217,  0.0435,  ...,  0.0686,  0.0873, -0.0467]],\n",
       "       device='cuda:0'), tensor([ 0.0287,  0.0525,  0.0733,  ...,  0.1063, -0.0076, -0.0339],\n",
       "       device='cuda:0'), tensor([0.3756, 0.3086, 0.2707,  ..., 0.3296, 0.3296, 0.3051], device='cuda:0'), tensor([ 0.0059,  0.0104,  0.0017,  ...,  0.0491,  0.0569, -0.0180],\n",
       "       device='cuda:0'), tensor([[-0.1648,  0.0364, -0.0407,  ..., -0.1119, -0.1316, -0.0264],\n",
       "        [ 0.0917,  0.3265,  0.1172,  ...,  0.0815,  0.0088,  0.2860],\n",
       "        [-0.0239, -0.0598, -0.0260,  ..., -0.0302,  0.0658, -0.0721],\n",
       "        ...,\n",
       "        [-0.0895, -0.0288,  0.3653,  ..., -0.0504,  0.0463,  0.0387],\n",
       "        [-0.0530, -0.1519, -0.1514,  ...,  0.1729,  0.1084,  0.3932],\n",
       "        [-0.0612, -0.0530,  0.0184,  ...,  0.0288,  0.1037, -0.0963]],\n",
       "       device='cuda:0'), tensor([-0.0675, -0.0418, -0.0639,  ..., -0.0234, -0.0932,  0.0113],\n",
       "       device='cuda:0'), tensor([[-0.1077,  0.0129, -0.1340,  ..., -0.0267, -0.1587,  0.0408],\n",
       "        [-0.1954, -0.1915,  0.0626,  ..., -0.0409,  0.0576,  0.0601],\n",
       "        [ 0.0149,  0.0781, -0.1187,  ...,  0.1080, -0.1413,  0.0961],\n",
       "        ...,\n",
       "        [ 0.1826, -0.0743,  0.0600,  ..., -0.0554,  0.3244, -0.0312],\n",
       "        [-0.2354, -0.0293,  0.1189,  ..., -0.0163,  0.0847, -0.0735],\n",
       "        [ 0.0750,  0.0648,  0.0491,  ..., -0.0241,  0.4055,  0.0356]],\n",
       "       device='cuda:0'), tensor([ 0.0490, -0.0323,  0.0442,  ...,  0.0371, -0.0022,  0.0668],\n",
       "       device='cuda:0'), tensor([0.3789, 0.3830, 0.2144,  ..., 0.3885, 0.4106, 0.3572], device='cuda:0'), tensor([ 0.0359,  0.0367, -0.0629,  ...,  0.0583,  0.1129, -0.0823],\n",
       "       device='cuda:0'), tensor([[ 0.0489,  0.0300, -0.2336,  ..., -0.0546,  0.0252, -0.0960],\n",
       "        [ 0.0674,  0.0662, -0.0252,  ...,  0.0236,  0.0362, -0.0783],\n",
       "        [ 0.0110,  0.0975,  0.1105,  ...,  0.0404,  0.0780, -0.0863],\n",
       "        ...,\n",
       "        [ 0.0367,  0.1395,  0.0744,  ..., -0.1115,  0.0557, -0.0247],\n",
       "        [ 0.0010,  0.0236, -0.0461,  ..., -0.0486,  0.0137,  0.1566],\n",
       "        [ 0.1350,  0.0277, -0.1526,  ...,  0.0397, -0.0285, -0.0299]],\n",
       "       device='cuda:0'), tensor([-3.0083e-01, -4.5420e-03,  4.4906e-02,  ..., -2.2314e-03,\n",
       "        -8.6945e-03,  6.3728e-05], device='cuda:0'), tensor([[ 0.1276,  0.0352,  0.0678,  ..., -0.0397, -0.0282, -0.1385],\n",
       "        [ 0.1296,  0.0182,  0.0012,  ..., -0.0542,  0.0311,  0.1244],\n",
       "        [ 0.0317,  0.0091,  0.0970,  ...,  0.0199,  0.1461,  0.1612],\n",
       "        ...,\n",
       "        [-0.0104,  0.0163,  0.0240,  ...,  0.1174,  0.0365, -0.0274],\n",
       "        [-0.0471,  0.0726, -0.0049,  ...,  0.1776,  0.0819, -0.2591],\n",
       "        [-0.0881,  0.0981,  0.1117,  ...,  0.0540, -0.0980, -0.1680]],\n",
       "       device='cuda:0'), tensor([-0.0164,  0.0444, -0.0170,  ...,  0.0098,  0.0574, -0.0160],\n",
       "       device='cuda:0'), tensor([[-0.0253, -0.0038, -0.2530,  ..., -0.0436, -0.1365,  0.0687],\n",
       "        [-0.0946, -0.0460,  0.1895,  ...,  0.0304,  0.0263, -0.1126],\n",
       "        [ 0.0081, -0.0236,  0.2020,  ...,  0.0467,  0.0474,  0.0407],\n",
       "        ...,\n",
       "        [ 0.1081,  0.0931,  0.1567,  ...,  0.0560,  0.0790,  0.0454],\n",
       "        [-0.0081, -0.0764, -0.0137,  ..., -0.0445, -0.0619,  0.0483],\n",
       "        [ 0.1263, -0.0419,  0.0668,  ...,  0.0956, -0.0057,  0.0584]],\n",
       "       device='cuda:0'), tensor([-0.5843, -0.7076,  0.3278,  ...,  0.1128, -0.0919, -0.0351],\n",
       "       device='cuda:0'), tensor([[-0.1125,  0.0633,  0.0482,  ...,  0.1527, -0.0789,  0.3513],\n",
       "        [-0.0354,  0.1316,  0.0118,  ..., -0.0609, -0.1259, -0.0956],\n",
       "        [ 0.0086, -0.1311,  0.0109,  ..., -0.0061, -0.1674,  0.0186],\n",
       "        ...,\n",
       "        [ 0.1416, -0.0042,  0.1811,  ..., -0.0814,  0.0021,  0.1912],\n",
       "        [-0.0447, -0.0635,  0.1820,  ..., -0.1250, -0.0494, -0.0589],\n",
       "        [ 0.0987, -0.1350,  0.0403,  ...,  0.0805,  0.2318,  0.1870]],\n",
       "       device='cuda:0'), tensor([ 0.0538, -0.0189,  0.0787,  ...,  0.0645,  0.0221, -0.0340],\n",
       "       device='cuda:0'), tensor([0.3471, 0.3066, 0.2647,  ..., 0.3494, 0.3479, 0.3225], device='cuda:0'), tensor([-0.0224,  0.0412, -0.0039,  ...,  0.0443,  0.0315, -0.0482],\n",
       "       device='cuda:0'), tensor([[-0.1580, -0.1075,  0.0377,  ..., -0.1843,  0.0243,  0.0516],\n",
       "        [-0.0241,  0.0043,  0.1234,  ..., -0.0534, -0.0466,  0.1448],\n",
       "        [-0.2111, -0.0298,  0.1805,  ..., -0.1951, -0.0424,  0.1339],\n",
       "        ...,\n",
       "        [ 0.0458,  0.0222, -0.0048,  ...,  0.0448,  0.0347,  0.0157],\n",
       "        [-0.0238, -0.2202, -0.1674,  ..., -0.1122, -0.0837, -0.1027],\n",
       "        [-0.1516, -0.0954,  0.2648,  ..., -0.0423, -0.0120, -0.0314]],\n",
       "       device='cuda:0'), tensor([-0.0379, -0.0557, -0.0236,  ..., -0.0512, -0.0473, -0.0893],\n",
       "       device='cuda:0'), tensor([[ 0.0043,  0.1816,  0.2071,  ...,  0.1409, -0.0280,  0.0410],\n",
       "        [-0.0153,  0.0605, -0.0603,  ...,  0.0276,  0.0604,  0.0409],\n",
       "        [-0.0581,  0.0410, -0.2404,  ...,  0.1619,  0.1746, -0.0676],\n",
       "        ...,\n",
       "        [ 0.0236,  0.0714,  0.2133,  ...,  0.0762, -0.0193,  0.0122],\n",
       "        [-0.0761, -0.1022,  0.0006,  ...,  0.1054,  0.2510, -0.1348],\n",
       "        [ 0.0492,  0.0716, -0.1004,  ..., -0.1809, -0.1219,  0.1805]],\n",
       "       device='cuda:0'), tensor([ 0.0603, -0.0314,  0.0760,  ..., -0.0016,  0.1003,  0.0001],\n",
       "       device='cuda:0'), tensor([0.3806, 0.3833, 0.2051,  ..., 0.3799, 0.3633, 0.3019], device='cuda:0'), tensor([ 0.0107,  0.0512, -0.0714,  ...,  0.0267,  0.0416, -0.0976],\n",
       "       device='cuda:0'), tensor([[ 0.2021, -0.0978,  0.1644,  ..., -0.0022, -0.1081, -0.1033],\n",
       "        [ 0.0456, -0.0514, -0.1429,  ..., -0.0658,  0.0778,  0.2134],\n",
       "        [ 0.1105,  0.0786,  0.0598,  ...,  0.1275,  0.0760,  0.1102],\n",
       "        ...,\n",
       "        [-0.0150,  0.0019,  0.0315,  ..., -0.0357, -0.1038, -0.0607],\n",
       "        [ 0.0402, -0.1535,  0.2799,  ..., -0.0095, -0.1126,  0.1334],\n",
       "        [ 0.0377, -0.1237,  0.1994,  ..., -0.1209, -0.0770, -0.0703]],\n",
       "       device='cuda:0'), tensor([ 0.0070,  0.0069,  0.0056,  ..., -0.0969,  0.0758, -0.0776],\n",
       "       device='cuda:0'), tensor([[-0.0800,  0.0648, -0.0332,  ...,  0.0765, -0.1330,  0.0779],\n",
       "        [-0.2581,  0.0412, -0.1032,  ...,  0.0733, -0.0710, -0.0610],\n",
       "        [-0.0160, -0.0664, -0.0954,  ...,  0.0231, -0.1022,  0.0963],\n",
       "        ...,\n",
       "        [ 0.0340,  0.0957, -0.0479,  ..., -0.1028, -0.0519,  0.0619],\n",
       "        [-0.1343, -0.0785, -0.0818,  ...,  0.0171,  0.0006, -0.0446],\n",
       "        [-0.0724,  0.2005, -0.0443,  ...,  0.0373, -0.0144, -0.0811]],\n",
       "       device='cuda:0'), tensor([-0.0111, -0.0642,  0.0199,  ...,  0.0027,  0.0180,  0.0068],\n",
       "       device='cuda:0'), tensor([[-0.0317,  0.1053, -0.0341,  ..., -0.0431, -0.1240, -0.2328],\n",
       "        [ 0.0082, -0.0365, -0.0112,  ..., -0.2512, -0.0025,  0.1421],\n",
       "        [-0.0918, -0.0159, -0.1501,  ...,  0.0299, -0.0814, -0.1814],\n",
       "        ...,\n",
       "        [ 0.0584,  0.0341, -0.0485,  ...,  0.0333, -0.0470, -0.0524],\n",
       "        [-0.1561,  0.0981,  0.1480,  ...,  0.0174,  0.2819, -0.0214],\n",
       "        [ 0.0821,  0.0954,  0.0471,  ...,  0.1833,  0.0313,  0.0086]],\n",
       "       device='cuda:0'), tensor([-0.1056,  0.0043, -0.1674,  ...,  0.4743, -0.0205,  0.1807],\n",
       "       device='cuda:0'), tensor([[-0.0761, -0.0142, -0.2504,  ..., -0.0786,  0.1975,  0.1076],\n",
       "        [-0.0423,  0.1770, -0.0712,  ...,  0.0985, -0.0288, -0.0202],\n",
       "        [ 0.1659,  0.0153,  0.0550,  ..., -0.1382,  0.0474,  0.1914],\n",
       "        ...,\n",
       "        [-0.1323, -0.0607,  0.1023,  ..., -0.1227,  0.0167,  0.0746],\n",
       "        [-0.0237,  0.0392, -0.1792,  ..., -0.1954,  0.0487, -0.1977],\n",
       "        [-0.0788, -0.1033,  0.0986,  ..., -0.0691, -0.0026, -0.0343]],\n",
       "       device='cuda:0'), tensor([ 0.0773, -0.1119,  0.1062,  ...,  0.0450,  0.0592,  0.0384],\n",
       "       device='cuda:0'), tensor([0.3658, 0.3248, 0.2968,  ..., 0.3908, 0.3718, 0.3310], device='cuda:0'), tensor([-0.0282,  0.0675, -0.0107,  ...,  0.0502,  0.0002, -0.0645],\n",
       "       device='cuda:0'), tensor([[-0.0417,  0.0563,  0.1798,  ..., -0.1063, -0.0006,  0.0007],\n",
       "        [ 0.0808, -0.0693,  0.1664,  ..., -0.2582, -0.0604,  0.1496],\n",
       "        [-0.0661,  0.1932, -0.1722,  ...,  0.0957, -0.1945, -0.0780],\n",
       "        ...,\n",
       "        [ 0.1233,  0.1545,  0.1558,  ...,  0.1087,  0.0393, -0.0286],\n",
       "        [ 0.1408, -0.0419, -0.0343,  ..., -0.0151, -0.0504,  0.0041],\n",
       "        [ 0.0785, -0.0913,  0.0853,  ...,  0.0312, -0.2333,  0.2410]],\n",
       "       device='cuda:0'), tensor([-0.0731, -0.0703, -0.0965,  ..., -0.0666,  0.0474, -0.1022],\n",
       "       device='cuda:0'), tensor([[ 0.1872,  0.2688,  0.1868,  ..., -0.1115, -0.1931,  0.1413],\n",
       "        [-0.0489,  0.0123, -0.0579,  ..., -0.0243,  0.0816,  0.0053],\n",
       "        [-0.1034,  0.0086,  0.1077,  ..., -0.0977, -0.0492,  0.0419],\n",
       "        ...,\n",
       "        [ 0.0358,  0.0780,  0.1982,  ..., -0.0491, -0.0651,  0.1779],\n",
       "        [ 0.0507,  0.1738,  0.1429,  ..., -0.0100, -0.0855,  0.0807],\n",
       "        [-0.1678, -0.0860,  0.1747,  ...,  0.1663,  0.0171, -0.1409]],\n",
       "       device='cuda:0'), tensor([ 0.0187,  0.0097, -0.0259,  ..., -0.0194,  0.0885,  0.0630],\n",
       "       device='cuda:0'), tensor([0.3746, 0.3649, 0.2053,  ..., 0.3884, 0.3583, 0.3489], device='cuda:0'), tensor([ 0.0108,  0.0562, -0.0630,  ..., -0.0063,  0.0200, -0.0941],\n",
       "       device='cuda:0'), tensor([[ 0.0545, -0.1920,  0.0979,  ..., -0.0018,  0.1087,  0.1643],\n",
       "        [-0.0990, -0.0281, -0.0855,  ...,  0.0139,  0.0121,  0.1174],\n",
       "        [-0.0146,  0.0338, -0.1663,  ..., -0.0384, -0.2601, -0.1100],\n",
       "        ...,\n",
       "        [-0.2124, -0.0432, -0.1185,  ...,  0.0402,  0.0040, -0.0095],\n",
       "        [-0.0362,  0.0989,  0.0099,  ...,  0.0085, -0.0014, -0.0315],\n",
       "        [-0.0273,  0.0141, -0.0354,  ...,  0.1243,  0.0987, -0.0004]],\n",
       "       device='cuda:0'), tensor([ 0.0063, -0.0004,  0.0014,  ...,  0.0071,  0.0041,  0.0184],\n",
       "       device='cuda:0'), tensor([[ 0.1011, -0.1190,  0.1218,  ...,  0.0338,  0.0292, -0.1324],\n",
       "        [ 0.2923,  0.0503, -0.1560,  ...,  0.0791,  0.0544,  0.1307],\n",
       "        [-0.0266,  0.1281,  0.0497,  ..., -0.0017, -0.0706, -0.1179],\n",
       "        ...,\n",
       "        [-0.0588, -0.0261,  0.0959,  ...,  0.0082,  0.0971,  0.1154],\n",
       "        [ 0.2264,  0.0280,  0.0457,  ...,  0.0012,  0.0698, -0.0384],\n",
       "        [-0.2202,  0.0314,  0.1101,  ...,  0.0780, -0.2101, -0.1358]],\n",
       "       device='cuda:0'), tensor([-0.0292, -0.0092,  0.0224,  ...,  0.0257, -0.0023, -0.0385],\n",
       "       device='cuda:0'), tensor([[ 0.0166, -0.0140, -0.0388,  ...,  0.1793, -0.2386, -0.0933],\n",
       "        [ 0.0629,  0.1062,  0.1255,  ...,  0.1386, -0.0110,  0.0678],\n",
       "        [-0.1330,  0.0668, -0.1329,  ...,  0.0671,  0.0088, -0.1215],\n",
       "        ...,\n",
       "        [-0.0586,  0.0492,  0.0814,  ...,  0.0211,  0.1274, -0.0084],\n",
       "        [ 0.1304, -0.0137, -0.0224,  ...,  0.0739, -0.0118, -0.0992],\n",
       "        [-0.1409, -0.1561, -0.1234,  ...,  0.0161, -0.0017, -0.0530]],\n",
       "       device='cuda:0'), tensor([-0.0537,  0.0771, -0.0552,  ...,  0.0539, -0.0005, -0.0133],\n",
       "       device='cuda:0'), tensor([[-0.0922,  0.0249, -0.0700,  ..., -0.0077, -0.2125,  0.0304],\n",
       "        [ 0.1155, -0.1201,  0.0089,  ...,  0.0348, -0.1551,  0.0913],\n",
       "        [-0.0898,  0.1861, -0.0732,  ..., -0.0138, -0.1873, -0.1646],\n",
       "        ...,\n",
       "        [ 0.0929, -0.1080,  0.0115,  ...,  0.1817, -0.0639, -0.0160],\n",
       "        [-0.1361, -0.0704, -0.1162,  ..., -0.0714,  0.1106,  0.2456],\n",
       "        [ 0.1320, -0.2549,  0.0785,  ..., -0.0668, -0.2170,  0.1419]],\n",
       "       device='cuda:0'), tensor([ 0.1028, -0.0558, -0.0480,  ...,  0.0322,  0.0809,  0.0275],\n",
       "       device='cuda:0'), tensor([0.3443, 0.3342, 0.2419,  ..., 0.3747, 0.3755, 0.2960], device='cuda:0'), tensor([ 0.0220,  0.0678, -0.0018,  ...,  0.0445, -0.0056, -0.0714],\n",
       "       device='cuda:0'), tensor([[-0.1862,  0.0314,  0.0878,  ...,  0.0352,  0.0107, -0.1072],\n",
       "        [-0.0209, -0.0791, -0.0724,  ...,  0.3985,  0.0744, -0.0365],\n",
       "        [ 0.0470,  0.0867, -0.0135,  ..., -0.1285,  0.0773,  0.0111],\n",
       "        ...,\n",
       "        [ 0.0761, -0.0542, -0.0068,  ...,  0.0719, -0.1119, -0.0968],\n",
       "        [ 0.1628,  0.1549,  0.1220,  ..., -0.0934, -0.0064,  0.2794],\n",
       "        [-0.0398,  0.1671,  0.1209,  ...,  0.0266, -0.0272, -0.0056]],\n",
       "       device='cuda:0'), tensor([ 0.0021, -0.0671, -0.0775,  ..., -0.0762, -0.0816, -0.0992],\n",
       "       device='cuda:0'), tensor([[ 0.1208, -0.0831, -0.0581,  ...,  0.0056,  0.0385,  0.0428],\n",
       "        [ 0.0567, -0.0383, -0.0945,  ..., -0.0468, -0.1010, -0.2069],\n",
       "        [-0.0771, -0.1888, -0.0014,  ...,  0.1347, -0.1538,  0.0447],\n",
       "        ...,\n",
       "        [ 0.0266, -0.0317,  0.0925,  ..., -0.0680,  0.0362,  0.0252],\n",
       "        [-0.0704, -0.0875, -0.0996,  ..., -0.0990,  0.1372, -0.0969],\n",
       "        [ 0.0931,  0.0571, -0.0368,  ..., -0.1652,  0.0245,  0.0759]],\n",
       "       device='cuda:0'), tensor([ 0.0826, -0.0566, -0.0388,  ..., -0.0315, -0.0805,  0.0437],\n",
       "       device='cuda:0'), tensor([0.3737, 0.3997, 0.2112,  ..., 0.4092, 0.4166, 0.3489], device='cuda:0'), tensor([ 0.0702,  0.0526, -0.0494,  ..., -0.0457,  0.0262, -0.1299],\n",
       "       device='cuda:0'), tensor([[ 0.1076,  0.1342,  0.0768,  ...,  0.0371, -0.1249, -0.2511],\n",
       "        [ 0.1861, -0.0736, -0.0813,  ..., -0.0053, -0.1522, -0.1619],\n",
       "        [ 0.1395, -0.1380,  0.0870,  ..., -0.2052, -0.0887,  0.0110],\n",
       "        ...,\n",
       "        [ 0.0737,  0.0858, -0.0073,  ..., -0.0492,  0.0040,  0.0081],\n",
       "        [-0.1151, -0.0157, -0.0993,  ..., -0.0141,  0.0445,  0.0378],\n",
       "        [ 0.0492, -0.0405,  0.0690,  ..., -0.1008,  0.0392,  0.0623]],\n",
       "       device='cuda:0'), tensor([-0.0012, -0.0046, -0.0020,  ..., -0.1706, -0.6543, -0.7660],\n",
       "       device='cuda:0'), tensor([[ 0.0155,  0.1187,  0.0468,  ..., -0.0238,  0.1431, -0.0429],\n",
       "        [-0.0331,  0.0319, -0.2060,  ..., -0.1919, -0.0316,  0.0623],\n",
       "        [ 0.0098,  0.0367,  0.1386,  ...,  0.0280,  0.1433, -0.0628],\n",
       "        ...,\n",
       "        [-0.0509, -0.0782,  0.0887,  ..., -0.1615, -0.0409, -0.0355],\n",
       "        [ 0.0682, -0.2225,  0.0909,  ..., -0.1260, -0.0289, -0.2085],\n",
       "        [ 0.0056,  0.0139, -0.1804,  ...,  0.0821, -0.0202, -0.0693]],\n",
       "       device='cuda:0'), tensor([-0.0142,  0.0146,  0.0485,  ...,  0.0306, -0.0462, -0.0365],\n",
       "       device='cuda:0'), tensor([[ 0.0357, -0.0572,  0.0493,  ...,  0.1321,  0.0391, -0.0134],\n",
       "        [ 0.0615,  0.0619, -0.1594,  ...,  0.0142, -0.0461, -0.0309],\n",
       "        [ 0.0953, -0.0650,  0.0145,  ..., -0.1899, -0.1577, -0.0223],\n",
       "        ...,\n",
       "        [-0.0145,  0.0338, -0.0434,  ..., -0.1194,  0.0369,  0.0979],\n",
       "        [ 0.0164, -0.0963, -0.0193,  ...,  0.0192,  0.0316, -0.1705],\n",
       "        [-0.1434,  0.0824,  0.0026,  ..., -0.1013,  0.0611,  0.0069]],\n",
       "       device='cuda:0'), tensor([-0.2409,  0.1287,  0.0375,  ..., -0.0252,  0.0258, -0.2473],\n",
       "       device='cuda:0'), tensor([[-0.0585, -0.0716, -0.0531,  ...,  0.0978,  0.1611, -0.0495],\n",
       "        [-0.0243,  0.0193,  0.0181,  ...,  0.3076,  0.3297,  0.0128],\n",
       "        [-0.0646,  0.0289, -0.1129,  ...,  0.1830,  0.1540,  0.0634],\n",
       "        ...,\n",
       "        [-0.0523,  0.0028,  0.0737,  ..., -0.0301, -0.0349,  0.1182],\n",
       "        [-0.0021,  0.0936, -0.1149,  ...,  0.0838,  0.0512,  0.0677],\n",
       "        [ 0.1588,  0.1085, -0.1998,  ..., -0.1593, -0.1539,  0.1175]],\n",
       "       device='cuda:0'), tensor([ 0.1518, -0.0452,  0.0382,  ..., -0.0626, -0.0847,  0.0459],\n",
       "       device='cuda:0'), tensor([0.3633, 0.3741, 0.2713,  ..., 0.4048, 0.3760, 0.3222], device='cuda:0'), tensor([ 0.0201,  0.0333,  0.0262,  ...,  0.0521, -0.0080, -0.0150],\n",
       "       device='cuda:0'), tensor([[-0.0786, -0.1382,  0.0582,  ...,  0.1009, -0.0972,  0.0725],\n",
       "        [-0.1765, -0.0673, -0.1118,  ...,  0.0577, -0.1593, -0.0696],\n",
       "        [ 0.1191, -0.0437,  0.0113,  ...,  0.1584, -0.0886, -0.1759],\n",
       "        ...,\n",
       "        [ 0.1757, -0.0688,  0.0767,  ...,  0.0131,  0.0977,  0.1099],\n",
       "        [ 0.0931,  0.0120, -0.0450,  ..., -0.0400, -0.1663,  0.0627],\n",
       "        [ 0.0151, -0.0504, -0.0957,  ...,  0.0480,  0.0701, -0.0322]],\n",
       "       device='cuda:0'), tensor([-0.0400, -0.0382, -0.0879,  ..., -0.0350, -0.0637, -0.0650],\n",
       "       device='cuda:0'), tensor([[-0.0268,  0.1538,  0.1955,  ..., -0.0467,  0.0321, -0.0616],\n",
       "        [ 0.0534, -0.0417, -0.1888,  ..., -0.0050,  0.3639, -0.2539],\n",
       "        [-0.1684,  0.1365,  0.0189,  ..., -0.0929, -0.0632, -0.0441],\n",
       "        ...,\n",
       "        [-0.0990,  0.1455, -0.2737,  ..., -0.1164,  0.1629, -0.0979],\n",
       "        [ 0.0873, -0.0710, -0.0169,  ..., -0.1288,  0.0859,  0.0906],\n",
       "        [-0.1802,  0.1278, -0.1705,  ..., -0.0890,  0.0783, -0.0041]],\n",
       "       device='cuda:0'), tensor([-0.0015, -0.0703, -0.0508,  ..., -0.0459, -0.0564,  0.0790],\n",
       "       device='cuda:0'), tensor([0.3683, 0.4392, 0.2470,  ..., 0.4127, 0.4023, 0.3429], device='cuda:0'), tensor([ 0.0750,  0.0447, -0.0232,  ..., -0.0015,  0.0229, -0.0579],\n",
       "       device='cuda:0'), tensor([[-0.1540, -0.1174,  0.1343,  ..., -0.0039,  0.0329, -0.1489],\n",
       "        [-0.0941, -0.0381, -0.0052,  ..., -0.0464, -0.0128, -0.0188],\n",
       "        [ 0.0002, -0.1749, -0.0276,  ...,  0.0311,  0.0277,  0.1083],\n",
       "        ...,\n",
       "        [ 0.0752,  0.0607,  0.0031,  ..., -0.0714,  0.1204,  0.1087],\n",
       "        [-0.0335,  0.0415,  0.0102,  ...,  0.1594, -0.1637,  0.0738],\n",
       "        [-0.1310,  0.0435, -0.0323,  ...,  0.0150,  0.0437, -0.0923]],\n",
       "       device='cuda:0'), tensor([ 0.2909,  0.3319,  0.0302,  ..., -0.0020, -0.0009,  0.0016],\n",
       "       device='cuda:0'), tensor([[-0.0404,  0.1025,  0.0029,  ..., -0.0699,  0.0353,  0.0350],\n",
       "        [ 0.0688, -0.0575,  0.0537,  ...,  0.0721, -0.0396,  0.0193],\n",
       "        [-0.0313,  0.0498, -0.1289,  ..., -0.0626,  0.0123,  0.0074],\n",
       "        ...,\n",
       "        [ 0.0469, -0.3114,  0.0255,  ...,  0.1326, -0.1383,  0.0503],\n",
       "        [ 0.0218, -0.1914,  0.1941,  ...,  0.1396,  0.0185,  0.0173],\n",
       "        [ 0.0016,  0.0691,  0.0987,  ..., -0.0081,  0.1975, -0.0468]],\n",
       "       device='cuda:0'), tensor([ 0.0568,  0.0095, -0.0488,  ..., -0.0596, -0.0109, -0.0077],\n",
       "       device='cuda:0'), tensor([[-0.0213,  0.0155, -0.0183,  ...,  0.2003,  0.0177,  0.0635],\n",
       "        [ 0.0114,  0.0427, -0.0262,  ...,  0.0341, -0.0421, -0.0696],\n",
       "        [-0.0152,  0.1001,  0.0126,  ..., -0.0166,  0.0140,  0.1934],\n",
       "        ...,\n",
       "        [ 0.0222, -0.0942,  0.0105,  ...,  0.0051, -0.1287, -0.0195],\n",
       "        [-0.0205, -0.0609, -0.0100,  ..., -0.1831, -0.0723, -0.0737],\n",
       "        [ 0.0446, -0.0507, -0.0672,  ..., -0.0096, -0.0249, -0.1346]],\n",
       "       device='cuda:0'), tensor([ 0.1608, -0.1227,  0.0178,  ...,  0.0809, -0.1209, -0.1413],\n",
       "       device='cuda:0'), tensor([[ 0.0555, -0.0358, -0.0166,  ..., -0.1419, -0.1916,  0.0567],\n",
       "        [-0.1079,  0.0344,  0.0581,  ...,  0.2320,  0.0918, -0.1873],\n",
       "        [ 0.0066, -0.0444, -0.0157,  ...,  0.2448, -0.0874,  0.0205],\n",
       "        ...,\n",
       "        [ 0.0043, -0.0775,  0.0124,  ..., -0.1164, -0.0092, -0.1060],\n",
       "        [-0.0735,  0.0452,  0.0115,  ...,  0.1575,  0.0270,  0.0061],\n",
       "        [-0.0564, -0.0297,  0.0003,  ...,  0.0141, -0.1077, -0.0740]],\n",
       "       device='cuda:0'), tensor([ 0.0271, -0.0456, -0.1059,  ...,  0.0205, -0.0213,  0.0276],\n",
       "       device='cuda:0'), tensor([0.3695, 0.4123, 0.3023,  ..., 0.3887, 0.3833, 0.3259], device='cuda:0'), tensor([ 0.0483,  0.0244,  0.0997,  ...,  0.0322, -0.0054,  0.0026],\n",
       "       device='cuda:0'), tensor([[-0.0788, -0.1028, -0.1396,  ...,  0.0810,  0.0469, -0.0741],\n",
       "        [ 0.1180, -0.1036,  0.1180,  ...,  0.0729,  0.0094, -0.2178],\n",
       "        [-0.1385, -0.2231,  0.1177,  ..., -0.0954, -0.2250, -0.0403],\n",
       "        ...,\n",
       "        [-0.0145,  0.0858, -0.0154,  ..., -0.1408, -0.0110,  0.0704],\n",
       "        [ 0.0087,  0.0252, -0.0067,  ..., -0.0548, -0.1027,  0.0039],\n",
       "        [ 0.0141,  0.0254,  0.0670,  ...,  0.1104, -0.0443, -0.1028]],\n",
       "       device='cuda:0'), tensor([-0.0635, -0.0611, -0.1338,  ..., -0.0689, -0.1583, -0.1101],\n",
       "       device='cuda:0'), tensor([[-0.0053, -0.0308,  0.3007,  ...,  0.1641,  0.1490,  0.0861],\n",
       "        [ 0.1292,  0.0366,  0.1455,  ...,  0.0877,  0.1655,  0.1119],\n",
       "        [ 0.1672,  0.1206, -0.0600,  ..., -0.1510,  0.0734, -0.1202],\n",
       "        ...,\n",
       "        [-0.0499, -0.0309,  0.1202,  ...,  0.0592,  0.3024,  0.3141],\n",
       "        [-0.0394, -0.0289,  0.0323,  ...,  0.0127, -0.1440, -0.0419],\n",
       "        [ 0.2038, -0.0142, -0.2084,  ..., -0.0938, -0.0347,  0.2995]],\n",
       "       device='cuda:0'), tensor([ 0.1394,  0.0015, -0.1377,  ..., -0.0658, -0.0571,  0.0767],\n",
       "       device='cuda:0'), tensor([0.3831, 0.4842, 0.2683,  ..., 0.4466, 0.4328, 0.3232], device='cuda:0'), tensor([ 0.0673,  0.0448, -0.0101,  ..., -0.0204,  0.0126, -0.0487],\n",
       "       device='cuda:0'), tensor([[ 0.1632, -0.0303, -0.1709,  ...,  0.1136,  0.1843,  0.2995],\n",
       "        [ 0.0323,  0.0210, -0.0073,  ...,  0.0109,  0.1264,  0.0240],\n",
       "        [ 0.0552,  0.0424, -0.0543,  ..., -0.0022,  0.0936, -0.0194],\n",
       "        ...,\n",
       "        [-0.0058,  0.0528, -0.0882,  ..., -0.1309, -0.1202,  0.1279],\n",
       "        [ 0.1455, -0.0667,  0.1197,  ...,  0.0670,  0.0574, -0.0471],\n",
       "        [-0.0170,  0.0810, -0.0675,  ..., -0.1098, -0.0244,  0.1325]],\n",
       "       device='cuda:0'), tensor([-0.0226, -0.0024, -0.0645,  ...,  1.3797, -0.4127,  0.1621],\n",
       "       device='cuda:0'), tensor([[ 0.1256, -0.0025,  0.1818,  ..., -0.1280, -0.1408, -0.2042],\n",
       "        [ 0.1230,  0.0752, -0.0220,  ..., -0.0096, -0.0283, -0.1883],\n",
       "        [ 0.0496,  0.0561, -0.0893,  ..., -0.1804,  0.0713,  0.0420],\n",
       "        ...,\n",
       "        [ 0.0260, -0.0620,  0.1639,  ..., -0.0432, -0.0887,  0.0252],\n",
       "        [ 0.1109, -0.0983,  0.0038,  ..., -0.0057, -0.0722, -0.1502],\n",
       "        [-0.1341, -0.1385, -0.1826,  ...,  0.0942,  0.0664, -0.0999]],\n",
       "       device='cuda:0'), tensor([ 0.0149, -0.0080, -0.0205,  ..., -0.0352,  0.0169, -0.0534],\n",
       "       device='cuda:0'), tensor([[ 0.0638,  0.0058, -0.2079,  ...,  0.1056,  0.1965, -0.0093],\n",
       "        [ 0.0268, -0.0422,  0.0553,  ..., -0.1071,  0.0409, -0.0776],\n",
       "        [ 0.0517,  0.1049, -0.0482,  ...,  0.0623,  0.0759, -0.0391],\n",
       "        ...,\n",
       "        [-0.0131,  0.1221, -0.0579,  ...,  0.0296, -0.0481,  0.2203],\n",
       "        [ 0.1173, -0.0987,  0.0329,  ...,  0.0569,  0.0813, -0.0129],\n",
       "        [-0.0121,  0.0757, -0.0546,  ..., -0.1377, -0.0346,  0.1308]],\n",
       "       device='cuda:0'), tensor([-0.1789, -0.1052, -0.2167,  ..., -0.5953,  0.0313, -0.2482],\n",
       "       device='cuda:0'), tensor([[-0.0694,  0.0535, -0.2220,  ...,  0.2391, -0.0328, -0.1196],\n",
       "        [ 0.0363,  0.0235,  0.0121,  ...,  0.0213,  0.0723,  0.0492],\n",
       "        [ 0.1316, -0.0272, -0.0765,  ..., -0.1202, -0.0896, -0.0425],\n",
       "        ...,\n",
       "        [ 0.0984,  0.0982,  0.1632,  ..., -0.0212,  0.1368, -0.1409],\n",
       "        [-0.0667,  0.3282, -0.2363,  ...,  0.0344, -0.0893, -0.1717],\n",
       "        [-0.1395,  0.0554, -0.1001,  ..., -0.0476, -0.1756,  0.0550]],\n",
       "       device='cuda:0'), tensor([ 0.3049, -0.0391, -0.1847,  ..., -0.0571, -0.0716,  0.1806],\n",
       "       device='cuda:0'), tensor([0.4015, 0.5021, 0.3129,  ..., 0.4454, 0.4411, 0.3674], device='cuda:0'), tensor([ 0.0017,  0.0458,  0.1196,  ...,  0.0537, -0.0066, -0.0408],\n",
       "       device='cuda:0'), tensor([[ 0.1525, -0.0579, -0.0666,  ..., -0.0519,  0.0487, -0.0615],\n",
       "        [-0.0399, -0.0895,  0.1175,  ...,  0.0199,  0.1337,  0.0443],\n",
       "        [-0.1105,  0.2133, -0.0248,  ...,  0.0029,  0.0263, -0.0475],\n",
       "        ...,\n",
       "        [ 0.1043, -0.0443, -0.0340,  ..., -0.0272, -0.0027, -0.1472],\n",
       "        [-0.1328,  0.0428, -0.0986,  ...,  0.0076, -0.0578, -0.0931],\n",
       "        [-0.0604, -0.1008,  0.1155,  ..., -0.0277,  0.1889,  0.1841]],\n",
       "       device='cuda:0'), tensor([ 0.0652, -0.0864, -0.0886,  ..., -0.0615, -0.1130, -0.1379],\n",
       "       device='cuda:0'), tensor([[-0.0867,  0.2158, -0.0397,  ...,  0.0840,  0.0322,  0.3091],\n",
       "        [ 0.0858,  0.0424,  0.0060,  ..., -0.1160,  0.0366,  0.2220],\n",
       "        [ 0.0514, -0.0884,  0.1088,  ..., -0.0786, -0.0226, -0.2249],\n",
       "        ...,\n",
       "        [ 0.1225,  0.1346,  0.0039,  ..., -0.0713, -0.0691,  0.0561],\n",
       "        [-0.0731,  0.0947, -0.0197,  ...,  0.0024, -0.0292,  0.0125],\n",
       "        [ 0.0881,  0.1398, -0.1274,  ..., -0.0320,  0.1860,  0.0881]],\n",
       "       device='cuda:0'), tensor([ 0.2011,  0.2456,  0.1931,  ...,  0.1443, -0.1456,  0.0726],\n",
       "       device='cuda:0'), tensor([0.4718, 0.5497, 0.2854,  ..., 0.4900, 0.4715, 0.3969], device='cuda:0'), tensor([ 0.0488,  0.1131,  0.0111,  ..., -0.0402,  0.0399, -0.1120],\n",
       "       device='cuda:0'), tensor([[-0.0128,  0.0135,  0.0614,  ...,  0.0215,  0.0955,  0.0332],\n",
       "        [ 0.0005, -0.0888,  0.1448,  ..., -0.2152, -0.1546, -0.0590],\n",
       "        [ 0.0018, -0.0403, -0.0757,  ...,  0.0040,  0.0335,  0.0133],\n",
       "        ...,\n",
       "        [ 0.0287,  0.0639, -0.0342,  ..., -0.0134,  0.0566,  0.0482],\n",
       "        [ 0.0101, -0.0369, -0.0003,  ...,  0.0776, -0.0370, -0.0626],\n",
       "        [-0.0693, -0.0300, -0.1320,  ...,  0.0333,  0.0142, -0.0510]],\n",
       "       device='cuda:0'), tensor([ 0.4507,  0.1327, -0.8731,  ..., -1.2928, -0.3476, -1.3614],\n",
       "       device='cuda:0'), tensor([[ 0.0625,  0.0357,  0.0017,  ..., -0.0989, -0.0357,  0.0389],\n",
       "        [ 0.0881,  0.0749,  0.0329,  ...,  0.0009, -0.0128,  0.0278],\n",
       "        [ 0.1291, -0.0643,  0.1129,  ..., -0.0015,  0.1083, -0.0021],\n",
       "        ...,\n",
       "        [-0.0253,  0.0800, -0.0418,  ...,  0.0339,  0.0005, -0.0495],\n",
       "        [-0.0165, -0.0028,  0.1214,  ...,  0.0104,  0.0251, -0.0562],\n",
       "        [ 0.0409, -0.0287, -0.0259,  ...,  0.0045,  0.0375, -0.1508]],\n",
       "       device='cuda:0'), tensor([-0.0093,  0.0109, -0.0134,  ...,  0.0179, -0.0382, -0.0067],\n",
       "       device='cuda:0'), tensor([[-0.1469, -0.0512,  0.0086,  ...,  0.0035,  0.0421,  0.1215],\n",
       "        [-0.1041, -0.0274,  0.1427,  ..., -0.2463, -0.0443, -0.0924],\n",
       "        [ 0.0851, -0.0654, -0.0552,  ..., -0.1604, -0.0973, -0.0140],\n",
       "        ...,\n",
       "        [ 0.0551,  0.1927, -0.1254,  ..., -0.2841,  0.2789, -0.0968],\n",
       "        [-0.0713,  0.0451, -0.1013,  ..., -0.1659,  0.0734, -0.0701],\n",
       "        [-0.0874,  0.0310,  0.0728,  ..., -0.2041,  0.3377,  0.0345]],\n",
       "       device='cuda:0'), tensor([-0.0632,  0.0670, -0.0112,  ...,  0.1119,  0.0746,  0.0255],\n",
       "       device='cuda:0'), tensor([[-0.0442,  0.0314, -0.1456,  ..., -0.0083,  0.0675,  0.0405],\n",
       "        [-0.0762,  0.0453,  0.0542,  ...,  0.0324,  0.0861,  0.0463],\n",
       "        [-0.0571,  0.0477,  0.0217,  ..., -0.0032, -0.0305,  0.0597],\n",
       "        ...,\n",
       "        [ 0.0411,  0.1100, -0.0931,  ..., -0.0004,  0.0015,  0.0252],\n",
       "        [-0.0022, -0.0299, -0.1096,  ..., -0.0146,  0.0158, -0.0222],\n",
       "        [ 0.0538,  0.0545,  0.0077,  ...,  0.0299,  0.0297,  0.0500]],\n",
       "       device='cuda:0'), tensor([ 0.2923,  0.2769,  0.1213,  ...,  0.1550, -0.2149,  0.0833],\n",
       "       device='cuda:0'), tensor([0.5258, 0.7791, 0.3637,  ..., 0.6947, 0.8655, 0.4832], device='cuda:0'), tensor([-0.0237,  0.0818,  0.0052,  ..., -0.0721,  0.2621, -0.0867],\n",
       "       device='cuda:0'), tensor([[ 0.1120, -0.0289, -0.1257,  ..., -0.0686,  0.0284,  0.0550],\n",
       "        [ 0.0258,  0.0812, -0.0851,  ...,  0.0197, -0.1400, -0.0398],\n",
       "        [-0.1123,  0.0326,  0.0505,  ...,  0.0043, -0.0018, -0.1162],\n",
       "        ...,\n",
       "        [-0.0868, -0.1811, -0.0863,  ...,  0.0409,  0.0188,  0.0559],\n",
       "        [-0.0221, -0.1370,  0.0319,  ...,  0.0579, -0.0307,  0.0804],\n",
       "        [-0.0114, -0.0055, -0.0511,  ...,  0.0004, -0.0150,  0.0144]],\n",
       "       device='cuda:0'), tensor([-0.1278, -0.0059, -0.0922,  ..., -0.1078, -0.0594, -0.0216],\n",
       "       device='cuda:0'), tensor([[ 0.2130,  0.0711, -0.0630,  ..., -0.0592,  0.0380,  0.0334],\n",
       "        [ 0.2598,  0.0398,  0.0911,  ...,  0.0762,  0.1360,  0.0449],\n",
       "        [-0.1939,  0.0568,  0.0514,  ...,  0.0767,  0.1712,  0.1512],\n",
       "        ...,\n",
       "        [-0.0766, -0.1151,  0.1393,  ..., -0.0847, -0.0968,  0.1044],\n",
       "        [ 0.1237,  0.0575, -0.0105,  ...,  0.2113,  0.0542,  0.0895],\n",
       "        [ 0.0684,  0.0165, -0.0010,  ...,  0.0211,  0.0492, -0.0778]],\n",
       "       device='cuda:0'), tensor([-0.0791, -0.1221,  0.1059,  ...,  0.0579, -0.0319, -0.0011],\n",
       "       device='cuda:0'), tensor([0.5318, 0.6338, 0.3051,  ..., 0.6336, 0.5775, 0.4297], device='cuda:0'), tensor([-0.0467, -0.0642,  0.0793,  ..., -0.0220, -0.0115, -0.1242],\n",
       "       device='cuda:0'), tensor([[-0.0111,  0.0012, -0.0025,  ..., -0.0178, -0.0045, -0.0055],\n",
       "        [-0.0232, -0.0453, -0.0046,  ...,  0.0118,  0.0042,  0.0398],\n",
       "        [ 0.0105,  0.0022, -0.0071,  ...,  0.0008,  0.0113,  0.0098],\n",
       "        ...,\n",
       "        [-0.0192,  0.0091, -0.0061,  ..., -0.0380,  0.0305, -0.0135],\n",
       "        [-0.0023,  0.0211, -0.0190,  ..., -0.0183,  0.0523, -0.0082],\n",
       "        [-0.0122, -0.0101,  0.0015,  ...,  0.0194, -0.0499,  0.0388]],\n",
       "       device='cuda:0'), tensor([-0.0477, -0.1356,  0.1586,  ...,  0.0230,  0.1987, -0.2090],\n",
       "       device='cuda:0'), tensor([[-0.0271,  0.0331,  0.0166,  ...,  0.0386,  0.0047, -0.0347],\n",
       "        [ 0.0402, -0.0194,  0.0217,  ..., -0.0200, -0.0674,  0.0033],\n",
       "        [ 0.0162, -0.0293,  0.0207,  ...,  0.0101,  0.0178,  0.0135],\n",
       "        ...,\n",
       "        [-0.0494,  0.0323,  0.0262,  ..., -0.0033, -0.0022, -0.0194],\n",
       "        [-0.0111,  0.0195, -0.0346,  ...,  0.0224, -0.0156, -0.0482],\n",
       "        [-0.0151, -0.1106, -0.0013,  ..., -0.0242,  0.0790, -0.0195]],\n",
       "       device='cuda:0'), tensor([ 0.0002,  0.0017,  0.0014,  ...,  0.0119,  0.0102, -0.0080],\n",
       "       device='cuda:0'), tensor([[ 0.0165,  0.0165,  0.0086,  ..., -0.0092,  0.0154, -0.0209],\n",
       "        [ 0.0150, -0.0064,  0.0145,  ...,  0.0209,  0.0316, -0.0054],\n",
       "        [-0.0351, -0.0034, -0.0020,  ..., -0.0099, -0.0381, -0.0178],\n",
       "        ...,\n",
       "        [ 0.0267, -0.0204, -0.0158,  ..., -0.0141, -0.0080, -0.0054],\n",
       "        [ 0.0034, -0.0298, -0.0066,  ...,  0.0041,  0.0121, -0.0147],\n",
       "        [ 0.0094,  0.0046,  0.0492,  ...,  0.0006, -0.0078, -0.0178]],\n",
       "       device='cuda:0'), tensor([ 0.0118,  0.0457, -0.0114,  ..., -0.0131, -0.1020,  0.1034],\n",
       "       device='cuda:0'), tensor([[-0.0253,  0.0261, -0.0082,  ...,  0.0197,  0.0299,  0.0236],\n",
       "        [-0.0063, -0.0069, -0.0032,  ..., -0.0351,  0.0157,  0.0844],\n",
       "        [ 0.0031,  0.0114, -0.0161,  ..., -0.0327, -0.0011,  0.0104],\n",
       "        ...,\n",
       "        [-0.0219,  0.0104,  0.0203,  ..., -0.0075,  0.0144,  0.0343],\n",
       "        [-0.0228,  0.0501, -0.0199,  ..., -0.0073, -0.0545, -0.0216],\n",
       "        [-0.0182, -0.0124, -0.0393,  ..., -0.0599,  0.0326,  0.0592]],\n",
       "       device='cuda:0'), tensor([-0.0725, -0.1236,  0.1135,  ...,  0.0426, -0.0410,  0.0452],\n",
       "       device='cuda:0'), tensor([0.5474, 0.4809, 0.3062,  ..., 0.5259, 0.4282, 0.3175], device='cuda:0'), tensor([ 0.0177,  0.0264, -0.0858,  ...,  0.0271,  0.1316, -0.1326],\n",
       "       device='cuda:0'), tensor([[-0.0482, -0.0467, -0.0611,  ..., -0.0329,  0.1178, -0.0982],\n",
       "        [-0.0293,  0.0719,  0.1294,  ..., -0.1485, -0.0751,  0.1878],\n",
       "        [ 0.0974,  0.0013, -0.1530,  ...,  0.0308, -0.1082,  0.0662],\n",
       "        ...,\n",
       "        [ 0.0173,  0.0722,  0.1093,  ...,  0.0139,  0.1591, -0.0218],\n",
       "        [ 0.0430, -0.1683,  0.0003,  ..., -0.0541, -0.0379,  0.0068],\n",
       "        [-0.1390, -0.0316, -0.0273,  ..., -0.0127, -0.0364, -0.0962]],\n",
       "       device='cuda:0'), tensor([-0.0507, -0.0823, -0.0273,  ..., -0.0598, -0.0096, -0.0730],\n",
       "       device='cuda:0'), tensor([[-0.0424, -0.0770, -0.2490,  ...,  0.0291, -0.0324,  0.0838],\n",
       "        [ 0.0690,  0.0602,  0.0212,  ..., -0.1415,  0.0341,  0.1708],\n",
       "        [ 0.0265, -0.1700, -0.0077,  ...,  0.0305,  0.0366,  0.1117],\n",
       "        ...,\n",
       "        [-0.0264, -0.1838, -0.1555,  ...,  0.0360, -0.1153, -0.1434],\n",
       "        [ 0.0100,  0.1407,  0.0700,  ..., -0.0363, -0.0204,  0.1020],\n",
       "        [ 0.0044, -0.1254, -0.0620,  ..., -0.1065, -0.0874,  0.0763]],\n",
       "       device='cuda:0'), tensor([-0.1740, -0.2430,  0.0341,  ...,  0.0773,  0.0547,  0.0899],\n",
       "       device='cuda:0'), tensor([0.7266, 0.9125, 0.4378,  ..., 0.9847, 0.8143, 0.6403], device='cuda:0'), tensor([-0.0007,  0.1183,  0.0965,  ...,  0.0668, -0.0245, -0.1312],\n",
       "       device='cuda:0'), tensor([[ 0.0136, -0.1979,  0.0076,  ...,  0.0523, -0.0271, -0.0586],\n",
       "        [-0.0148, -0.0050,  0.0152,  ..., -0.0102, -0.0249, -0.0349],\n",
       "        [ 0.0933,  0.0553, -0.0533,  ..., -0.0500, -0.0048,  0.0237],\n",
       "        ...,\n",
       "        [ 0.0300,  0.0982,  0.0898,  ..., -0.0147,  0.0845, -0.0379],\n",
       "        [ 0.0041, -0.1799, -0.0744,  ..., -0.0465,  0.0732, -0.0172],\n",
       "        [ 0.0306, -0.0679, -0.0364,  ...,  0.0230, -0.1173, -0.0272]],\n",
       "       device='cuda:0'), tensor([-0.0829,  0.1775,  0.1671,  ...,  0.1033, -0.0278,  0.0272],\n",
       "       device='cuda:0'), tensor([[ 0.0309,  0.1252, -0.0229,  ...,  0.1456, -0.0929,  0.0145],\n",
       "        [-0.0469,  0.0856, -0.0377,  ..., -0.2061, -0.0818,  0.0950],\n",
       "        [ 0.0046, -0.0014, -0.0861,  ..., -0.1492, -0.0116, -0.0204],\n",
       "        ...,\n",
       "        [-0.0266, -0.1409,  0.0039,  ..., -0.1079,  0.0366,  0.0685],\n",
       "        [-0.0263,  0.1519, -0.0079,  ..., -0.0105,  0.0455, -0.0144],\n",
       "        [ 0.0060, -0.1036, -0.1145,  ..., -0.0252, -0.1063,  0.0658]],\n",
       "       device='cuda:0'), tensor([ 0.0844, -0.0475, -0.0558,  ..., -0.0727,  0.0581, -0.0273],\n",
       "       device='cuda:0'), tensor([[ 3.1087e-02,  1.3779e-05, -1.1939e-01,  ..., -1.6417e-02,\n",
       "         -4.3188e-03, -1.9199e-02],\n",
       "        [ 6.3148e-02,  6.3650e-02, -7.4141e-02,  ...,  9.1362e-02,\n",
       "         -5.6212e-02, -3.7817e-02],\n",
       "        [ 5.1161e-02,  7.5411e-02,  1.0853e-01,  ..., -6.3126e-02,\n",
       "          1.1904e-02, -5.7356e-03],\n",
       "        ...,\n",
       "        [ 5.6145e-02,  5.0112e-03,  7.2984e-02,  ..., -7.2843e-02,\n",
       "          2.3563e-03, -1.0487e-01],\n",
       "        [-4.0226e-03, -8.8760e-02,  4.0457e-03,  ...,  7.2197e-02,\n",
       "         -2.7172e-02,  1.5163e-02],\n",
       "        [-9.5654e-02, -9.7420e-04,  1.1048e-02,  ...,  1.7026e-02,\n",
       "          7.4628e-02,  3.7256e-02]], device='cuda:0'), tensor([-0.0469, -0.0253,  0.1985,  ..., -0.0682, -0.0293, -0.1351],\n",
       "       device='cuda:0'), tensor([[-0.0614,  0.0830, -0.2659,  ...,  0.1189, -0.0137,  0.0482],\n",
       "        [-0.1778,  0.0193, -0.1147,  ...,  0.1466, -0.0712,  0.1222],\n",
       "        [ 0.0252, -0.0838, -0.0195,  ..., -0.0486, -0.0444, -0.0685],\n",
       "        ...,\n",
       "        [-0.1649,  0.1263,  0.0311,  ..., -0.0836,  0.0380, -0.0516],\n",
       "        [-0.0097,  0.0115,  0.0143,  ...,  0.0433, -0.0180,  0.0949],\n",
       "        [-0.0218,  0.0125,  0.0613,  ..., -0.0623, -0.0480, -0.1458]],\n",
       "       device='cuda:0'), tensor([-0.1127, -0.1792,  0.0809,  ...,  0.1097,  0.0021,  0.0733],\n",
       "       device='cuda:0'), tensor([0.4561, 0.5275, 0.2312,  ..., 0.5893, 0.4331, 0.3593], device='cuda:0'), tensor([ 0.0729, -0.0250, -0.0457,  ..., -0.0262, -0.0447, -0.0536],\n",
       "       device='cuda:0'), tensor([[-0.1028,  0.0698,  0.1335,  ...,  0.1710,  0.0488, -0.0980],\n",
       "        [ 0.0132,  0.0105,  0.0242,  ..., -0.0337, -0.0026,  0.0546],\n",
       "        [-0.2227,  0.0437, -0.0522,  ..., -0.0687, -0.0296,  0.1942],\n",
       "        ...,\n",
       "        [ 0.0008, -0.0201,  0.0148,  ...,  0.0112,  0.0257, -0.0705],\n",
       "        [-0.1210, -0.0218, -0.0837,  ..., -0.1207, -0.0391, -0.0844],\n",
       "        [ 0.0805,  0.1489,  0.0721,  ..., -0.1160,  0.0319, -0.1644]],\n",
       "       device='cuda:0'), tensor([-0.0369, -0.1199, -0.0818,  ..., -0.0308, -0.1571, -0.0752],\n",
       "       device='cuda:0'), tensor([[ 0.0215, -0.0185, -0.0173,  ...,  0.0350,  0.0150, -0.0128],\n",
       "        [ 0.0435, -0.0274,  0.1219,  ..., -0.0442,  0.3414, -0.0263],\n",
       "        [-0.1005, -0.0914,  0.0306,  ..., -0.0630,  0.1763,  0.0070],\n",
       "        ...,\n",
       "        [ 0.0203, -0.0778, -0.0883,  ...,  0.0515, -0.0361,  0.0554],\n",
       "        [-0.0291,  0.0385, -0.0238,  ..., -0.0122, -0.0612, -0.0042],\n",
       "        [-0.0974,  0.0313,  0.0910,  ...,  0.0649,  0.0486,  0.0081]],\n",
       "       device='cuda:0'), tensor([-0.1150, -0.1159, -0.0356,  ..., -0.0069,  0.1031,  0.0880],\n",
       "       device='cuda:0'), tensor([0.5998, 0.8034, 0.4082,  ..., 0.8271, 0.6566, 0.5157], device='cuda:0'), tensor([ 0.1007,  0.0980,  0.0009,  ..., -0.0521, -0.2230, -0.2087],\n",
       "       device='cuda:0'), tensor([[-0.1148,  0.0905,  0.1255,  ..., -0.0447, -0.0077,  0.0235],\n",
       "        [ 0.0227, -0.0388, -0.0751,  ..., -0.1040,  0.0571, -0.1132],\n",
       "        [-0.0100, -0.0067, -0.0975,  ..., -0.0971,  0.0095, -0.0666],\n",
       "        ...,\n",
       "        [-0.0133, -0.0473, -0.0430,  ...,  0.0444, -0.0553, -0.0594],\n",
       "        [ 0.0859,  0.0782, -0.0141,  ...,  0.1291,  0.0061, -0.0648],\n",
       "        [ 0.0798,  0.0164,  0.0360,  ...,  0.0089,  0.0366,  0.0336]],\n",
       "       device='cuda:0'), tensor([ 0.8698, -0.5138, -0.7796,  ...,  0.0119, -0.3281, -0.0789],\n",
       "       device='cuda:0'), tensor([[ 0.1488, -0.1183, -0.0090,  ...,  0.3059, -0.1909, -0.1597],\n",
       "        [ 0.0210,  0.1277, -0.0060,  ...,  0.1128, -0.0275, -0.1188],\n",
       "        [ 0.0793, -0.2348,  0.1016,  ...,  0.2104,  0.0439, -0.0489],\n",
       "        ...,\n",
       "        [ 0.0053, -0.0250, -0.0481,  ...,  0.0606, -0.0597,  0.0413],\n",
       "        [ 0.0389,  0.0091, -0.0363,  ...,  0.1967, -0.0971, -0.0097],\n",
       "        [ 0.0397,  0.0817, -0.0524,  ...,  0.0157,  0.0410, -0.0281]],\n",
       "       device='cuda:0'), tensor([-0.0766,  0.0535,  0.1032,  ..., -0.0426, -0.0827, -0.0039],\n",
       "       device='cuda:0'), tensor([[ 3.9575e-02, -1.5180e-01,  5.4038e-02,  ...,  3.6284e-03,\n",
       "          1.5258e-01,  1.5396e-02],\n",
       "        [-4.3381e-02,  4.9828e-02,  8.5105e-04,  ..., -9.3451e-05,\n",
       "         -1.0547e-01,  7.5072e-04],\n",
       "        [-4.2779e-02,  1.2108e-01, -5.0635e-02,  ...,  7.2685e-02,\n",
       "          1.1608e-01, -5.5684e-02],\n",
       "        ...,\n",
       "        [ 1.0107e-01, -8.0742e-02,  7.1307e-02,  ...,  7.3882e-02,\n",
       "         -7.0298e-03,  4.4404e-02],\n",
       "        [ 3.4613e-02, -4.5924e-03,  2.2631e-02,  ..., -3.8851e-02,\n",
       "         -1.9028e-02,  1.5851e-03],\n",
       "        [-4.9128e-02,  7.3954e-03, -7.5088e-02,  ...,  8.0002e-02,\n",
       "          1.4112e-01, -1.1694e-01]], device='cuda:0'), tensor([-0.1149,  0.0129,  0.0266,  ...,  0.0672,  0.0410,  0.0039],\n",
       "       device='cuda:0'), tensor([[-0.0873, -0.1309, -0.1500,  ...,  0.0875,  0.0364, -0.0141],\n",
       "        [ 0.1068, -0.1612, -0.1637,  ..., -0.0737,  0.0630,  0.1133],\n",
       "        [-0.0015,  0.0786,  0.0177,  ...,  0.1006,  0.0607,  0.1152],\n",
       "        ...,\n",
       "        [-0.1001, -0.1455, -0.3283,  ..., -0.0808, -0.1226, -0.0690],\n",
       "        [ 0.0866,  0.1115, -0.1070,  ...,  0.0747,  0.0599, -0.1113],\n",
       "        [-0.1053,  0.1374,  0.1053,  ..., -0.1164, -0.0608,  0.0086]],\n",
       "       device='cuda:0'), tensor([-0.1295, -0.0927, -0.0168,  ..., -0.0089,  0.0321,  0.0654],\n",
       "       device='cuda:0'), tensor([0.2905, 0.3848, 0.1781,  ..., 0.4464, 0.3485, 0.2402], device='cuda:0'), tensor([ 0.0217,  0.0299, -0.0829,  ...,  0.0100,  0.0031, -0.0101],\n",
       "       device='cuda:0'), tensor([[ 0.0026, -0.0456, -0.0133,  ..., -0.0046,  0.0019, -0.0075],\n",
       "        [ 0.0177, -0.0390,  0.0326,  ...,  0.1081, -0.0831,  0.0097],\n",
       "        [-0.0122, -0.0270,  0.0348,  ..., -0.1447, -0.0542, -0.0683],\n",
       "        ...,\n",
       "        [ 0.0288, -0.0703, -0.0562,  ...,  0.1041,  0.1497, -0.0531],\n",
       "        [-0.0651, -0.1335,  0.0159,  ...,  0.1253, -0.0523, -0.0089],\n",
       "        [-0.0188, -0.0243, -0.0647,  ..., -0.0262, -0.0414, -0.0242]],\n",
       "       device='cuda:0'), tensor([-0.1316, -0.0887, -0.1332,  ..., -0.1879, -0.0412, -0.0548],\n",
       "       device='cuda:0'), tensor([[-0.0125, -0.0266,  0.1369,  ..., -0.0047,  0.0738,  0.0317],\n",
       "        [ 0.0883,  0.0046,  0.0051,  ...,  0.1313,  0.0184,  0.0381],\n",
       "        [-0.0536,  0.0099, -0.0182,  ..., -0.0358, -0.0322, -0.0075],\n",
       "        ...,\n",
       "        [ 0.0536, -0.0006,  0.1760,  ..., -0.0994, -0.0184, -0.0802],\n",
       "        [ 0.0850,  0.0724, -0.0182,  ..., -0.0956, -0.0076, -0.0221],\n",
       "        [-0.0217, -0.0157, -0.0165,  ..., -0.0084, -0.0153, -0.0431]],\n",
       "       device='cuda:0'), tensor([-0.0703,  0.0455, -0.0384,  ..., -0.0872, -0.0717,  0.0024],\n",
       "       device='cuda:0'), tensor([0.3796, 0.5284, 0.2881,  ..., 0.5173, 0.4062, 0.3457], device='cuda:0'), tensor([ 0.0523,  0.1230, -0.0670,  ...,  0.0007, -0.1076, -0.1015],\n",
       "       device='cuda:0'), tensor([[-0.0487,  0.0610,  0.0422,  ...,  0.0483, -0.0477,  0.0610],\n",
       "        [ 0.1364, -0.0333, -0.0485,  ..., -0.0257, -0.0567, -0.0632],\n",
       "        [ 0.1516, -0.0479, -0.0620,  ..., -0.0541, -0.0555, -0.0791],\n",
       "        ...,\n",
       "        [-0.0002, -0.0873,  0.0644,  ...,  0.0660,  0.0457, -0.0648],\n",
       "        [-0.0779, -0.0012, -0.1335,  ..., -0.1219,  0.0574,  0.0105],\n",
       "        [ 0.1241, -0.0091,  0.0344,  ..., -0.0020,  0.0375,  0.0551]],\n",
       "       device='cuda:0'), tensor([ 5.1623e-02, -6.0152e-01, -6.0362e-01, -7.0756e-02,  4.7726e-04,\n",
       "        -3.3200e-02, -4.0185e-02,  3.2312e-03, -2.1396e-02, -1.0725e-02,\n",
       "        -1.2793e-02,  8.5522e-03, -2.7930e-02, -6.8949e-03, -1.2504e-02,\n",
       "        -2.2481e-03, -4.7258e-02, -1.8597e-02, -1.4799e-02, -2.8081e-02,\n",
       "        -1.0862e-02, -5.7171e-03, -2.5538e-02, -5.4265e-02, -1.4168e-02,\n",
       "        -7.9462e-03,  1.0719e-03, -6.4024e-03, -2.5612e-02, -2.0320e-02,\n",
       "        -5.1308e-02, -1.6362e-02, -4.0602e-02], device='cuda:0')])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for key in origin.keys():\n",
    "    if key in compose.keys():\n",
    "        if not torch.equal(origin[key], compose[key]):\n",
    "            print(f\"Difference found in key: {key}\")\n",
    "            print(f\"Values for {key} in origin: {origin[key]}\")\n",
    "            print(f\"Values for {key} in compose: {compose[key]}\")\n",
    "    else:\n",
    "        print(f\"Key '{key}' not found in compose\")\n",
    "        \n",
    "for key in compose.keys():\n",
    "    if key not in origin.keys():\n",
    "        print(f\"Key '{key}' not found in origin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "origin.keys() == compose.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jy_zhang/anaconda3/envs/env_a/lib/python3.11/site-packages/mlflow/pytorch/__init__.py'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow.pytorch\n",
    "mlflow.pytorch.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jy_zhang/anaconda3/envs/env_a/lib/python3.11/site-packages/mlflow/models/__init__.py'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow.models\n",
    "\n",
    "test = mlflow.models\n",
    "\n",
    "test.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/jy_zhang/code/pycode/test1.ipynb 单元格 65\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jy_zhang/code/pycode/test1.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m test \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmlflow.models\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jy_zhang/code/pycode/test1.ipynb#Y121sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtest\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jy_zhang/code/pycode/test1.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mlflow\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mlflow' is not defined"
     ]
    }
   ],
   "source": [
    "test = mlflow.models\n",
    "\n",
    "import test\n",
    "\n",
    "mlflow.models.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "model = torch.load(\"../my_model.pt\")\n",
    "isinstance(model,torch.nn.Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
